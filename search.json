[
  {
    "objectID": "R-InductionWeek.html",
    "href": "R-InductionWeek.html",
    "title": "html version",
    "section": "",
    "text": "Primary language is Python\n\nProgramming (MSIN00143), Business Strategy (MSIN0093), Machine Learning electives\n\nSecondary language is R\n\nMarketing Analytics (MSIN0094), Operations Analytics (MSIN0095), Statistical Foundations (MSIN0096)\n\n\n\n\n\n\nR project was initiated by Robert Gentleman and Ross Ihaka (Univ of Auckland) in 1991; both are statisticians, who later made the language open-source.\nSince 1997, R has been developed by the R Core Team on CRAN.\nAs of January 2022, it has 18,728 contributed packages. As of March 2022, R ranks 11th in the TIOBE index1; the language peaked in 8th place in August 2020.\n\n\n\n\n\nSuper powerful data analytics and visualizations, including2\n\nData wrangling (dplyr) and data visualization (ggplot)\nEconometrics (numerous packages)\nPredictive analytics (numerous packages)\n\nWrite beautiful reports/dissertations/presentations using quarto\n\nWrite your MSc dissertation (highly recommended; super efficient)\nEffortlessly build websites. I built and maintain my personal website and the marketing course website all in R.\n\n\n\n\n\n\nR versus Python\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nLanguage purpose\nR is a statistical language specialized in the data analytics and visualization. Best for data science, may not be robust for production environment.\nPython is a general-purpose language that is used for the deployment and development of various projects. Best for production environment.\n\n\nData analytics\nR is better at statistical models and econometrics.\nPython is better at machine learning due to support from PyTorch and TensorFlow.\n\n\nIDEs\nRStudio\nMany options such as Jupyter Notebook, Spyder, Pycharm, etc.\n\n\nTargeted users\nPrimary users of R include researchers in academia and data scientists, who heavily rely on data analyses and visualization.\nPrimary users of python include developers and programmers.\n\n\n\n\n\n\nR is the programming language, and we need a “place” to write codes. This place is called an Integrated development environment (IDE).\nRStudio is THE BEST R IDE to date. And it’s interface consists of the following:\n\nscript: (top left) where you do the coding\nconsole: (bottom left) where you can run commands interactively with R and see code outputs\nenvironment: (top right) a list of named objects that we have generated\nhistory: (top right) the list of past commands that we have used\nhelp: (bottom right) user manuals of functions available in R\npackage: (bottom right) a collection of ready-to-use packages written by others\n\n\n\n\n\n\nYou can write codes interactively in the R console. See an example: Type the following code into your console and see what happens.\n\nprint('Hello World')\n\n[1] \"Hello World\"\n\n\nOften used for simple exploratory tasks, where you don’t need to keep a record of codes.\n\ncheck summary statistics; inspect datasets; etc.\n\n\n\n\n\n\nR script is a text-readable file ending with .R suffix. See an example.\n\ncodes can be run line-by-line or sourced altogether\n\n\n\n\n\n\nImportant\n\n\n\nAll texts in the script will be treated as R codes unless commented out.\n\n\n\nOften used for project development and deployment, where you don’t need to communicate results to others\n\n\n\n\n\nQuarto3 files have a .qmd suffix. You can think of Quarto as Microsoft Word that can run R codes.\nQuarto can create dynamic content with Python and R, conveniently combining data analytics work with beautiful reporting.\n\nQuarto can be thought of as the R equivalent of Jupyter Notebook but is much more powerful.\nWe will be mainly using Quarto in the marketing analytics module. You can also use Quarto to do your assignments, write your dissertation, and build your own blogging websites.\n\nLet’s create a new quarto file together!"
  },
  {
    "objectID": "R-InductionWeek.html#bilingual-arrangements-at-msc-ba",
    "href": "R-InductionWeek.html#bilingual-arrangements-at-msc-ba",
    "title": "html version",
    "section": "",
    "text": "Primary language is Python\n\nProgramming (MSIN00143), Business Strategy (MSIN0093), Machine Learning electives\n\nSecondary language is R\n\nMarketing Analytics (MSIN0094), Operations Analytics (MSIN0095), Statistical Foundations (MSIN0096)"
  },
  {
    "objectID": "R-InductionWeek.html#a-brief-history-of-r",
    "href": "R-InductionWeek.html#a-brief-history-of-r",
    "title": "html version",
    "section": "",
    "text": "R project was initiated by Robert Gentleman and Ross Ihaka (Univ of Auckland) in 1991; both are statisticians, who later made the language open-source.\nSince 1997, R has been developed by the R Core Team on CRAN.\nAs of January 2022, it has 18,728 contributed packages. As of March 2022, R ranks 11th in the TIOBE index1; the language peaked in 8th place in August 2020."
  },
  {
    "objectID": "R-InductionWeek.html#why-learn-r",
    "href": "R-InductionWeek.html#why-learn-r",
    "title": "html version",
    "section": "",
    "text": "Super powerful data analytics and visualizations, including2\n\nData wrangling (dplyr) and data visualization (ggplot)\nEconometrics (numerous packages)\nPredictive analytics (numerous packages)\n\nWrite beautiful reports/dissertations/presentations using quarto\n\nWrite your MSc dissertation (highly recommended; super efficient)\nEffortlessly build websites. I built and maintain my personal website and the marketing course website all in R."
  },
  {
    "objectID": "R-InductionWeek.html#one-one-comparison-with-python",
    "href": "R-InductionWeek.html#one-one-comparison-with-python",
    "title": "html version",
    "section": "",
    "text": "R versus Python\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nLanguage purpose\nR is a statistical language specialized in the data analytics and visualization. Best for data science, may not be robust for production environment.\nPython is a general-purpose language that is used for the deployment and development of various projects. Best for production environment.\n\n\nData analytics\nR is better at statistical models and econometrics.\nPython is better at machine learning due to support from PyTorch and TensorFlow.\n\n\nIDEs\nRStudio\nMany options such as Jupyter Notebook, Spyder, Pycharm, etc.\n\n\nTargeted users\nPrimary users of R include researchers in academia and data scientists, who heavily rely on data analyses and visualization.\nPrimary users of python include developers and programmers."
  },
  {
    "objectID": "R-InductionWeek.html#a-first-look-at-the-rstudio-interface",
    "href": "R-InductionWeek.html#a-first-look-at-the-rstudio-interface",
    "title": "html version",
    "section": "",
    "text": "R is the programming language, and we need a “place” to write codes. This place is called an Integrated development environment (IDE).\nRStudio is THE BEST R IDE to date. And it’s interface consists of the following:\n\nscript: (top left) where you do the coding\nconsole: (bottom left) where you can run commands interactively with R and see code outputs\nenvironment: (top right) a list of named objects that we have generated\nhistory: (top right) the list of past commands that we have used\nhelp: (bottom right) user manuals of functions available in R\npackage: (bottom right) a collection of ready-to-use packages written by others"
  },
  {
    "objectID": "R-InductionWeek.html#where-to-write-r-codes-i-console",
    "href": "R-InductionWeek.html#where-to-write-r-codes-i-console",
    "title": "html version",
    "section": "",
    "text": "You can write codes interactively in the R console. See an example: Type the following code into your console and see what happens.\n\nprint('Hello World')\n\n[1] \"Hello World\"\n\n\nOften used for simple exploratory tasks, where you don’t need to keep a record of codes.\n\ncheck summary statistics; inspect datasets; etc."
  },
  {
    "objectID": "R-InductionWeek.html#where-to-write-r-codes-ii-.r-script",
    "href": "R-InductionWeek.html#where-to-write-r-codes-ii-.r-script",
    "title": "html version",
    "section": "",
    "text": "R script is a text-readable file ending with .R suffix. See an example.\n\ncodes can be run line-by-line or sourced altogether\n\n\n\n\n\n\nImportant\n\n\n\nAll texts in the script will be treated as R codes unless commented out.\n\n\n\nOften used for project development and deployment, where you don’t need to communicate results to others"
  },
  {
    "objectID": "R-InductionWeek.html#where-to-write-r-codes-iii-.qmd-script",
    "href": "R-InductionWeek.html#where-to-write-r-codes-iii-.qmd-script",
    "title": "html version",
    "section": "",
    "text": "Quarto3 files have a .qmd suffix. You can think of Quarto as Microsoft Word that can run R codes.\nQuarto can create dynamic content with Python and R, conveniently combining data analytics work with beautiful reporting.\n\nQuarto can be thought of as the R equivalent of Jupyter Notebook but is much more powerful.\nWe will be mainly using Quarto in the marketing analytics module. You can also use Quarto to do your assignments, write your dissertation, and build your own blogging websites.\n\nLet’s create a new quarto file together!"
  },
  {
    "objectID": "R-InductionWeek.html#yaml-header",
    "href": "R-InductionWeek.html#yaml-header",
    "title": "html version",
    "section": "2.1 YAML header",
    "text": "2.1 YAML header\n\nYou can think of YAML header as a MS Word template, which determines how your final report looks like (font, font size, color, margins, etc.).\nThe YAML header is typically at the beginning of a document, separated from the main text by three dashes (---). YAML will not appear in the final report.\nTo make life easier, I will set YAML headers for all .qmd files for you in Marketing Analytics module."
  },
  {
    "objectID": "R-InductionWeek.html#authoring-with-normal-texts",
    "href": "R-InductionWeek.html#authoring-with-normal-texts",
    "title": "html version",
    "section": "2.2 Authoring with normal texts",
    "text": "2.2 Authoring with normal texts\nRStudio provides two ways to edit a quarto file (1) visual mode and (2) source mode.\n\nRStudio’s visual editor offers an WYSIWYM (Microsoft Word like) authoring experience for markdown\n\nrecommended and easier to learn; we will be using this mode in class\ncheck the rich formatting tools we can use for authoring a report\n\nIn the source mode, you can edit the file using markdown syntax\n\noptional; for advanced users once you’re familiar with the markdown syntax\n\n\n\n\n\nVisual Mode versus Source Mode\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new quarto file from RStudio with the following level-1 and level-2 headers\n\nBasics of R\nVectors\n\nCreating vectors"
  },
  {
    "objectID": "R-InductionWeek.html#coding-with-code-blocks",
    "href": "R-InductionWeek.html#coding-with-code-blocks",
    "title": "html version",
    "section": "2.3 Coding with code blocks",
    "text": "2.3 Coding with code blocks\n\nIn qmd files, we write actual R codes in code chunks identified with {r}.\nYou can run each code chunk interactively by clicking the render icon. RStudio executes the code and displays the results below the code chunks.\nTo insert a code chunk, click Insert -&gt;Code Chunk -&gt; R.\nSee an example and try on your computer!\n\n\nprint('R is the Best Language! Better than Python! And dont tell David I said this!')\n\n[1] \"R is the Best Language! Better than Python! And dont tell David I said this!\"\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInsert the above R code block in your quarto file under any section."
  },
  {
    "objectID": "R-InductionWeek.html#rendering-a-report",
    "href": "R-InductionWeek.html#rendering-a-report",
    "title": "html version",
    "section": "2.4 Rendering a report",
    "text": "2.4 Rendering a report\nAt the end, when codes and main texts are ready, use the Render button in the RStudio IDE to render the file.\nThe rendered report will be in the same folder with your qmd file.\n\n\n\n\n\n\n\nExercise\n\n\n\nRender your quarto file into a document and see how it looks like."
  },
  {
    "objectID": "R-InductionWeek.html#more-learning-resources-for-quarto",
    "href": "R-InductionWeek.html#more-learning-resources-for-quarto",
    "title": "html version",
    "section": "2.5 More learning resources for Quarto",
    "text": "2.5 More learning resources for Quarto\n\nThe available YAML fields vary based on document format\n\nHere for YAML fields for PDF documents\nHere for MS Word\nHere for HTML documents\n\nMarkdown syntax\n\nMarkdown basics\nMarkdown practice\n\nQuarto (recommended to be reviewed after-class)\n\nGet started"
  },
  {
    "objectID": "R-InductionWeek.html#named-objects",
    "href": "R-InductionWeek.html#named-objects",
    "title": "html version",
    "section": "3.1 Named objects",
    "text": "3.1 Named objects\n\nR is an object-oriented language, so we will be working on named objects.\nWe use the left arrow &lt;- to create a named object, which assigns the objects on the RHS to the name on the LHS.4\n\nThe below code creates a new object called ‘x’ in the environment, which is a number 2.\n\n\n\nx &lt;- 3\nx\n\n[1] 3\n\n\n\nAfter an object is created, we can refer to the object by its name, and operates on it.\n\n\n# Question: why Wei chooses these two numbers?\nx^2\n\n[1] 9\n\nx^3\n\n[1] 27\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInsert a code block in your quarto file, which does the following:\n\nCreate an object with name ‘x’ with value 2 + 2"
  },
  {
    "objectID": "R-InductionWeek.html#rules-for-object-names",
    "href": "R-InductionWeek.html#rules-for-object-names",
    "title": "html version",
    "section": "3.2 Rules for object names",
    "text": "3.2 Rules for object names\nFor a variable to be valid, it should follow these rules\n\nIt should contain letters, numbers, and only dot or underscore characters.\nIt cannot start with a number (eg: 2iota).\n\n\n# 2iota &lt;- 2\n\n\nIt cannot start with a dot followed by a number (eg: .2iota).\n\n\n# .iota &lt;- 2\n\n\nIt should not start with an underscore (eg: _iota).\n\n\n# _iota &lt;- 2\n\n\nIt should not be a reserved keyword.\n\n\n# mean &lt;- 2\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s good practice to use memorable names to name an object\n\nFor instance, use prefix “df_” or “data_” to name datasets."
  },
  {
    "objectID": "R-InductionWeek.html#functions",
    "href": "R-InductionWeek.html#functions",
    "title": "html version",
    "section": "3.3 Functions",
    "text": "3.3 Functions\n\nIn R, a function takes object(s) as input, run specific actions on the object(s) defined by the function, and then return an outcome object.\n\nThe example below shows the function mean, which computes the average of several numbers.\n\n\n\na &lt;- 1:3 # which generates a sequence 1,2,3\na\n\n[1] 1 2 3\n\nmean(a)\n\n[1] 2\n\n\n\nWe will heavily rely on functions to conduct data analyses. For how to use a new function, search the function in RStudio’s help panel.\n\nDescription: what the function does in a nutshell\nUsage: how to call the function\nArguments: how you would like to run the function\nValue: what will be returned\nExamples: examples of how to use the function\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nSearch and learn the usage of function “sum”\nInsert a code block in your quarto file to compute the sum of vector 1:3"
  },
  {
    "objectID": "R-InductionWeek.html#collection-of-functions-packages",
    "href": "R-InductionWeek.html#collection-of-functions-packages",
    "title": "html version",
    "section": "3.4 Collection of functions: Packages",
    "text": "3.4 Collection of functions: Packages\nThe base R already has many useful built-in functions to perform basic tasks, but as data scientists, we need more.\nTo perform certain tasks (such as a machine learning model), we can definitely write our own code from scratch, but it takes lots of (unnecessary) effort. Fortunately, many packages have been written by others for us to directly use.\n\nInstall the package using the built-in function install.packages(). R will download the package.\n\n\ninstall.packages('praise')\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\n\n\nLoad the packages using library(). Every time you restart the RStudio, packages need to be reloaded.\n\n\nlibrary(praise)\n\n\nNow that the package is loaded, you can use the functions in it. praise() is a function in the praise package.\n\n\npraise()\n\n[1] \"You are beautiful!\"\n\n\n\n\n\n\n\n\nTips\n\n\n\nInstallation of a package is only needed for the first time. After installation, just need to reload the packages using library() every time your restart RStudio."
  },
  {
    "objectID": "R-InductionWeek.html#comment-codes",
    "href": "R-InductionWeek.html#comment-codes",
    "title": "html version",
    "section": "3.5 Comment codes",
    "text": "3.5 Comment codes\nYou can put a # before any code, to indicate that any codes after the # on the same line are your comments, and will not be run by R.\nIt’s a good practice to often comment your codes, so that you can help the future you to remember what you were trying to achieve.\n\n# print(\"Support Wei for an iPhone 14 Pro!\")\n\n# Below, x will be 1 rather than 1+1\nx &lt;- 1 # +1"
  },
  {
    "objectID": "R-InductionWeek.html#data-structures",
    "href": "R-InductionWeek.html#data-structures",
    "title": "html version",
    "section": "3.6 Data structures",
    "text": "3.6 Data structures\nBelow are the complete list of objects in R."
  },
  {
    "objectID": "R-InductionWeek.html#data-types",
    "href": "R-InductionWeek.html#data-types",
    "title": "html version",
    "section": "3.7 Data types",
    "text": "3.7 Data types\nTo make the best of the R language, you’ll need a strong understanding of the basic data types and data structures and how to operate on them. Data structures are very important to understand because these are the objects you will manipulate on a day-to-day basis in R.\n\nNumeric (e.g.,2.5)\n\nWe can use R as a calculator for numeric objects\n\n\n\n# Numeric Vector \nnum2 &lt;- 2.5\nlog(num2)\n\n[1] 0.9162907\n\nnum2^2\n\n[1] 6.25\n\nexp(num2)\n\n[1] 12.18249\n\n\n\nLogical (TRUE, FALSE)\n\nTRUE is equivalent to 1 in R; FALSE is equivalent to 0.\n\n\n\nlog1 &lt;- TRUE\nlog2 &lt;- FALSE\n\n\nCharacter (e.g. “Wei”, “UCL”, “1 + 1 = 3”, “TRUE”, etc.)\n\nwithin a pair of quotation marks; single or double quotation marks can both work.\n\n\n\nstr1 &lt;- \"1 + 1 = 2\"\n\n\nFactor (“male”, “female”, etc.)\n\nthis is an important class for describing categories. We will discuss in more detail later in class when we learn linear regression.\n\n\n\ncountry &lt;- c('UK','Spain','Italy','Multiverse')\nfactor(country)\n\n[1] UK         Spain      Italy      Multiverse\nLevels: Italy Multiverse Spain UK"
  },
  {
    "objectID": "R-InductionWeek.html#check-data-types-using-class",
    "href": "R-InductionWeek.html#check-data-types-using-class",
    "title": "html version",
    "section": "3.8 Check data types using class()",
    "text": "3.8 Check data types using class()\nWe can use class() to check the type of an object in R.\n\na &lt;- '1+1'\nclass(a)\n\n[1] \"character\"\n\n\n\nb &lt;- 1+1\nclass(b)\n\n[1] \"numeric\"\n\n\nThis is very useful when we first load data from external databases, we need to make sure variables are of the correct data types."
  },
  {
    "objectID": "R-InductionWeek.html#data-type-conversion",
    "href": "R-InductionWeek.html#data-type-conversion",
    "title": "html version",
    "section": "3.9 Data type: conversion",
    "text": "3.9 Data type: conversion\nSometimes, data types of variables from raw data may not be what we want; we need to change the data type of a variable to the appropriate one.\nSee the following example:\n\na is a string, and we cannot use mathematical operations on it, or R will report errors.\n\n\na &lt;- '1'\nclass(a)\n\n[1] \"character\"\n\na + 1\n\nError in a + 1: non-numeric argument to binary operator\n\n\n\nWe can convert a to a numeric value. To convert from character to numeric, we use as.numeric()\n\n\nb &lt;- as.numeric(a)\nclass(b)\n\n[1] \"numeric\""
  },
  {
    "objectID": "R-InductionWeek.html#creating-vectors",
    "href": "R-InductionWeek.html#creating-vectors",
    "title": "html version",
    "section": "4.1 Creating vectors",
    "text": "4.1 Creating vectors\n\n4.1.1 Creating vectors: c()\nVector can be created using the function c() by listing all the values in the parenthesis, separated by comma ‘,’.\n\nx &lt;- c(1, 3, 5, 10)\nx\n\n[1]  1  3  5 10\n\nclass(x)\n\n[1] \"numeric\"\n\n\nVectors must contain elements of the same data type. Otherwise, it will implicitly convert elements into the same type.\n\nx &lt;- c(1, \"intro\", TRUE)\n\nclass(x)\n\n[1] \"character\"\n\n\n\n\n4.1.2 Checking the number of elements in a vector: length()\nYou can measure the length of a vector using the command length()\n\nx &lt;- c('R',' is', ' fun')\nlength(x)\n\n[1] 3\n\ny &lt;- c()\nlength(y)\n\n[1] 0\n\n\n\n\n4.1.3 Creating numeric sequences: seq() and rep()\nIt is also possible to easily create sequences with patterns\n\nuse seq() to create sequence with fixed steps\n\n\n# use seq()\nseq(from = 1, to = 2, by = 0.1)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0\n\n\n\nIf step is 1, there’s a simpler way using :\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n\n\nuse rep() to create repeated sequences.\n\n\n# replication using rep()\nrep(c(\"A\",\"B\"), times = 5)\n\n [1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\"\n\n\n\n\n4.1.4 Combine vectors\nYou can use c() to combine different vectors; this is very commonly used to concatenate vectors.\n\nx &lt;- 1:3 # from 1 to 3\ny &lt;- c(10, 15) # 10 and 15\nz &lt;- c(x,y) # x first and then y \nz\n\n[1]  1  2  3 10 15\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a sequence of {1,1,2,2,3,3,3} using different methods."
  },
  {
    "objectID": "R-InductionWeek.html#indexing-and-subsetting",
    "href": "R-InductionWeek.html#indexing-and-subsetting",
    "title": "html version",
    "section": "4.2 Indexing and subsetting",
    "text": "4.2 Indexing and subsetting\nWe put the index of elements we would like to extract in a square bracket [ ].5\n\nWhich element is in the second position?\n\n\nx &lt;- c(1,3,8,7) \nx[2]\n\n[1] 3\n\n\n\nWhat are the first 2 elements?\n\n\nx[1:2] \n\n[1] 1 3\n\n\n\nWhat are the 1st, 3rd and 4th elements?\n\n\nx[c(1,3,4)] \n\n[1] 1 8 7"
  },
  {
    "objectID": "R-InductionWeek.html#element-wise-operations",
    "href": "R-InductionWeek.html#element-wise-operations",
    "title": "html version",
    "section": "4.3 Element-wise operations",
    "text": "4.3 Element-wise operations\nR is a vectorized language, meaning by default it will do vector operation internally.\n\nIf you operate on a vector with a single number, the operation will be applied to all elements in the vector\n\n\nx &lt;- c(1,3,8,7)\nx+2\n\n[1]  3  5 10  9\n\nx^2\n\n[1]  1  9 64 49\n\n\n\n\n\n\n\n\nCaveats\n\n\n\nWhen the length of vectors do not match, R will still do it for you without reporting error but a warning message. As you can see, even if the length of vectors does not match, R can still return an output but throws a warning message. It’s important to check the warning messages when there is any!\n\n\n\nx &lt;- c(1,3,8,7)\n\ny &lt;- c(1,3,4) # careful!!! does not report error\nx + y\n\nWarning in x + y: longer object length is not a multiple of shorter object\nlength\n\n\n[1]  2  6 12  8\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a geometric sequence {2,4,8,16,32} using seq()."
  },
  {
    "objectID": "R-InductionWeek.html#relational-operations",
    "href": "R-InductionWeek.html#relational-operations",
    "title": "html version",
    "section": "4.4 Relational operations",
    "text": "4.4 Relational operations\n\nWe can compare a vector with a vector of the same length, which will do element-wise (element-by-element) comparison\n\n\nx &lt;- c(1,3,8,7) \ny &lt;- c(2,3,7,8)\nx &gt; y\n\n[1] FALSE FALSE  TRUE FALSE\n\nx == y\n\n[1] FALSE  TRUE FALSE FALSE\n\n\n\nWe can also compare a vector with a scalar, because R is vectorized\n\n\nx &lt;- c(1,3,8,7) \nx &lt; 6 # is each element lower than 6?\n\n[1]  TRUE  TRUE FALSE FALSE\n\nx == 10 # is the element equal to 10?\n\n[1] FALSE FALSE FALSE FALSE\n\n\n\nReturn the positions of elements that satisfy certain conditions: which()\n\n\nwhich(x == 8) # which element equals 8 \n\n[1] 3\n\nwhich.max(x) # which is the max element \n\n[1] 3\n\nwhich.min(x)\n\n[1] 1\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFind the minimum value of vector x using which()\n\n\n\nSometimes, we may need to operation on multiple relational operations using and or no\n\nT & F # and\n\n[1] FALSE\n\nT | F # or\n\n[1] TRUE\n\n!T # not\n\n[1] FALSE\n\n\n\nFor instance, we may want to find out elements that are smaller than 8 and larger than 3.\n\n\n\nwhich(x &lt; 8 & x &gt; 3 )\n\n[1] 4"
  },
  {
    "objectID": "R-InductionWeek.html#special-relational-operation-in",
    "href": "R-InductionWeek.html#special-relational-operation-in",
    "title": "html version",
    "section": "4.5 Special relational operation: %in%",
    "text": "4.5 Special relational operation: %in%\n\nA special relational operation is %in% in R, which tests whether an element exists in the object.\n\n\nx &lt;- c(1,3,8,7) \n\n3 %in% x\n\n[1] TRUE\n\n4 %in% x\n\n[1] FALSE"
  },
  {
    "objectID": "R-InductionWeek.html#after-class-exercise",
    "href": "R-InductionWeek.html#after-class-exercise",
    "title": "html version",
    "section": "4.6 After-class exercise",
    "text": "4.6 After-class exercise\n\nDatacamp Introduction to R, finish the following:\n\nIntro to basics\nVectors"
  },
  {
    "objectID": "R-InductionWeek.html#matrices-creating-matrices",
    "href": "R-InductionWeek.html#matrices-creating-matrices",
    "title": "html version",
    "section": "5.1 Matrices: creating matrices",
    "text": "5.1 Matrices: creating matrices\n\n5.1.1 Creating matrices: matrix()\n\nA matrix can be created using the command matrix()\n\nthe first argument is the vector to be converted into matrix\nthe second argument is the number of rows\nthe last argument is the number of cols (optional)\n\n\n\nmatrix(1:9, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR by default inserts elements vertically by columns\n\n\n\nR will fill in the matrix by order and discard the remaining elements once fully filled\n\n\nmatrix(1:9, nrow = 3, ncol = 2)\n\nWarning in matrix(1:9, nrow = 3, ncol = 2): data length [9] is not a\nsub-multiple or multiple of the number of columns [2]\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\nR will fill in the matrix by order and recycle to fill in the remaining elements\n\n\nmatrix(1:9, nrow = 3, ncol = 4)\n\nWarning in matrix(1:9, nrow = 3, ncol = 4): data length [9] is not a\nsub-multiple or multiple of the number of columns [4]\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7    1\n[2,]    2    5    8    2\n[3,]    3    6    9    3\n\n\n\n\n5.1.2 Creating matrices: inserting by row\nHowever, we can ask R to insert by rows by setting the byrow argument.\n\nmatrix(1:9, nrow = 3, ncol = 3, byrow = TRUE)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\n5.1.3 Creating matrices: concatenation of matrices cbind() and rbind()\nWe can use cbind() and rbind() to concatenate vectors and matrices into new matrices.\n\ncbind() does the column binding\n\n\nx &lt;- cbind(1:3, 4:6) # column bind\nx\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\ncbind() can also operate on matrices.\n\n\ncbind(x,x)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    1    4\n[2,]    2    5    2    5\n[3,]    3    6    3    6\n\n\n\nrbind() does the row binding\n\n\nrbind(7:9, 10:12) # row bind\n\n     [,1] [,2] [,3]\n[1,]    7    8    9\n[2,]   10   11   12"
  },
  {
    "objectID": "R-InductionWeek.html#matrices-indexing-and-subsetting",
    "href": "R-InductionWeek.html#matrices-indexing-and-subsetting",
    "title": "html version",
    "section": "5.2 Matrices: indexing and subsetting",
    "text": "5.2 Matrices: indexing and subsetting\nMatrices have two dimensions: rows and columns. Therefore, to extract elements from a matrix, we just need to specify which row(s) and which column(s) we want.\n\nx\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\nExtract an element\n\n1 is specified for row index, so we will extract elements from the first row\n1 is specified for column index, so we will extract elements from the the second column\nAltogether, we extract the single element in row 1, column 2.\n\n\n\nx[1,2] # the element in the 1st row, 2nd column\n\n[1] 4\n\n\n\nIf we leave blank for a dimension, we extract all elements of that dimension.\n\n1 is specified for row index, so we will extract elements from the first row\nNothing is specified for column index, so we will extract all elements from all columns\nAltogether, we extract all elements in the first row\n\n\n\nx[1,] # all elements in the first row\n\n[1] 1 4\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExtract all elements in the second column\nExtract all elements in the first and third rows"
  },
  {
    "objectID": "R-InductionWeek.html#matrices-operations",
    "href": "R-InductionWeek.html#matrices-operations",
    "title": "html version",
    "section": "5.3 Matrices: operations",
    "text": "5.3 Matrices: operations\nLet’s use 3 matrices x, y, and z:\n\nx &lt;- matrix(1:6, nrow = 3)\ny &lt;- matrix(1:6, byrow = T, nrow = 2)\n\n\nFunctions will be vectorized over all elements in a matrix\n\n\nx\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nz&lt;- x^2\nz\n\n     [,1] [,2]\n[1,]    1   16\n[2,]    4   25\n[3,]    9   36\n\n\n\n5.3.1 Matrices’ operations: matrix addition and multiplication\n\nIf the two matrices are of the same dimensions, they can do element-wise operations, including the *\n\n\nx + z   # elementwise addition\n\n     [,1] [,2]\n[1,]    2   20\n[2,]    6   30\n[3,]   12   42\n\nx * z\n\n     [,1] [,2]\n[1,]    1   64\n[2,]    8  125\n[3,]   27  216\n\n\n\nWe can also use %*% to indicate matrix multiplication\n\n\nx%*%y # matrix multiplication\n\n     [,1] [,2] [,3]\n[1,]   17   22   27\n[2,]   22   29   36\n[3,]   27   36   45\n\n\n\n\n5.3.2 Matrices’ operations: inverse and transpose\n\nWe use t() to do matrix transpose\n\n\nt(x) # transpose\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\n\nWe use solve() to get the inverse of an matrix\n\n\nsolve(t(x)%*%x) # inverse; must be on a square matrix\n\n           [,1]       [,2]\n[1,]  1.4259259 -0.5925926\n[2,] -0.5925926  0.2592593"
  },
  {
    "objectID": "R-InductionWeek.html#data-frames-creating-dataframe",
    "href": "R-InductionWeek.html#data-frames-creating-dataframe",
    "title": "html version",
    "section": "6.1 Data Frames: creating dataframe",
    "text": "6.1 Data Frames: creating dataframe\n\n6.1.1 Data Frames: create dataframe using data.frame()\n\nData Frame is the R object that we will deal with most of the time in the MSc program. You can think of data.frame as a spreadsheet in excel.\n\n\ndf &lt;-  data.frame(id = 1:4,\n  name = c(\"David\", \"Yongdong\", \"Anil\", \"Wei\"),\n  wage = rnorm(n=4, mean = 10^5, sd = 10^3), \n  male = c(T, T, T, T)\n  )\ndf\n\n\n\n  \n\n\n\n\nData frames can also be created from external sources, e.g., from a csv file or database."
  },
  {
    "objectID": "R-InductionWeek.html#data-frames-basics",
    "href": "R-InductionWeek.html#data-frames-basics",
    "title": "html version",
    "section": "6.2 Data Frames: Basics",
    "text": "6.2 Data Frames: Basics\n\nEach row stands for an observation; each column stands for a variable.\nEach column should have a unique name.\nEach column must contain the same data type, but the different columns can store different data types.\n\ncompare with matrix?\n\nEach column must be of same length, because rows have the same length across variables."
  },
  {
    "objectID": "R-InductionWeek.html#data-frames-check-dimensions-and-variable-types",
    "href": "R-InductionWeek.html#data-frames-check-dimensions-and-variable-types",
    "title": "html version",
    "section": "6.3 Data Frames: check dimensions and variable types",
    "text": "6.3 Data Frames: check dimensions and variable types\n\nYou can verify the size of the data.frame using the command dim(); or nrow() and ncol()\n\n\ndim(df)\n\n[1] 4 4\n\nnrow(df)\n\n[1] 4\n\nncol(df)\n\n[1] 4\n\n\n\nYou can get the data type info using the command str()\n\n\nclass(df)\n\n[1] \"data.frame\"\n\nstr(df)\n\n'data.frame':   4 obs. of  4 variables:\n $ id  : int  1 2 3 4\n $ name: chr  \"David\" \"Yongdong\" \"Anil\" \"Wei\"\n $ wage: num  99991 99914 101066 100597\n $ male: logi  TRUE TRUE TRUE TRUE\n\n\n\nGet the variables names\n\n\nnames(df)\n\n[1] \"id\"   \"name\" \"wage\" \"male\""
  },
  {
    "objectID": "R-InductionWeek.html#data-frames-summary",
    "href": "R-InductionWeek.html#data-frames-summary",
    "title": "html version",
    "section": "6.4 Data Frames: summary",
    "text": "6.4 Data Frames: summary\n\nSummarize the data frame\n\n\nsummary(df)\n\n       id           name                wage          male        \n Min.   :1.00   Length:4           Min.   : 99914   Mode:logical  \n 1st Qu.:1.75   Class :character   1st Qu.: 99971   TRUE:4        \n Median :2.50   Mode  :character   Median :100294                 \n Mean   :2.50                      Mean   :100392                 \n 3rd Qu.:3.25                      3rd Qu.:100714                 \n Max.   :4.00                      Max.   :101066"
  },
  {
    "objectID": "R-InductionWeek.html#data-frames-subsetting",
    "href": "R-InductionWeek.html#data-frames-subsetting",
    "title": "html version",
    "section": "6.5 Data Frames: subsetting",
    "text": "6.5 Data Frames: subsetting\nSince a dataframe is essentially a matrix, all the subsetting syntax with matrices can be applied here.\n\ndf$name # subset a column\n\n[1] \"David\"    \"Yongdong\" \"Anil\"     \"Wei\"     \n\ndf[,c(2,3)] # can also subset like a matrix\n\n\n\n  \n\n\n\nWe are interesting in the cylinders and the weights of inefficient cars (lower than 15 miles per gallon).\n\npoll_cars &lt;- mtcars[mtcars$mpg&lt;15, c(\"cyl\", \"wt\")] # remember to assign the generated dataframe to a new name\npoll_cars"
  },
  {
    "objectID": "R-InductionWeek.html#arrays",
    "href": "R-InductionWeek.html#arrays",
    "title": "html version",
    "section": "7.1 Arrays",
    "text": "7.1 Arrays\n\nWe can use array() to generate a high-dimensional array\nJust like vectors and matrices, arrays can include only data types of the same kind.\nA 3D array is basically a combination of matrices each laid on top of other\n\n\nx &lt;- 1:4\nx &lt;- array(data = x, dim = c(2,3,2))\nx\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    1\n[2,]    2    4    2\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    3    1    3\n[2,]    4    2    4"
  },
  {
    "objectID": "R-InductionWeek.html#lists",
    "href": "R-InductionWeek.html#lists",
    "title": "html version",
    "section": "7.2 Lists",
    "text": "7.2 Lists\nA list is an R object that can contain anything. List is pretty useful when you need to store objects for latter use.\n\nx &lt;- 1:2\ny &lt;- c(\"a\", \"b\")\nL &lt;- list( numbers = x, letters = y)"
  },
  {
    "objectID": "R-InductionWeek.html#lists-indexing-and-subsetting",
    "href": "R-InductionWeek.html#lists-indexing-and-subsetting",
    "title": "html version",
    "section": "7.3 Lists: indexing and subsetting",
    "text": "7.3 Lists: indexing and subsetting\nThere are many ways to extract a certain element from a list.\n\nby index\nby the name of the element\nby dollar sign $\n\n\nL[[1]] # extract the first element\n\n[1] 1 2\n\nL[['numbers']] # based on element name\n\n[1] 1 2\n\nL$numbers # extract the element called numbers\n\n[1] 1 2\n\n\nAfter extracting the element, we can work on the element further:\n\nL$numbers[1:3] &gt; 2\n\n[1] FALSE FALSE    NA"
  },
  {
    "objectID": "R-InductionWeek.html#ifelse",
    "href": "R-InductionWeek.html#ifelse",
    "title": "html version",
    "section": "8.1 if/else",
    "text": "8.1 if/else\nSometimes, you want to run your code based on different conditions. For instance, if the observation is a missing value, then use the population average to impute the missing value. This is where if/else kicks in.\nif (condition == TRUE) {\n  action 1\n} else if (condition == TRUE ){\n  action 2\n} else {\n  action 3\n}\nExample 1:\n\na &lt;- 15\n\nif (a &gt; 10) {\nlarger_than_10 &lt;- TRUE  \n} else {\n  larger_than_10 &lt;- FALSE\n}\n\nlarger_than_10  \n\n[1] TRUE\n\n\nExample 2:\n\nx &lt;- -5\nif(x &gt; 0){\n  print(\"x is a non-negative number\")\n} else {\n  print(\"x is a negative number\")\n}\n\n[1] \"x is a negative number\""
  },
  {
    "objectID": "R-InductionWeek.html#loops",
    "href": "R-InductionWeek.html#loops",
    "title": "html version",
    "section": "8.2 Loops",
    "text": "8.2 Loops\nAs the name suggests, in a loop the program repeats a set of instructions many times, until the stopping criteria is met.\nLoop is very useful for repetitive jobs.\n\nfor (i in 1:10){ # i is the iterator\n  # loop body: gets executed each time\n  # the value of i changes with each iteration\n}"
  },
  {
    "objectID": "R-InductionWeek.html#nested-loops",
    "href": "R-InductionWeek.html#nested-loops",
    "title": "html version",
    "section": "8.3 Nested loops",
    "text": "8.3 Nested loops\nWe can also nest loops into other loops.\n\nx &lt;- cbind(1:3, 4:6) # column bind\nx\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ny &lt;- cbind(7:9, 10:12) # row bind\ny\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\nz &lt;- x\n\nfor (i in 1:nrow(x)) {\n  for (j in 1:ncol(x)){\n    z[i,j] &lt;- x[i,j] + y[i,j]\n  }\n}\n\nz\n\n     [,1] [,2]\n[1,]    8   14\n[2,]   10   16\n[3,]   12   18"
  },
  {
    "objectID": "R-InductionWeek.html#functions-1",
    "href": "R-InductionWeek.html#functions-1",
    "title": "html version",
    "section": "8.4 Functions",
    "text": "8.4 Functions\nA function takes the argument as input, run some specified actions, and then return the result to us.\nFunctions are very useful. When we would like to test different ideas, we can combine functions with loops: We can write a function which takes different parameters as input, and we can use a loop to go through all the possible combinations of parameters.\n\n8.4.1 User-defined function syntax\nHere is how to define a function in general:\n\nfunction_name &lt;- function(arg1 ,arg2 = default_value){\n  # write the actions to be done with arg1 and arg2\n  # you can have any number of arguments, with or without defaults\n  return() # the last line is to return some value \n}\n\nExample:\n\nmagic &lt;- function( x, y){\n  return(x^2 + y)\n}\n\nmagic(1,3)\n\n[1] 4"
  },
  {
    "objectID": "R-InductionWeek.html#a-comprehensive-example",
    "href": "R-InductionWeek.html#a-comprehensive-example",
    "title": "html version",
    "section": "8.5 A comprehensive example",
    "text": "8.5 A comprehensive example\nTask: write a function, which takes a vector as input, and returns the max value of the vector\n\nget_max &lt;- function(input){\n  max_value &lt;- input[1]\n  for (i in 2:length(input) ) {\n    if (input[i] &gt; max_value) {\n      max &lt;- input[i]\n    }\n  }\n  \n  return(max)\n}\n\nget_max(c(-1,3,2))\n\n[1] 2\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWrite your own version of which.max() function"
  },
  {
    "objectID": "R-InductionWeek.html#footnotes",
    "href": "R-InductionWeek.html#footnotes",
    "title": "html version",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA measure of programming language popularity↩︎\nThere are many R-exclusive packages, such as the state-of-the-art causal machine learning library grf , which we will learn in the final week.↩︎\nWhy the name Quarto? “We wanted to use a name that had meaning in the history of publishing and landed on Quarto, which is the format of a book or pamphlet produced from full sheets printed with eight pages of text, four to a side, then folded twice to produce four leaves. The earliest known European printed book is a Quarto, the Sibyllenbuch, believed to have been printed by Johannes Gutenberg in 1452–53.”↩︎\nYou can also use equal sign =, but it’s recommended to stick with R’s tradition.↩︎\nNote that Python uses different ways to index and subset vectors and matrices.↩︎"
  },
  {
    "objectID": "Week4-Lecture1.html",
    "href": "Week4-Lecture1.html",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "\\[\nY = f(X;\\theta) + \\epsilon\n\\]\n\n\\(f\\) is the function that characterizes the true relationship between \\(X\\) and \\(Y\\)\n\n\\(f\\) is often called Data Generating Process (DGP), which is NEVER known to us\n\n\\(Y\\) the response/outcome/explained variable to be predicted\n\ne.g., whether customer responds to our offer (1/0)\n\n\\(X = (X_1,X_2,...,X_p)\\) are a set of predictors/features/explanatory variables\n\ncustomers’ past purchase history (e.g., spending in each category)\ndemographic variables (e.g., income, age, kids, etc.)\n\n\\(\\theta\\) represents the set of function parameters of the DGP \\(f\\)\n\\(\\epsilon\\) is the random error term, with mean zero.\n\n\n\n\n\nA supervised learning model is used when we have one or more explanatory variables and a response variable and we would like to find some function that describes the DGP between the explanatory variables and the response variable as accurately as possible.\n\n\n\n\n\n\n\nSince we never know the true DGP: All models are wrong, but some are useful1\n\n\n\n\nDepending on the type of the response variable, supervised learning tasks can be divided into two major groups\n\nClassification tasks: outcome is a categorical variable\n\nWhether a customer responds to marketing offers (acquisition)\nWhether a customer churns (retention)\n\nRegression tasks: outcome is a continuous variable\n\nProbability of customers responding to marketing offers (acquisition)\nCustomer total spending in each period (development)\nProbability of customer churn (retention)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression class models (easy to interpret, low accuracy)\n\nOLS, Lasso, Ridge regressions\n\nTree-based Models (good balance between interpretability and accuracy)\n\nDecision tree, random forest\n\nNeural-network based models (hard to interpret, high accuracy)\n\nDeep learning models\n\n\n\n\n\n\nOverfitting means a predictive model corresponds too closely to historical data, and therefore fails to predict new data reliably.\n\nOverfitting leads to high variance error, which is the error from being unable to predict for new data points\n\nUnderfitting occurs when a predictive model cannot adequately capture the underlying structure of historical data.\n\nUnderfitting leads to high bias error, which means high prediction error on the historical data, as the model fails to sufficiently capture the relations between \\(X\\) and \\(Y\\)\nAn underfitting model also tends to have poor predictive performance for future data points.\n\n\n\n\n\n\n\n\n\n\n\nTo mitigate the underfitting problem, select better models and tune the parameters.\nTo mitigate the overfitting problem, when building predictive models, we need to divide the full labelled historical data into different sets. The percentage of split depends on the context.\n\nTraining set (70% - 80% of labelled data): train the model parameters\nTest set (20% - 30% of labelled data): evaluate the prediction accuracy\n\n\n\n\n\n\n\n\nFor more complicated models with hyper-parameters such as deep learning models, we may even need to split our data into 3 sets (training, validation, and test sets). You will learn more details in term 2 from ML specialization modules."
  },
  {
    "objectID": "Week4-Lecture1.html#data-generating-process",
    "href": "Week4-Lecture1.html#data-generating-process",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "\\[\nY = f(X;\\theta) + \\epsilon\n\\]\n\n\\(f\\) is the function that characterizes the true relationship between \\(X\\) and \\(Y\\)\n\n\\(f\\) is often called Data Generating Process (DGP), which is NEVER known to us\n\n\\(Y\\) the response/outcome/explained variable to be predicted\n\ne.g., whether customer responds to our offer (1/0)\n\n\\(X = (X_1,X_2,...,X_p)\\) are a set of predictors/features/explanatory variables\n\ncustomers’ past purchase history (e.g., spending in each category)\ndemographic variables (e.g., income, age, kids, etc.)\n\n\\(\\theta\\) represents the set of function parameters of the DGP \\(f\\)\n\\(\\epsilon\\) is the random error term, with mean zero."
  },
  {
    "objectID": "Week4-Lecture1.html#supervised-learning-1",
    "href": "Week4-Lecture1.html#supervised-learning-1",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "A supervised learning model is used when we have one or more explanatory variables and a response variable and we would like to find some function that describes the DGP between the explanatory variables and the response variable as accurately as possible.\n\n\n\n\n\n\n\nSince we never know the true DGP: All models are wrong, but some are useful1"
  },
  {
    "objectID": "Week4-Lecture1.html#types-of-supervised-learning-tasks-for-customer-relationship-management",
    "href": "Week4-Lecture1.html#types-of-supervised-learning-tasks-for-customer-relationship-management",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "Depending on the type of the response variable, supervised learning tasks can be divided into two major groups\n\nClassification tasks: outcome is a categorical variable\n\nWhether a customer responds to marketing offers (acquisition)\nWhether a customer churns (retention)\n\nRegression tasks: outcome is a continuous variable\n\nProbability of customers responding to marketing offers (acquisition)\nCustomer total spending in each period (development)\nProbability of customer churn (retention)"
  },
  {
    "objectID": "Week4-Lecture1.html#commonly-used-supervised-learning-models",
    "href": "Week4-Lecture1.html#commonly-used-supervised-learning-models",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "Linear regression class models (easy to interpret, low accuracy)\n\nOLS, Lasso, Ridge regressions\n\nTree-based Models (good balance between interpretability and accuracy)\n\nDecision tree, random forest\n\nNeural-network based models (hard to interpret, high accuracy)\n\nDeep learning models"
  },
  {
    "objectID": "Week4-Lecture1.html#overfitting-and-underfitting",
    "href": "Week4-Lecture1.html#overfitting-and-underfitting",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "Overfitting means a predictive model corresponds too closely to historical data, and therefore fails to predict new data reliably.\n\nOverfitting leads to high variance error, which is the error from being unable to predict for new data points\n\nUnderfitting occurs when a predictive model cannot adequately capture the underlying structure of historical data.\n\nUnderfitting leads to high bias error, which means high prediction error on the historical data, as the model fails to sufficiently capture the relations between \\(X\\) and \\(Y\\)\nAn underfitting model also tends to have poor predictive performance for future data points."
  },
  {
    "objectID": "Week4-Lecture1.html#mitigate-the-underfitting-and-overfitting-problems",
    "href": "Week4-Lecture1.html#mitigate-the-underfitting-and-overfitting-problems",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "",
    "text": "To mitigate the underfitting problem, select better models and tune the parameters.\nTo mitigate the overfitting problem, when building predictive models, we need to divide the full labelled historical data into different sets. The percentage of split depends on the context.\n\nTraining set (70% - 80% of labelled data): train the model parameters\nTest set (20% - 30% of labelled data): evaluate the prediction accuracy\n\n\n\n\n\n\n\n\nFor more complicated models with hyper-parameters such as deep learning models, we may even need to split our data into 3 sets (training, validation, and test sets). You will learn more details in term 2 from ML specialization modules."
  },
  {
    "objectID": "Week4-Lecture1.html#introduction-to-decision-tree",
    "href": "Week4-Lecture1.html#introduction-to-decision-tree",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.1 Introduction to Decision Tree",
    "text": "2.1 Introduction to Decision Tree\n\nThere are many methodologies for constructing regression trees but one of the oldest and most commonly used is known as the classification and regression tree (CART) approach developed by Breiman et al. (1984)."
  },
  {
    "objectID": "Week4-Lecture1.html#motivation-example-predicting-mpg-from-mtcars",
    "href": "Week4-Lecture1.html#motivation-example-predicting-mpg-from-mtcars",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.2 Motivation Example: Predicting mpg from mtcars",
    "text": "2.2 Motivation Example: Predicting mpg from mtcars\n\nFrom mtcars, we want to predict the outcome variable mpg based on cyl and hp\n\nPredictors \\(X\\) include cyl and hp\nOutcome variable \\(Y\\) is mpg\n\n\n\npacman::p_load(dplyr,modelsummary)\ndata(\"mtcars\")\n# Generate a new data for the task\ndata_decision_tree &lt;- mtcars %&gt;%\n  select(mpg,cyl,hp)\n# check first 4 rows\ndata_decision_tree %&gt;%\n  head(n = 4) \n\n\n\n\n\n\n\n\n\nmpg\n\n\ncyl\n\n\nhp\n\n\n\n\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n110\n\n\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n110\n\n\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n93\n\n\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n110"
  },
  {
    "objectID": "Week4-Lecture1.html#implementation-of-decision-tree-in-r",
    "href": "Week4-Lecture1.html#implementation-of-decision-tree-in-r",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.3 Implementation of Decision Tree in R",
    "text": "2.3 Implementation of Decision Tree in R\n\nPackage rpart provides implementation of decision trees in R\n\nrpart() is the function in the package to train a decision tree; refer to its help function for more details.\n\nPackage rpart.plot provides nice visualizations of decision trees\n\n\n# Load the necessary packages\npacman::p_load(rpart,rpart.plot)\n\n\nThe downloaded binary packages are in\n    /var/folders/zv/5cydqfcx20b0p1lfmtwn2mv40000gn/T//RtmphAORTs/downloaded_packages\n\n\n\nrpart.plot installed\n\n# Below example shows how to train a decisin tree\ntree1 &lt;- rpart(\n  formula = mpg ~ cyl + hp,\n  data    = data_decision_tree,\n  method  = \"anova\" \n  )"
  },
  {
    "objectID": "Week4-Lecture1.html#intuition-behind-decision-trees",
    "href": "Week4-Lecture1.html#intuition-behind-decision-trees",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.4 Intuition behind Decision Trees",
    "text": "2.4 Intuition behind Decision Trees\n\n\n\n\n\n\n\n\n\n\nFirst, we find that cyl matters more than hp in terms of predicting mpg, so we split two branches according to cyl. We find that {4} and {6,8} split can best differentiate car models.\n\ncyl = 4, go to the right branch\ncyl = 6 or 8, go to the left branch\n\n\n\nFor car models with 4 cylinders, we try all possible splits based on hp, and find that 193 can best differentiate car models.\n\nhp &gt;= 193, use the average mpg in that terminal node, 13, as the predicted mpg\nhp &lt; 193, use the average mpg in that terminal node, 18, as the predicted mpg\n\n\n\nFor car models with less than 5 cylinders (the right branch), we find that hp does not differentiate car models, so we do not further split.\n\nUse the average mpg in that terminal node, 27, as the predicted mpg"
  },
  {
    "objectID": "Week4-Lecture1.html#pros-and-cons-of-decision-trees",
    "href": "Week4-Lecture1.html#pros-and-cons-of-decision-trees",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.5 Pros and Cons of Decision Trees",
    "text": "2.5 Pros and Cons of Decision Trees\nAdvantages of Decision Trees\n\nThey are very interpretable.\nMaking predictions is fast.\nIt’s easy to understand what variables are important in making the prediction. The internal nodes (splits) are those variables that most largely reduce the SSE (criteria for split).\n\nDisadvantages of Decision Trees\n\nSingle regression trees have high variance (overfitting), resulting in unstable predictions.\nDue to the high variance, single regression trees tend to have poor predictive accuracy."
  },
  {
    "objectID": "Week4-Lecture1.html#random-forest",
    "href": "Week4-Lecture1.html#random-forest",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.6 Random Forest",
    "text": "2.6 Random Forest\nTo overcome the overfitting tendency of a single decision tree, random forest has been developed by (Breiman 2001).\n\nBootstrap subsampling\n\nEach tree is grown to a bootstrapped subsample\n\nSplit-variable randomization\n\neach time a split is to be performed, the search for the split variable is limited to a random subset of m out of the p variables. For regression trees, typical default values are \\(m=p/3\\)."
  },
  {
    "objectID": "Week4-Lecture1.html#visualization-of-random-forest",
    "href": "Week4-Lecture1.html#visualization-of-random-forest",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.7 Visualization of Random Forest",
    "text": "2.7 Visualization of Random Forest\n\n\n\n\n\n\nEach tree gives a prediction\nRandom forest takes the majority of voting as the final prediction"
  },
  {
    "objectID": "Week4-Lecture1.html#implementation-of-random-forest-in-r",
    "href": "Week4-Lecture1.html#implementation-of-random-forest-in-r",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.8 Implementation of Random Forest in R",
    "text": "2.8 Implementation of Random Forest in R\n\nPackage ranger provides implementation of random forest in R.\nranger() is the function in the package to train a random forest; refer to its help function for more details.\nThe following code shows how to train a random forest consisting of 500 decision trees, where the outcome variable is mpg, and the predictors are 5 car attribute variables.\n\n\npacman::p_load(ranger)\n\n\nThe downloaded binary packages are in\n    /var/folders/zv/5cydqfcx20b0p1lfmtwn2mv40000gn/T//RtmphAORTs/downloaded_packages\n\n\n\nranger installed\n\nrandomforest1 &lt;- ranger(\n    formula   = mpg ~ hp + cyl + disp + wt + gear, \n    data      = mtcars, # dataset to train the model\n    num.trees = 500, # 500 decision trees\n    seed = 888 # make sure of replication\n  )"
  },
  {
    "objectID": "Week4-Lecture1.html#make-predictions-from-random-forest",
    "href": "Week4-Lecture1.html#make-predictions-from-random-forest",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.9 Make Predictions from Random Forest",
    "text": "2.9 Make Predictions from Random Forest\n\nAfter we train the predictive model, we can use predict() function to make predictions\n\nThe 1st argument is the trained model object\nThe 2nd argument is the dataset to make predictions on\n\n\n\n# Make predictions on the mtcars\nprediction_rf &lt;- predict(randomforest1,\n                         data = mtcars)\n\n# Because prediction_rf is a list object\n# Need to use $ to extract the predicted value as a numeric vector\nprediction_rf$predictions\n\n [1] 20.80814 20.80526 24.13373 20.14718 17.50231 18.98494 14.73153 24.16436\n [9] 22.84680 18.73638 18.73638 16.26754 16.26130 16.17000 11.96695 11.57377\n[17] 13.35940 29.91299 30.71855 31.04063 22.58045 16.30967 16.48256 14.42590\n[25] 17.42216 29.60584 26.18860 28.29520 15.87313 20.14866 14.97831 22.12769"
  },
  {
    "objectID": "Week4-Lecture1.html#after-class-reading",
    "href": "Week4-Lecture1.html#after-class-reading",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "2.10 After-Class Reading",
    "text": "2.10 After-Class Reading\n\n(optional) Varian, Hal R. “Big data: New tricks for econometrics.” Journal of Economic Perspectives 28, no. 2 (2014): 3-28\n(recommended) Decision tree in R\n(recommended) Random forest in R"
  },
  {
    "objectID": "Week4-Lecture1.html#footnotes",
    "href": "Week4-Lecture1.html#footnotes",
    "title": "Class 7 Predictive Analytics for STP (II): Supervised Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA famous quote by statistician George Box.↩︎"
  },
  {
    "objectID": "Week3-Lecture2.html",
    "href": "Week3-Lecture2.html",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "",
    "text": "The core of any business decision is break-even analysis (cost-benefit analysis)\n\nBEQ; NPV; CLV (Week 1 and Week 2)\n\nFor better profitability management, we can work on either reducing the cost or boosting the benefit.\n\n\n\n\n\n\n\n\n\n\nIn Weeks 3 and 4, we will learn how to utilize predictive analytics to reduce marketing costs and improve marketing efficiency\n\n\n\n\n\n\n\n\n\n\nUnderstand the concept of statistical learning\nUnderstand the concept of unsupervised learning and how to apply clustering analyses for customer segmentation"
  },
  {
    "objectID": "Week3-Lecture2.html#our-journey-so-far",
    "href": "Week3-Lecture2.html#our-journey-so-far",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "",
    "text": "The core of any business decision is break-even analysis (cost-benefit analysis)\n\nBEQ; NPV; CLV (Week 1 and Week 2)\n\nFor better profitability management, we can work on either reducing the cost or boosting the benefit."
  },
  {
    "objectID": "Week3-Lecture2.html#roadmap-of-predictive-analytics",
    "href": "Week3-Lecture2.html#roadmap-of-predictive-analytics",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "",
    "text": "In Weeks 3 and 4, we will learn how to utilize predictive analytics to reduce marketing costs and improve marketing efficiency"
  },
  {
    "objectID": "Week3-Lecture2.html#learning-objectives",
    "href": "Week3-Lecture2.html#learning-objectives",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "",
    "text": "Understand the concept of statistical learning\nUnderstand the concept of unsupervised learning and how to apply clustering analyses for customer segmentation"
  },
  {
    "objectID": "Week3-Lecture2.html#types-of-predictive-analytics",
    "href": "Week3-Lecture2.html#types-of-predictive-analytics",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "2.1 Types of Predictive Analytics",
    "text": "2.1 Types of Predictive Analytics\n\nUnsupervised Learning\n\nOnly observe X =&gt; Want to uncover unknown subgroups\n\nSupervised Learning\n\nObserve both X and Y =&gt; Want to predict Y for new data\n\nReinforcement Learning\n\nRewards and punishments =&gt; Learn the best decision rules\nDynamic Coupon Targeting Using Batch Deep Reinforcement Learning: An Application to Livestream Shopping\n\n\nIn Term 2, you will learn predictive analytics models systematically. By then, think about how those techniques can be applied back to these case studies."
  },
  {
    "objectID": "Week3-Lecture2.html#types-of-predictive-analytics-1",
    "href": "Week3-Lecture2.html#types-of-predictive-analytics-1",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "2.2 Types of Predictive Analytics",
    "text": "2.2 Types of Predictive Analytics"
  },
  {
    "objectID": "Week3-Lecture2.html#customer-segmentation",
    "href": "Week3-Lecture2.html#customer-segmentation",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.1 Customer Segmentation",
    "text": "3.1 Customer Segmentation\nSegmentation is the process of dividing customers into meaningful groups based on any characteristics relevant to design and execution of your marketing strategy. It assumes that different customer groups offer different levels of value to the company and/or require different marketing programs to succeed with (e.g., based on different goals and needs)."
  },
  {
    "objectID": "Week3-Lecture2.html#conventional-ways-for-customer-segmentation",
    "href": "Week3-Lecture2.html#conventional-ways-for-customer-segmentation",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.2 Conventional Ways for Customer Segmentation",
    "text": "3.2 Conventional Ways for Customer Segmentation\n\nCustomer value segmentation is for targeting decisions based on customers’ potential long-term financial and strategic value to your company.\nBenefit segmentation is for positioning and marketing mix design on the basis of customer and consumer goals or usage, the needs, wants, problems and the trade-offs they are willing to make across benefits (e.g., price vs. quality).\nPsychographic segmentation is for positioning and marketing mix design based on the psychology of the customer and consumer, including attitudes, identity, lifestyle, personality, etc.\nDemographic segmentation uses variables such as age, gender, income, family life cycle, educational qualification, socio-economic status, religion, company size and income, etc. These serve as proxies for goals, preferences or psychographics, as well as to characterize segments for marketing mix decisions.\n\nConventional segmentation methods require heavy human judgments. A more sensible way is to “let the data speak”."
  },
  {
    "objectID": "Week3-Lecture2.html#commonly-used-clustering-algorithms",
    "href": "Week3-Lecture2.html#commonly-used-clustering-algorithms",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.3 Commonly Used Clustering Algorithms",
    "text": "3.3 Commonly Used Clustering Algorithms\n\nK-means clustering\n\nThe number of clusters need to be pre-speciﬁed\n\nHierarchical clustering\n\nObservations are clustered in a tree-structured graph or dendrogram. No need to pre-determine the number of clusters."
  },
  {
    "objectID": "Week3-Lecture2.html#k-means-clustering",
    "href": "Week3-Lecture2.html#k-means-clustering",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.4 K-Means Clustering",
    "text": "3.4 K-Means Clustering\nK-means clustering is one of the most commonly used unsupervised machine learning algorithms for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst.\nIt can classify customers into multiple segments (i.e., clusters), such that customers within the same cluster are as similar as possible, whereas customers from different clusters are as dissimilar as possible. \n\nInput: customer data (characteristics of interest) and the number of clusters\nOutput: clusters\n\nLet \\(C_1 , C_2 , · · · , C_k\\) be the clusters\nEvery customer is categorized to only one of the clusters"
  },
  {
    "objectID": "Week3-Lecture2.html#k-means-clustering-intuition",
    "href": "Week3-Lecture2.html#k-means-clustering-intuition",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.5 K-Means Clustering: Intuition",
    "text": "3.5 K-Means Clustering: Intuition"
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.6 Implementation of K-Means in R for Tesco",
    "text": "3.6 Implementation of K-Means in R for Tesco\n\nDecide to do customer segmentation based on total spending and income\n\n\n\n\n\n\n\nx: data with selected variables to apply K-means\ncenters: number of clusters\niter.max: the maximum number of iterations allowed\nnstart: how many random sets should be chosen\nalgorithm: which algorithm to choose; default often works\ntrace: do you want to trace intermediate steps?"
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-1",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-1",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.7 Implementation of K-Means in R for Tesco",
    "text": "3.7 Implementation of K-Means in R for Tesco\n\nNeed to re-scale the two variables using scale(), because the two variables are of very different scales\n\nThis is extremely important!\nset.seed() is to allow replication of results. Refer to this data camp tutorial for more details.\n\n\n\nset.seed(888)\ndata_kmeans &lt;- data_full%&gt;%\n  select(Income,total_spending)%&gt;%\n  mutate(Income = scale(Income),\n         total_spending = scale(total_spending))\n\nresult_kmeans &lt;- kmeans(data_kmeans,\n                        centers = 2,\n                        nstart = 10)"
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-2",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-2",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.8 Implementation of K-Means in R for Tesco",
    "text": "3.8 Implementation of K-Means in R for Tesco\n\nExamine the returned object, result_kmeans\n\n\nstr(result_kmeans)\n\nList of 9\n $ cluster     : int [1:2000] 1 2 1 2 2 1 2 2 2 2 ...\n $ centers     : num [1:2, 1:2] 0.95 -0.663 1.022 -0.713\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"1\" \"2\"\n  .. ..$ : chr [1:2] \"Income\" \"total_spending\"\n $ totss       : num 3998\n $ withinss    : num [1:2] 726 553\n $ tot.withinss: num 1280\n $ betweenss   : num 2718\n $ size        : int [1:2] 822 1178\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\n\ncluster: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.\ncenters: A matrix of cluster centers.\ntotss: The total sum of squares.\nwithinss: Vector of within-cluster sum of squares, one component per cluster.\ntot.withinss: Total within-cluster sum of squares, i.e. sum(withinss).\nbetweenss: The between-cluster sum of squares, i.e. $totss-tot.withinss$.\nsize: The number of points in each cluster."
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-3",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-3",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.9 Implementation of K-Means in R for Tesco",
    "text": "3.9 Implementation of K-Means in R for Tesco\n\nVisualize the clusters\n\nWe need 2 packages cluster and factoextra\nUse fviz_cluster() to generate visualizations\n\n\n\npacman::p_load(cluster,factoextra)\n\nalso installing the dependencies 'lazyeval', 'crosstalk', 'estimability', 'mvtnorm', 'viridis', 'DT', 'ellipse', 'emmeans', 'flashClust', 'leaps', 'multcompView', 'scatterplot3d', 'dendextend', 'FactoMineR'\n\n\n\nThe downloaded binary packages are in\n    /var/folders/zv/5cydqfcx20b0p1lfmtwn2mv40000gn/T//RtmpV4CvQa/downloaded_packages\n\n\n\nfactoextra installed\n\nset.seed(888)\nfviz_cluster(result_kmeans,\n             data = data_kmeans)"
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-4",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-4",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.10 Implementation of K-Means in R for Tesco",
    "text": "3.10 Implementation of K-Means in R for Tesco\n\nDetermine the optimal number of clusters using statistical criteria\n\n\nGap Method\n\n\nset.seed(888)\ngap_stat &lt;- clusGap(data_kmeans, \n                    FUN = kmeans,\n                    K.max = 10,\n                    B = 50)\nfviz_gap_stat(gap_stat)"
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-5",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-5",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.11 Implementation of K-Means in R for Tesco",
    "text": "3.11 Implementation of K-Means in R for Tesco\n\nDetermine the optimal number of clusters using statistical criteria\n\n\nSilhouette method\n\n\nset.seed(888)\nfviz_nbclust(data_kmeans, kmeans, method = \"silhouette\")"
  },
  {
    "objectID": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-6",
    "href": "Week3-Lecture2.html#implementation-of-k-means-in-r-for-tesco-6",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.12 Implementation of K-Means in R for Tesco",
    "text": "3.12 Implementation of K-Means in R for Tesco\n\nCompare the CLV in the two segments, and decide which segment to serve.\n\nThis is a general idea of segmentation and targeting using unsupervised learning\nFinish this exercise after class"
  },
  {
    "objectID": "Week3-Lecture2.html#pros-and-cons-of-k-means-clustering",
    "href": "Week3-Lecture2.html#pros-and-cons-of-k-means-clustering",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.13 Pros and Cons of K-means Clustering",
    "text": "3.13 Pros and Cons of K-means Clustering\nAdvantages\n\nEasy to implement and explain\nComputationally eﬃcient\n\nDrawbacks\n\nAs the number of variable increases, curse of dimensionality problem occurs\nSensitive to outliers and initial seeds"
  },
  {
    "objectID": "Week3-Lecture2.html#after-class-readings",
    "href": "Week3-Lecture2.html#after-class-readings",
    "title": "Class 7 Predictive Analytics for STP (I): Unsupervised Learning",
    "section": "3.14 After-Class Readings",
    "text": "3.14 After-Class Readings\n\nUseful source: K-means Cluster Analysis"
  },
  {
    "objectID": "Week5-Lecture1.html",
    "href": "Week5-Lecture1.html",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "To make business decisions, we need two pieces of information for cost-benefit analyses (e.g., NPV and CLV)\n\nCost of the business activity\n\nEasy to estimate ex ante via budgeting\nWe can use predictive analytics to conduct targeted marketing to reduce costs\n\nExpected benefit of the business activity\n\nWe’re given this information in the case studies so far\n\n(PineApple) incremental sales from influencer marketing: 2.5% additional sales\n(1st assignment) loyalty program increases retention rate\n\n\n\nIn reality, this “benefit” information is not readily available, and will be for us to estimate. We need to use causal inference tools to obtain this information scientifically.\n\n\n\n\nUnderstand key concepts of causal inference and Rubin’s potential outcome framework\nLearn the steps to conduct randomized controlled trials (RCTs)\nAble to design appropriate RCTs to solve real-life marketing problems\n\n\n\n\nCar manufacturer BMW partnered with a car dealership (“the retailer”) for an online advertising campaign. Ads are targeted to individuals who are predicted to have a higher likelihood of being in the market to buy a car. In the end, a customer either saw no ads, saw only the dealership (retailer) ads, saw only the BMW (manufacturer) ads, or saw both the dealership and BMW ads. The conversion rates for each group are shown below.\n\n\n\n\n\nQuestion: What can you conclude from this chart? Are you confident to recommend to your manager that the ads are effective in acquiring customers?\n\n\n\nA bubble tea business owner Tom bought a marketing survey dataset from a consulting agency. The survey collected customers’ purchase intention for different bubble shops in Canary Wharf. After running a correlation test between item price and purchase intention for all bubble tea shops, he finds that there is a positive correlation. That is, pricy items tend to have higher purchase intention. Tom concluded that he should increase the bubble tea price in his shop so that the sales will increase accordingly.\nQuestion: What is your favoriate bubble tea flavor?\n\n\n\nThis is a plane that just returned from the battlefield. Red dots are bullet holes.\nWhich part of A, B, and C would you reinforce to increase the pilot’s survival rate?\n\n\n\n\n\n\n\n\nTo test the effectiveness of a new COVID vaccine, the government sent invites to the general public to seek volunteers for the clinical trial. The voluntarily recruited subjects then received the vaccination.\nAfter 3 months, the government ran a t-test on the infection rate between general public who didn’t receive the vaccine versus volunteers who received the vaccine, and found that the infection rate was statistically indifferent.\nThe scientists concluded that the new vaccine is ineffective and should not be adopted.\nWhat is your take?\n\n\n\n\n\n\n\n\n\n\n\n[…] the other half jointly to Joshua D. Angrist and Guido W. Imbens “for their methodological contributions to the analysis of causal relationships.”\n\n\n\n\nCausal inference is the process of determining the unbiased, actual, causal effect of a particular intervention on the outcome.\nCorrelation != Causation !!!\n\nOn rainy days, we observe more umbrellas on the street\n\ncorrect correlational statement: number of umbrellas is positively correlated with rainfall\ncorrect causal statement: more rainfall causes more umbrellas\nincorrect causal statement: more umbrellas cause more rainfall\n\nCausality becomes less straightforward in the business world, and managers can easily make mistakes if not understanding how to infer causal relations. It’s extremely important for you to train a “causal mindset”\n\n\n\n\n\n\n\n\n\nEssentially, we need to show\n\nIf we run the initiative, the desired outcome occurs\n\nThis proves: The desired outcome occurs when we run the initiative\n\nIf we do not run the initiative, the desired outcome does not occur\n\nThis proves: The outcome was not caused by other factors\nThis step is to rule out alternative explanations\n\n\n\n\n\n\n\n\n\n\n\nIf we show the ads to the targeted customers, will the sales be higher compared with customers who didn’t see the ads?\nIf we do not show the ads to the targeted customers, will the sales be higher compared with customers who didn’t see the ads?\n\nTherefore, the BMW ads campaign can NOT reveal the causal effect of the ads."
  },
  {
    "objectID": "Week5-Lecture1.html#our-journey-so-far",
    "href": "Week5-Lecture1.html#our-journey-so-far",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "To make business decisions, we need two pieces of information for cost-benefit analyses (e.g., NPV and CLV)\n\nCost of the business activity\n\nEasy to estimate ex ante via budgeting\nWe can use predictive analytics to conduct targeted marketing to reduce costs\n\nExpected benefit of the business activity\n\nWe’re given this information in the case studies so far\n\n(PineApple) incremental sales from influencer marketing: 2.5% additional sales\n(1st assignment) loyalty program increases retention rate\n\n\n\nIn reality, this “benefit” information is not readily available, and will be for us to estimate. We need to use causal inference tools to obtain this information scientifically."
  },
  {
    "objectID": "Week5-Lecture1.html#learning-objectives",
    "href": "Week5-Lecture1.html#learning-objectives",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "Understand key concepts of causal inference and Rubin’s potential outcome framework\nLearn the steps to conduct randomized controlled trials (RCTs)\nAble to design appropriate RCTs to solve real-life marketing problems"
  },
  {
    "objectID": "Week5-Lecture1.html#why-causal-inference-matters-example-1",
    "href": "Week5-Lecture1.html#why-causal-inference-matters-example-1",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "Car manufacturer BMW partnered with a car dealership (“the retailer”) for an online advertising campaign. Ads are targeted to individuals who are predicted to have a higher likelihood of being in the market to buy a car. In the end, a customer either saw no ads, saw only the dealership (retailer) ads, saw only the BMW (manufacturer) ads, or saw both the dealership and BMW ads. The conversion rates for each group are shown below.\n\n\n\n\n\nQuestion: What can you conclude from this chart? Are you confident to recommend to your manager that the ads are effective in acquiring customers?"
  },
  {
    "objectID": "Week5-Lecture1.html#why-causal-inference-matters-example-2",
    "href": "Week5-Lecture1.html#why-causal-inference-matters-example-2",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "A bubble tea business owner Tom bought a marketing survey dataset from a consulting agency. The survey collected customers’ purchase intention for different bubble shops in Canary Wharf. After running a correlation test between item price and purchase intention for all bubble tea shops, he finds that there is a positive correlation. That is, pricy items tend to have higher purchase intention. Tom concluded that he should increase the bubble tea price in his shop so that the sales will increase accordingly.\nQuestion: What is your favoriate bubble tea flavor?"
  },
  {
    "objectID": "Week5-Lecture1.html#why-causal-inference-matters-example-3",
    "href": "Week5-Lecture1.html#why-causal-inference-matters-example-3",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "This is a plane that just returned from the battlefield. Red dots are bullet holes.\nWhich part of A, B, and C would you reinforce to increase the pilot’s survival rate?"
  },
  {
    "objectID": "Week5-Lecture1.html#why-causal-inference-matters-example-4",
    "href": "Week5-Lecture1.html#why-causal-inference-matters-example-4",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "To test the effectiveness of a new COVID vaccine, the government sent invites to the general public to seek volunteers for the clinical trial. The voluntarily recruited subjects then received the vaccination.\nAfter 3 months, the government ran a t-test on the infection rate between general public who didn’t receive the vaccine versus volunteers who received the vaccine, and found that the infection rate was statistically indifferent.\nThe scientists concluded that the new vaccine is ineffective and should not be adopted.\nWhat is your take?"
  },
  {
    "objectID": "Week5-Lecture1.html#nobel-prize-in-economics-of-year-2021",
    "href": "Week5-Lecture1.html#nobel-prize-in-economics-of-year-2021",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "[…] the other half jointly to Joshua D. Angrist and Guido W. Imbens “for their methodological contributions to the analysis of causal relationships.”"
  },
  {
    "objectID": "Week5-Lecture1.html#causal-inference-1",
    "href": "Week5-Lecture1.html#causal-inference-1",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "Causal inference is the process of determining the unbiased, actual, causal effect of a particular intervention on the outcome.\nCorrelation != Causation !!!\n\nOn rainy days, we observe more umbrellas on the street\n\ncorrect correlational statement: number of umbrellas is positively correlated with rainfall\ncorrect causal statement: more rainfall causes more umbrellas\nincorrect causal statement: more umbrellas cause more rainfall\n\nCausality becomes less straightforward in the business world, and managers can easily make mistakes if not understanding how to infer causal relations. It’s extremely important for you to train a “causal mindset”"
  },
  {
    "objectID": "Week5-Lecture1.html#rule-of-thumb-for-checking-causal-inference-for-business-managers",
    "href": "Week5-Lecture1.html#rule-of-thumb-for-checking-causal-inference-for-business-managers",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "Essentially, we need to show\n\nIf we run the initiative, the desired outcome occurs\n\nThis proves: The desired outcome occurs when we run the initiative\n\nIf we do not run the initiative, the desired outcome does not occur\n\nThis proves: The outcome was not caused by other factors\nThis step is to rule out alternative explanations"
  },
  {
    "objectID": "Week5-Lecture1.html#revisit-of-bmw-example",
    "href": "Week5-Lecture1.html#revisit-of-bmw-example",
    "title": "Class 9 Causal Inference",
    "section": "",
    "text": "If we show the ads to the targeted customers, will the sales be higher compared with customers who didn’t see the ads?\nIf we do not show the ads to the targeted customers, will the sales be higher compared with customers who didn’t see the ads?\n\nTherefore, the BMW ads campaign can NOT reveal the causal effect of the ads."
  },
  {
    "objectID": "Week5-Lecture1.html#rubin-causal-model-1",
    "href": "Week5-Lecture1.html#rubin-causal-model-1",
    "title": "Class 9 Causal Inference",
    "section": "2.1 Rubin Causal Model",
    "text": "2.1 Rubin Causal Model\n\nThe Rubin causal model (RCM), also known as the Neyman–Rubin causal model, is an approach to the statistical analysis of cause and effect based on the framework of potential outcomes."
  },
  {
    "objectID": "Week5-Lecture1.html#treatment-allocation-rule",
    "href": "Week5-Lecture1.html#treatment-allocation-rule",
    "title": "Class 9 Causal Inference",
    "section": "2.2 Treatment Allocation Rule",
    "text": "2.2 Treatment Allocation Rule\n\nIn RCM, treatment assignment is often denoted by the variable \\(D_i\\)\nWe will focus on A-B testing this week, where there are 2 groups, a treatment group and a control group\n\n\\(D_i = 1\\) if customer \\(i\\) receives the treatment and thus is in the treatment group\n\\(D_i = 0\\) if customer \\(i\\) does not receive the treatment and thus is in the control condition\n\nTreatment allocation rule determines how treatment is assigned to individuals\n\nLet customer make their own choices into treatment/control\nLet customers be randomized into treatment/control"
  },
  {
    "objectID": "Week5-Lecture1.html#potential-outcome-and-individual-level-treatment-effect",
    "href": "Week5-Lecture1.html#potential-outcome-and-individual-level-treatment-effect",
    "title": "Class 9 Causal Inference",
    "section": "2.3 Potential Outcome and Individual-Level Treatment Effect",
    "text": "2.3 Potential Outcome and Individual-Level Treatment Effect\n\nFor each unit \\(i\\), we define two potential outcomes:\n\n\\(Y^1_i\\): the outcome that unit \\(i\\) is going to have if it receives the treatment\n\\(Y^0_i\\): the outcome that unit \\(i\\) is going to have if it does not receive the treatment\n\nThe causal effect of our training program is the difference between \\(Y^1_i\\) and \\(Y^0_i\\). Define the individual treatment effect as\n\n\\[\n    \\delta_i = Y^1_i - Y^0_i\n\\]\n\nThe treatment effect is at the individual level, so it can vary across the population."
  },
  {
    "objectID": "Week5-Lecture1.html#an-illustration-of-individual-treatment-effect",
    "href": "Week5-Lecture1.html#an-illustration-of-individual-treatment-effect",
    "title": "Class 9 Causal Inference",
    "section": "2.4 An Illustration of Individual Treatment Effect",
    "text": "2.4 An Illustration of Individual Treatment Effect\nTom would like to measure the causal effect of introducing a loyalty program (LP) on customer retention.\nLet’s say we have retrieved all infinity stones from Thanos, and have created 2 parallel universes, where everything else is equal, except that only in universe 1 Tom has adopted the LP.\nFor a random customer Dr Strange,\n\nIn universe 1, Tom adopts the LP, and Dr Strange has a retention rate of 70%\nIn universe 2, Tom does not adopt the LP, and Dr Strange has a retention rate of 60%.\n\n\n\n\n\n\nSubject\n$Y^1$\n$Y^0$\n$Y^1 - Y^0$\n\n\n\n\nDr Strange\n0.7\n0.6\n0.1\n\n\n\n\n\n\n\n\nConclusion: The treatment effect of LP on Dr Strange is +10%."
  },
  {
    "objectID": "Week5-Lecture1.html#a-motivating-example-a-group-of-customers",
    "href": "Week5-Lecture1.html#a-motivating-example-a-group-of-customers",
    "title": "Class 9 Causal Inference",
    "section": "2.5 A Motivating Example: A Group of Customers",
    "text": "2.5 A Motivating Example: A Group of Customers\n\nWe can collect a sample of customers, and estimate individual treatment effect for each of them\n\n\n\n\n\n\nSubject\n$Y^1$\n$Y^0$\n$Y^1 - Y^0$\n\n\n\n\nDr Strange\n0.70\n0.60\n0.10\n\n\nIron Man\n0.55\n0.50\n0.05\n\n\nThor\n0.80\n0.72\n0.08\n\n\nHulk\n0.60\n0.62\n-0.02"
  },
  {
    "objectID": "Week5-Lecture1.html#fundamental-problem-of-causal-inference",
    "href": "Week5-Lecture1.html#fundamental-problem-of-causal-inference",
    "title": "Class 9 Causal Inference",
    "section": "2.6 Fundamental Problem of Causal Inference",
    "text": "2.6 Fundamental Problem of Causal Inference\nIn reality, to measure the treatment effect of LP on a customer’s retention rate\n\nWe need to compare the outcome for the same individual customer in alternative universes, with and without the treatment.\nHowever, in reality, we only observe one of these two outcomes, the realized outcome. The missing outcome is often called counterfactual outcome.\n\n\n\n\n\n\nSubject\n$Y_1$\n$Y_0$\n$Y_1 - Y_0$\n\n\n\n\nDr Strange\n0.7\n?\n?\n\n\nIron Man\n?\n0.5\n?\n\n\nThor\n?\n0.72\n?\n\n\nHulk\n0.6\n?\n?\n\n\n\n\n\n\n\n\nSince it is impossible to see both potential outcomes at once, one of the potential outcomes is always missing. This dilemma is called the Fundamental Problem of Causal Inference."
  },
  {
    "objectID": "Week5-Lecture1.html#average-treatment-effect-ate",
    "href": "Week5-Lecture1.html#average-treatment-effect-ate",
    "title": "Class 9 Causal Inference",
    "section": "2.7 Average Treatment Effect (ATE)",
    "text": "2.7 Average Treatment Effect (ATE)\n\nIndividual treatment is never observed, and individual treatment effects are likely to vary with individual customers. Therefore, estimating individual treatment effects is infeasible and not policy interesting.\nAs data scientists/policymakers, we often care more about the treatment effects on the population, which is often referred to as the average treatment effects (ATE).\n\n\\[\nATE = E[\\delta_i] = E[Y^1_i - Y^0_i]\n\\]"
  },
  {
    "objectID": "Week5-Lecture1.html#att-and-atu",
    "href": "Week5-Lecture1.html#att-and-atu",
    "title": "Class 9 Causal Inference",
    "section": "2.8 ATT and ATU",
    "text": "2.8 ATT and ATU\n\nSometimes, we may also care about the treatment effects on the treated (ATT).\n\ne.g., NHS only gives booster jabs to vulnerable groups\n\n\n\\[\nATT = E[\\delta_i|D_i = 1] = E[Y^1_i - Y^0_i|D_i = 1]\n\\]\n\nSimilarly, we can also define the Treatment Effect on the Untreated (ATU), but ATU is often of less interest to policymakers.\n\n\\[\nATU = E[\\delta_i|D_i = 0] = E[Y^1_i - Y^0_i|D_i = 0]\n\\]\n\nIf treatment is randomly assigned among the whole population, then we have the following equality:\n\n\\[\nATE = ATT = ATU\n\\]"
  },
  {
    "objectID": "Week5-Lecture1.html#basic-identity-of-causal-inference",
    "href": "Week5-Lecture1.html#basic-identity-of-causal-inference",
    "title": "Class 9 Causal Inference",
    "section": "2.9 Basic Identity of Causal Inference",
    "text": "2.9 Basic Identity of Causal Inference\n\nLet’s say, in a dataset, based on some treatment assignment scheme, some individuals receive the treatment while others do not.\nWe can compare the average outcome of the treated with the average outcome of the untreated. But can this simple difference give us the average treatment effect?\n\n\n\n\n\n\n\nBasic Identity of Causal Inference\n\n\n\nAverage outcome for treated - Average outcome for untreated\n= [Average outcome for treated - Average counterfactual outcome for treated] + [Average counterfactual outcome for treated - Avereage outcome for untreated]\n= ATT + Selection Bias"
  },
  {
    "objectID": "Week5-Lecture1.html#basic-identity-of-causal-inference-bmw-case",
    "href": "Week5-Lecture1.html#basic-identity-of-causal-inference-bmw-case",
    "title": "Class 9 Causal Inference",
    "section": "2.10 Basic Identity of Causal Inference: BMW Case",
    "text": "2.10 Basic Identity of Causal Inference: BMW Case\n\nIf BMW only shows ads to customers who have shown a strong interest in buying from BMW through targeted marketing\n\n\n\n\n\n\n\nATT != ATE, because those who saw the ads are different from those who didn’t.\nSelection bias is not equal to 0, because those who saw the ads alreayd have a higher purchase intention than those who didn’t."
  },
  {
    "objectID": "Week5-Lecture1.html#basic-identity-of-causal-inference-random-assignment",
    "href": "Week5-Lecture1.html#basic-identity-of-causal-inference-random-assignment",
    "title": "Class 9 Causal Inference",
    "section": "2.11 Basic Identity of Causal Inference: Random Assignment",
    "text": "2.11 Basic Identity of Causal Inference: Random Assignment\n\nIf BMW randomly decides which customers see the ads rather than through targeted marketing\n\n\n\n\n\n\n\nIf the treatment (i.e., seeing the ads) is randomly assigned among customers, the above figure gives causal effects, because\n\nThe first term is an estimate of the ATT; but because customers are randomly selected, the treatment group and control group should be similar; therefore, we have ATT = ATU = ATE.\nThe second term selection bias has an expected value of zero, because customers are randomly selected."
  },
  {
    "objectID": "Week5-Lecture1.html#concluding-remarks",
    "href": "Week5-Lecture1.html#concluding-remarks",
    "title": "Class 9 Causal Inference",
    "section": "2.12 Concluding Remarks",
    "text": "2.12 Concluding Remarks\n\nRandomized Treatment not only makes sure ATE = ATT = ATU, but also fully removes selection bias.\nThis is why randomized experiments are considered the gold standard of causal inference."
  },
  {
    "objectID": "Week5-Lecture2.html",
    "href": "Week5-Lecture2.html",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "If BMW ads are not randomized to consumers but targeted at interested consumers\n\n\n\n\n\n\n\n\n\nObserved outcomes in data\n\n\\(E[Y^1 | D = 1] = 14\\%\\)\n\\(E[Y^0 | D = 0] = 0.7\\%\\)\n\nCounterfactual outcomes in parallel universe\n\n\\(E[Y^0 | D = 1] = 5\\%\\)\n\\(E[Y^1 | D = 0] = 2.7\\%\\)\n\n\n\n\n\nAverage outcome for treated - Average outcome for untreated\n\n= \\(E[Y^1| D = 1] - E[Y^0| D = 0]\\) = 13.3%\n= \\((E[Y^1| D = 1] - E[Y^0 | D = 1]) + (E[Y^0 | D = 1] - E[Y^0| D = 0])\\)\n= ATT + Selection Bias\n\nATT = \\(E[Y^1| D = 1] - E[Y^0| D = 1]\\) = 14% - 5% = 9%\nATU = \\(E[Y^1| D = 0] - E[Y^0| D = 0]\\) = 2.7% - 0.7% = 2%\nSelection Bias = \\(E[Y^0 | D = 1] - E[Y^0 | D = 0]\\) = 5% - 0.7% = 4.3%\nATE = 0.5 * ATT + 0.5 * ATU = 5.5%\n\n\n\n\nIf BMW ads are randomized to consumers\n\n\n\n\n\n\n\n\n\nObserved outcomes in data\n\n\\(E[Y^1 | D = 1] = 14\\%\\)\n\\(E[Y^0 | D = 0] = 0.7\\%\\)\n\nCounterfactual outcomes in parallel universe\n\n\\(E[Y^0 | D = 1] = 0.7\\%\\)\n\\(E[Y^1 | D = 0] = 14\\%\\)\n\n\n\n\n\nAverage outcome for treated - Average outcome for untreated\n\n= \\(E[Y^1| D = 1] - E[Y^0| D = 0]\\) = 13.3%\n= \\((E[Y^1| D = 1] - E[Y^0 | D = 1]) + (E[Y^0 | D = 1] - E[Y^0| D = 0])\\)\n= ATT + Selection Bias\n\nATT = \\(E[Y^1| D = 1] - E[Y^0| D = 1]\\) = 14% - 0.7% = 13.3%\nATU = \\(E[Y^1| D = 0] - E[Y^0| D = 0]\\) = 14% - 0.7% = 13.3%\nSelection Bias = \\(E[Y^0 | D = 1] - E[Y^0 | D = 0]\\) = 0.7% - 0.7% = 0%\nATE = 0.5 * ATT + 0.5 * ATU = 13.3%\n\n\n\n\n\nAfter assigning individuals into the treatment group and control group, we also observe the outcome for each individual in both groups.\nWe can decompose the observed outcome of a treatment into two effects\n\n\n\n\n\n\n\nBasic Identity of Causal Inference\n\n\n\nAverage outcome for treated - Average outcome for untreated\n= [Average outcome for treated - Average counterfactual outcome for treated] + [Average counterfactual outcome for treated - Avereage outcome for untreated]\n= ATT + Selection Bias\n\n\n\n\n\n\nBasic Identity of Causal Inference shows why randomized controlled trials are the gold standard for causal inference: If the treated group is a random sample of the population,\n\nthe first term is an estimate of the causal impact of the treatment on the population\nthe second term has an expected value of zero.\n\nThen by computing the average difference between the treatment group and control group, we obtain the average treatment effect!\n\n\n\n\n\n\nA randomized controlled trial (RCT) is an experimental form of impact evaluation in which the population receiving the program or policy intervention is chosen at random from the eligible population, and a control group is also chosen at random from the same eligible population.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe only vary the level of a single treatment variable (e.g., loyalty program)\n\nA/B testing (treatment group + control group)\n\nLoyalty program\nNo loyalty program\n\nA/B/N testing (multiple treatment groups + control group)\n\nPoint-based loyalty program; points can be redeemd for price vouchers\nPoint-based loyalty program; points can be redeemd for gifts\nPoint-based loyalty program; points can be redeemd for free top ups\nNo loyalty program\n\n\n\n\n\nWe are interested in multiple treatment variables and their interaction effects.\n\n2-by-2 factorial design\n\ntreatment group 1: LP Yes + Promo Yes\ntreatment group 2: LP Yes + Promo No\ntreatment group 3: LP No + Promo Yes\ncontrol group: LP No + Promo No"
  },
  {
    "objectID": "Week5-Lecture2.html#revisit-of-ate-att-and-atu-in-bmw-case-targeted-ads",
    "href": "Week5-Lecture2.html#revisit-of-ate-att-and-atu-in-bmw-case-targeted-ads",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "If BMW ads are not randomized to consumers but targeted at interested consumers\n\n\n\n\n\n\n\n\n\nObserved outcomes in data\n\n\\(E[Y^1 | D = 1] = 14\\%\\)\n\\(E[Y^0 | D = 0] = 0.7\\%\\)\n\nCounterfactual outcomes in parallel universe\n\n\\(E[Y^0 | D = 1] = 5\\%\\)\n\\(E[Y^1 | D = 0] = 2.7\\%\\)\n\n\n\n\n\nAverage outcome for treated - Average outcome for untreated\n\n= \\(E[Y^1| D = 1] - E[Y^0| D = 0]\\) = 13.3%\n= \\((E[Y^1| D = 1] - E[Y^0 | D = 1]) + (E[Y^0 | D = 1] - E[Y^0| D = 0])\\)\n= ATT + Selection Bias\n\nATT = \\(E[Y^1| D = 1] - E[Y^0| D = 1]\\) = 14% - 5% = 9%\nATU = \\(E[Y^1| D = 0] - E[Y^0| D = 0]\\) = 2.7% - 0.7% = 2%\nSelection Bias = \\(E[Y^0 | D = 1] - E[Y^0 | D = 0]\\) = 5% - 0.7% = 4.3%\nATE = 0.5 * ATT + 0.5 * ATU = 5.5%"
  },
  {
    "objectID": "Week5-Lecture2.html#revisit-of-ate-att-and-atu-in-bmw-case-randomized-ads",
    "href": "Week5-Lecture2.html#revisit-of-ate-att-and-atu-in-bmw-case-randomized-ads",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "If BMW ads are randomized to consumers\n\n\n\n\n\n\n\n\n\nObserved outcomes in data\n\n\\(E[Y^1 | D = 1] = 14\\%\\)\n\\(E[Y^0 | D = 0] = 0.7\\%\\)\n\nCounterfactual outcomes in parallel universe\n\n\\(E[Y^0 | D = 1] = 0.7\\%\\)\n\\(E[Y^1 | D = 0] = 14\\%\\)\n\n\n\n\n\nAverage outcome for treated - Average outcome for untreated\n\n= \\(E[Y^1| D = 1] - E[Y^0| D = 0]\\) = 13.3%\n= \\((E[Y^1| D = 1] - E[Y^0 | D = 1]) + (E[Y^0 | D = 1] - E[Y^0| D = 0])\\)\n= ATT + Selection Bias\n\nATT = \\(E[Y^1| D = 1] - E[Y^0| D = 1]\\) = 14% - 0.7% = 13.3%\nATU = \\(E[Y^1| D = 0] - E[Y^0| D = 0]\\) = 14% - 0.7% = 13.3%\nSelection Bias = \\(E[Y^0 | D = 1] - E[Y^0 | D = 0]\\) = 0.7% - 0.7% = 0%\nATE = 0.5 * ATT + 0.5 * ATU = 13.3%"
  },
  {
    "objectID": "Week5-Lecture2.html#revisit-of-basic-identity-of-causal-inference",
    "href": "Week5-Lecture2.html#revisit-of-basic-identity-of-causal-inference",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "After assigning individuals into the treatment group and control group, we also observe the outcome for each individual in both groups.\nWe can decompose the observed outcome of a treatment into two effects\n\n\n\n\n\n\n\nBasic Identity of Causal Inference\n\n\n\nAverage outcome for treated - Average outcome for untreated\n= [Average outcome for treated - Average counterfactual outcome for treated] + [Average counterfactual outcome for treated - Avereage outcome for untreated]\n= ATT + Selection Bias"
  },
  {
    "objectID": "Week5-Lecture2.html#random-assignment-of-individuals",
    "href": "Week5-Lecture2.html#random-assignment-of-individuals",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "Basic Identity of Causal Inference shows why randomized controlled trials are the gold standard for causal inference: If the treated group is a random sample of the population,\n\nthe first term is an estimate of the causal impact of the treatment on the population\nthe second term has an expected value of zero.\n\nThen by computing the average difference between the treatment group and control group, we obtain the average treatment effect!"
  },
  {
    "objectID": "Week5-Lecture2.html#randomized-controlled-trials",
    "href": "Week5-Lecture2.html#randomized-controlled-trials",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "A randomized controlled trial (RCT) is an experimental form of impact evaluation in which the population receiving the program or policy intervention is chosen at random from the eligible population, and a control group is also chosen at random from the same eligible population."
  },
  {
    "objectID": "Week5-Lecture2.html#types-of-rcts-in-marketing-univariate-testing",
    "href": "Week5-Lecture2.html#types-of-rcts-in-marketing-univariate-testing",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "We only vary the level of a single treatment variable (e.g., loyalty program)\n\nA/B testing (treatment group + control group)\n\nLoyalty program\nNo loyalty program\n\nA/B/N testing (multiple treatment groups + control group)\n\nPoint-based loyalty program; points can be redeemd for price vouchers\nPoint-based loyalty program; points can be redeemd for gifts\nPoint-based loyalty program; points can be redeemd for free top ups\nNo loyalty program"
  },
  {
    "objectID": "Week5-Lecture2.html#types-of-rcts-in-marketing-multivariate-testing",
    "href": "Week5-Lecture2.html#types-of-rcts-in-marketing-multivariate-testing",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "",
    "text": "We are interested in multiple treatment variables and their interaction effects.\n\n2-by-2 factorial design\n\ntreatment group 1: LP Yes + Promo Yes\ntreatment group 2: LP Yes + Promo No\ntreatment group 3: LP No + Promo Yes\ncontrol group: LP No + Promo No"
  },
  {
    "objectID": "Week5-Lecture2.html#motivating-example",
    "href": "Week5-Lecture2.html#motivating-example",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.1 Motivating Example",
    "text": "2.1 Motivating Example\n\nTom is considering whether or not to introduce a loyalty program for his bubble tea business. This decision is essentially a cost-benefit analysis\n\nCost: it takes money and time to develop the loyalty program\nBenefit: it may increase spending and retention rate, and hence future CLV\n\nCost can be estimated through budgeting, but how to estimate the benefit from introducing LP?\nHow you design the experiment is more an art than a science."
  },
  {
    "objectID": "Week5-Lecture2.html#step-1-decide-on-the-unit-of-randomization",
    "href": "Week5-Lecture2.html#step-1-decide-on-the-unit-of-randomization",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.2 Step 1: Decide on the Unit of Randomization",
    "text": "2.2 Step 1: Decide on the Unit of Randomization\nIn the first step, we decide the level of granularity random assignment should occur at.\n\nindividual/household/store/city level"
  },
  {
    "objectID": "Week5-Lecture2.html#step-1-proposal-i",
    "href": "Week5-Lecture2.html#step-1-proposal-i",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.3 Step 1: Proposal I",
    "text": "2.3 Step 1: Proposal I\nProposal 1: It decides at random to test ‘No’ in West London and ‘Yes’ in East London.\n\nDo you expect the “at random” to be true randomization?"
  },
  {
    "objectID": "Week5-Lecture2.html#step-1-proposal-ii",
    "href": "Week5-Lecture2.html#step-1-proposal-ii",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.4 Step 1: Proposal II",
    "text": "2.4 Step 1: Proposal II\nProposal 2: It randomizes each individual customer to either the ‘No’ condition or ‘Yes’ pricing condition.\n\nIs this true randomization?\nWhat problems can we still have?"
  },
  {
    "objectID": "Week5-Lecture2.html#step-1-pros-and-cons-of-granularity",
    "href": "Week5-Lecture2.html#step-1-pros-and-cons-of-granularity",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.5 Step 1: Pros and Cons of Granularity",
    "text": "2.5 Step 1: Pros and Cons of Granularity\nDisadvantages of granularity:\n\nCosts and logistics\nSpillovers and crossovers\n\nAdvantages of granularity:\n\nReduces the chance that the unobserved factors matter ex ante\nReduces the chance that there might be a systematic error/unbalance of covariates\n\nAdditional Questions:\n\nHow can we randomize individualized price discounts to customers?"
  },
  {
    "objectID": "Week5-Lecture2.html#step-2-ensure-no-spillover-and-crossover-effects",
    "href": "Week5-Lecture2.html#step-2-ensure-no-spillover-and-crossover-effects",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.6 Step 2: Ensure No Spillover and Crossover Effects",
    "text": "2.6 Step 2: Ensure No Spillover and Crossover Effects\n\nCrossover Effects: A crossover occurs when an individual who was supposed to be assigned to one treatment is accidentally exposed to another treatment.\n\nSolution: Make sure that the same unit receives the same treatment throughout the experiment\n\nSpillover effects: The behavior of the treatment group can affect control group as well\n\nSolution: Randomize at the level of plausibly isolated social networks such as a community, rather than individual level."
  },
  {
    "objectID": "Week5-Lecture2.html#step-2-ensure-no-spillover-and-crossover-effects-1",
    "href": "Week5-Lecture2.html#step-2-ensure-no-spillover-and-crossover-effects-1",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.7 Step 2: Ensure No Spillover and Crossover Effects",
    "text": "2.7 Step 2: Ensure No Spillover and Crossover Effects\nProposal: How should Tom mitigate spillover and crossover effects"
  },
  {
    "objectID": "Week5-Lecture2.html#step-3-decide-on-randomization-allocation-scheme",
    "href": "Week5-Lecture2.html#step-3-decide-on-randomization-allocation-scheme",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.8 Step 3: Decide on Randomization Allocation Scheme",
    "text": "2.8 Step 3: Decide on Randomization Allocation Scheme\n\nComplete Randomization: individuals (or the relevant unit of randomization) are simply allocated at random into a treatment.\n\nMost commonly used; easy to implement; no data required ex ante\n\nStratified Randomization: individuals are first divided into subsamples based on certain characteristics, and then randomization is conducted in each subsample\n\nThis stratified technique is useful if a covariate is strongly correlated with an outcome.\nLimitations: reliable data that would allow such stratification may not be present\n\n\nProposal: We can use complete randomization as customer purchase history data may not be available."
  },
  {
    "objectID": "Week5-Lecture2.html#step-4-collect-data",
    "href": "Week5-Lecture2.html#step-4-collect-data",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.9 Step 4: Collect Data",
    "text": "2.9 Step 4: Collect Data\n\nAny field experiment should be aware of the potential need for a large sample size\n\nThe larger sample size, the higher statistical power for the experiment\nrun a power calculation [link for tutorial] if there is a budget\n\nCollect both data on the outcome variables of interest and consumer characteristics data\n\nProposal: We need to collect customers’ retention rate data and link the retention data with their treatment assignment."
  },
  {
    "objectID": "Week5-Lecture2.html#step-5-interpreting-results-from-a-field-experiment",
    "href": "Week5-Lecture2.html#step-5-interpreting-results-from-a-field-experiment",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.10 Step 5: Interpreting Results from a Field Experiment",
    "text": "2.10 Step 5: Interpreting Results from a Field Experiment\nStep 5.1: Randomization check\n\nWe need to check if the treatment group and control group are indifferent and well-balanced in terms of their pre-treatment characteristics.\n\nStep 5.2: Analyze the data and estimate the ATE\n\nt-test to examine the difference in the average outcome between the treatment group and control group. In R, we can use t.test()\nRegression analysis (next week)"
  },
  {
    "objectID": "Week5-Lecture2.html#after-class-readings",
    "href": "Week5-Lecture2.html#after-class-readings",
    "title": "Class 10 Randomized Controlled Trials",
    "section": "2.11 After-Class Readings",
    "text": "2.11 After-Class Readings\n\n(optional) Varian, Hal R. ‘Causal Inference in Economics and Marketing’. Proceedings of the National Academy of Sciences 113, no. 27 (5 July 2016): 7310–15.\n(optional) Angrist, Joshua & Pischke, Jörn-Steffen. (2009). Mostly Harmless Econometrics: An Empiricist’s Companion."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Module leader: Dr. Wei Miao\nEmail: wei.miao@ucl.ac.uk\nOffice: S3, Level 38, One Canada Square\nTeaching assistants\n\nRafael Machado Molina, rafael.molina.21@ucl.ac.uk (UCL MSc BA 2022)\nKayi Yeung, ka.yeung.21@ucl.ac.uk (UCL MSc BA 2022)\nJiafan Lu, jiafan.lu@ucl.ac.uk (UCL PhD Management 2022)\n\nOffice hours: See Moodle for the links to book an office hour session with Wei (for lecture contents) and TAs (for R programming)"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJust a little heads up – unlike undergrad marketing modules where you mainly learn marketing strategies and concepts, this module focuses on “analytics” (which comes first) and its application in “marketing” (which comes next).↩︎"
  },
  {
    "objectID": "Week8-Lecture2.html",
    "href": "Week8-Lecture2.html",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "The necessary condition for OLS to reveal causal effect is: all confounding variables (i.e., variables that are correlated with \\(X\\) and affect outcome variable) are controlled in the regression.\nOtherwise, the OLS estimator will be biased and we can only obtain the total effect (correlation) rather than the direct effect (causal effect).\nFrom secondary data, we will never be able to control all confounding factors, which means we can never obtain causation from OLS regressions.\nSo, is there still a way for us to obtain causal inference from secondary data?\n\n\n\n\n\n\nAn instrumental variable is a variable \\(z\\) that satisfies two requirements:\n\n\\(z\\) is uncorrelated with \\(\\epsilon\\); that is, \\(cov(z,\\epsilon) = 0\\)\n\\(z\\) is correlated with \\(x\\); that is, \\(cov(z,x) \\neq 0\\)\n\n\n\nPoint 1 is called exogeneity requirement: the instrumental variable should be beyond an individual’s control, such that the instrumental variable will not be correlated with any individual’s choices/omitted variables.\n\nThe spirit is similar to RCT’s randomization\n\nPoint 2 is called relevance requirement: though beyond an individual’s control, the instrumental variable should still affect the individual’s \\(X\\), causing some exogenous variations in \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\nReturn of Military Service to Lifetime Income1\n\\[\nIncome = \\beta_0 + \\beta_1MilitaryService + \\epsilon\n\\]\n\nOLS suffers from endogeneity problems, for example\n\nindividual ability correlates with military service and affects income\nindividual health status correlates with military service and affects income\n\nA draft lottery was used to determine if a soldier with a certain birthday goes to the war.\nThe date of birth (\\(z\\)) is an instrumental variable\n\nis correlated with military service: \\(cov(z,x) \\neq 0\\)\nbut does not directly affect income: \\(cov(z,\\epsilon) = 0\\)\n\n\n\n\n\n\nExogeneity requires that \\(z\\) should only affect \\(Y\\) through \\(X\\), but not directly affect \\(Y\\).\nThe instrumental variable should be beyond an individual’s control. Because omitted variable bias is often caused by individual’s own selection, instrumental variables are thus not correlated with omitted variable bias.\n\n\n\n\n\nThe instrumental variable must be sufficiently correlated with \\(x\\).\nIf the correlation between \\(z\\) and \\(x\\) is too small, we have a weak IV problem.\nFor more mathematical details of the weak IV issue, refer to this resource.\n\n\n\n\nCan you come up with IV candidates for the following causation questions?\n\nCOVID-19 cases =&gt; Uber Driver Supply\n\nnew cases from neighboring cities\nnew cases from overseas\n\nNumber of restaurants on UberEat =&gt; Number of orders on UberEat\n\nclose down of restaurants due to government inspections\n\nRetail price =&gt; Sales\n\nwholesale price\ncosts of raw materials\nCOGS\nHausman instruments: the prices of the same product in other markets"
  },
  {
    "objectID": "Week8-Lecture2.html#causal-inference-from-ols",
    "href": "Week8-Lecture2.html#causal-inference-from-ols",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "The necessary condition for OLS to reveal causal effect is: all confounding variables (i.e., variables that are correlated with \\(X\\) and affect outcome variable) are controlled in the regression.\nOtherwise, the OLS estimator will be biased and we can only obtain the total effect (correlation) rather than the direct effect (causal effect).\nFrom secondary data, we will never be able to control all confounding factors, which means we can never obtain causation from OLS regressions.\nSo, is there still a way for us to obtain causal inference from secondary data?"
  },
  {
    "objectID": "Week8-Lecture2.html#what-is-an-instrumental-variable",
    "href": "Week8-Lecture2.html#what-is-an-instrumental-variable",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "An instrumental variable is a variable \\(z\\) that satisfies two requirements:\n\n\\(z\\) is uncorrelated with \\(\\epsilon\\); that is, \\(cov(z,\\epsilon) = 0\\)\n\\(z\\) is correlated with \\(x\\); that is, \\(cov(z,x) \\neq 0\\)\n\n\n\nPoint 1 is called exogeneity requirement: the instrumental variable should be beyond an individual’s control, such that the instrumental variable will not be correlated with any individual’s choices/omitted variables.\n\nThe spirit is similar to RCT’s randomization\n\nPoint 2 is called relevance requirement: though beyond an individual’s control, the instrumental variable should still affect the individual’s \\(X\\), causing some exogenous variations in \\(X\\)."
  },
  {
    "objectID": "Week8-Lecture2.html#a-classic-example-of-instrumental-variable",
    "href": "Week8-Lecture2.html#a-classic-example-of-instrumental-variable",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "Return of Military Service to Lifetime Income1\n\\[\nIncome = \\beta_0 + \\beta_1MilitaryService + \\epsilon\n\\]\n\nOLS suffers from endogeneity problems, for example\n\nindividual ability correlates with military service and affects income\nindividual health status correlates with military service and affects income\n\nA draft lottery was used to determine if a soldier with a certain birthday goes to the war.\nThe date of birth (\\(z\\)) is an instrumental variable\n\nis correlated with military service: \\(cov(z,x) \\neq 0\\)\nbut does not directly affect income: \\(cov(z,\\epsilon) = 0\\)"
  },
  {
    "objectID": "Week8-Lecture2.html#iv-requirement-i-exogeneity",
    "href": "Week8-Lecture2.html#iv-requirement-i-exogeneity",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "Exogeneity requires that \\(z\\) should only affect \\(Y\\) through \\(X\\), but not directly affect \\(Y\\).\nThe instrumental variable should be beyond an individual’s control. Because omitted variable bias is often caused by individual’s own selection, instrumental variables are thus not correlated with omitted variable bias."
  },
  {
    "objectID": "Week8-Lecture2.html#iv-requirement-ii-relevance",
    "href": "Week8-Lecture2.html#iv-requirement-ii-relevance",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "The instrumental variable must be sufficiently correlated with \\(x\\).\nIf the correlation between \\(z\\) and \\(x\\) is too small, we have a weak IV problem.\nFor more mathematical details of the weak IV issue, refer to this resource."
  },
  {
    "objectID": "Week8-Lecture2.html#more-examples-of-ivs",
    "href": "Week8-Lecture2.html#more-examples-of-ivs",
    "title": "Class 16 Instrumental Variables",
    "section": "",
    "text": "Can you come up with IV candidates for the following causation questions?\n\nCOVID-19 cases =&gt; Uber Driver Supply\n\nnew cases from neighboring cities\nnew cases from overseas\n\nNumber of restaurants on UberEat =&gt; Number of orders on UberEat\n\nclose down of restaurants due to government inspections\n\nRetail price =&gt; Sales\n\nwholesale price\ncosts of raw materials\nCOGS\nHausman instruments: the prices of the same product in other markets"
  },
  {
    "objectID": "Week8-Lecture2.html#solving-endogeneity-using-iv",
    "href": "Week8-Lecture2.html#solving-endogeneity-using-iv",
    "title": "Class 16 Instrumental Variables",
    "section": "2.1 Solving Endogeneity Using IV",
    "text": "2.1 Solving Endogeneity Using IV\n\nGiven an endogenous OLS regression,\n\n\\[\n    y_{i}=X_{i} \\beta+\\varepsilon_{i}, \\quad \\operatorname{cov}\\left(X_{i}, \\varepsilon_{i}\\right) \\neq 0\n\\]\n\nFind instrumental variables \\(Z_i\\) that do not (directly) inﬂuence \\(y_i\\) , but are correlated with \\(X_i\\)"
  },
  {
    "objectID": "Week8-Lecture2.html#two-stage-least-squares-stage-1",
    "href": "Week8-Lecture2.html#two-stage-least-squares-stage-1",
    "title": "Class 16 Instrumental Variables",
    "section": "2.2 Two-Stage Least Squares: Stage 1",
    "text": "2.2 Two-Stage Least Squares: Stage 1\n\nRun a regression with X ~ Z. The predicted \\(X\\) is predicted by Z, which should be uncorrelated with the error term \\(\\epsilon\\).\n\n\\(\\hat{X}\\) (predicted \\(X\\) from \\(Z\\)) is exogenous, because \\(Z\\) is exogenous\nAll endogenous parts are now left in the error term in the first-stage regression \\(\\epsilon_{i}\\)\n\n\n\\[\nX_{i}=Z_{i}\\eta+\\epsilon_{i}\n\\]"
  },
  {
    "objectID": "Week8-Lecture2.html#two-stage-least-squares-stage-2",
    "href": "Week8-Lecture2.html#two-stage-least-squares-stage-2",
    "title": "Class 16 Instrumental Variables",
    "section": "2.3 Two-Stage Least Squares: Stage 2",
    "text": "2.3 Two-Stage Least Squares: Stage 2\n\nRun a regression with \\(Y\\) ~ \\(\\hat{X}\\): now \\(\\hat{X}\\) is uncorrelated with the error term and thus we can get causal inference from the second stage regression.\n\n\\[\ny_{i}=\\hat{X} \\beta+\\varepsilon_{i}, \\quad \\operatorname{cov}\\left(\\hat{X}_{i}, \\varepsilon_{i}\\right) = 0\n\\]"
  },
  {
    "objectID": "Week8-Lecture2.html#causal-impact-of-covid-19",
    "href": "Week8-Lecture2.html#causal-impact-of-covid-19",
    "title": "Class 16 Instrumental Variables",
    "section": "3.1 Causal Impact of COVID-19",
    "text": "3.1 Causal Impact of COVID-19\n\nThe COVID-19 pandemic has brought unprecedented disruptions to many industries, and platform businesses, especially sharing economy platforms, are among the most disrupted ones.\nA common data science interview question: how would you evaluate causal impact of COVID-19 on the company’s business and profits?\n\nCan we collect data on the COVID cases and KPI measures, and run an OLS regression to get the causal effect? \\(KPI\\) ~ \\(NumCovid\\)\nWhat would hinder us from causal inference from the above OLS regressions?"
  },
  {
    "objectID": "Week8-Lecture2.html#causal-impact-of-covid-19-on-ubereat-delivery-drivers-labor-supply",
    "href": "Week8-Lecture2.html#causal-impact-of-covid-19-on-ubereat-delivery-drivers-labor-supply",
    "title": "Class 16 Instrumental Variables",
    "section": "3.2 Causal Impact of COVID-19 on UberEat Delivery Drivers’ Labor Supply",
    "text": "3.2 Causal Impact of COVID-19 on UberEat Delivery Drivers’ Labor Supply\n\nIn this case workshop, we will see an application of instrumental variable in evaluating the causal impact of COVID-19 on UberEat delivery drivers’ labor supply decision.\nLet’s take out the Quarto document."
  },
  {
    "objectID": "Week8-Lecture2.html#beyond-the-impact-of-covid-19-on-labor-supply",
    "href": "Week8-Lecture2.html#beyond-the-impact-of-covid-19-on-labor-supply",
    "title": "Class 16 Instrumental Variables",
    "section": "3.3 Beyond the Impact of COVID-19 on Labor Supply",
    "text": "3.3 Beyond the Impact of COVID-19 on Labor Supply\n\nYou can follow this case study and propose similar topics for your term 3 dissertation project, depending on the company you work with.\n\nThe causal impact of COVID-19 on Uber/Bolt drivers’ labor supply\nThe causal impact of COVID-19 on customer demand for offline shopping\netc.\n\nFor similar causal inference interview questions/data science tasks, when RCTs are difficult to implement, instrumental variable method can be a very powerful solution."
  },
  {
    "objectID": "Week8-Lecture2.html#after-class-readings",
    "href": "Week8-Lecture2.html#after-class-readings",
    "title": "Class 16 Instrumental Variables",
    "section": "3.4 After-Class Readings",
    "text": "3.4 After-Class Readings\n\n(optional) Econometrics with R: Instrumental Variables Regression"
  },
  {
    "objectID": "Week8-Lecture2.html#footnotes",
    "href": "Week8-Lecture2.html#footnotes",
    "title": "Class 16 Instrumental Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAngrist, Joshua D., Stacey H. Chen, and Jae Song. “Long-term consequences of Vietnam-era conscription: New estimates using social security data.” American Economic Review 101, no. 3 (2011): 334-38.↩︎"
  },
  {
    "objectID": "R-installR.html",
    "href": "R-installR.html",
    "title": "Install and Setup R",
    "section": "",
    "text": "Uninstall Old R and RStudio\n\n\n\nIf you have used R or RStudio before, please uninstall both R and RStudio and follow the guide below to install the latest versions. Otherwise, Quarto may not work properly on older versions."
  },
  {
    "objectID": "R-installR.html#for-windows-computers",
    "href": "R-installR.html#for-windows-computers",
    "title": "Install and Setup R",
    "section": "1.1 For Windows computers",
    "text": "1.1 For Windows computers\n\nGo to R’s official website in this link\nClick CRAN under download section\n\n\n\nThese are different mirrors for R download. Basically they store the same installation files but on different servers in different places. Simply click into any mirror.\n\n\n\nClick into download R for Windows for installation files for Windows computers\n\n\n\nDownload and install (1) base and (2) Rtools. It’s recommended to use the default options during the installations.\n\nNotes: The former is the R program, and the latter is the tool to compile R packages.\nIt’s highly recommended to change your system language to English before proceeding, or there could be weird bugs later on.\n\n\n\n\nClick into this link. Download and instsall Quarto CLI plugin."
  },
  {
    "objectID": "R-installR.html#for-mac-computers",
    "href": "R-installR.html#for-mac-computers",
    "title": "Install and Setup R",
    "section": "1.2 For Mac computers",
    "text": "1.2 For Mac computers\n\nGo to R’s official website in this link\nClick CRAN under download section\n\n\n\nThese are different mirrors for R download. Basically they store the same installation files but on different servers in different places. Simply click into any mirror.\n\n\n\nClick into download R for macOS for the download file\n\n\n\nDownload the correct pkg file to install R\n\nif you use Intel based CPU, download the R-4.2.1.pkg\nif you use Apple’s silicon chip such as M1, M1 pro, or M2, download the R-4.2.1-arm64.pkg\nrefer to this link if you don’t know how to check Intel or Apple CPU\n\n\n\n\nInstall Command Line Tools following the steps below. This is essential for R to be able to compile packages so do not skip this step.\n\nOpen terminal app on your mac (the icon is in the screenshot)\n\n\nType the following code xcode-select --install into terminal and hit enter to run the code. Admin passwords may be required to proceed. The code may run for a few minutes. This step is to install MacOS tools that can help compile R packages.\n\nIf your terminal says “xcode-select: error: command line tools are already installed, use”Software Update” to install updates”. It means the needed tool is already installed on your computer. Then there is nothing needed in this step."
  },
  {
    "objectID": "R-installR.html#check-r-and-rstudio-are-properly-installed",
    "href": "R-installR.html#check-r-and-rstudio-are-properly-installed",
    "title": "Install and Setup R",
    "section": "4.1 Check R and RStudio are properly installed",
    "text": "4.1 Check R and RStudio are properly installed\nPlease follow the following steps to make sure you have successfully installed R and RStudio.\n\nStep 1: Launch RStudio from your computer. Check if you see the following screen without any error messages\n\nThen congrats, you have successfully installed R and RStudio!\n\n\nStep 2: As shown in the picture, type 1+1 behind the &gt; and hit enter, check if you see a 2 output\n\nIf there are no error messages and you see exactly the same output, then congrats, you have successfully installed R and RStudio!"
  },
  {
    "objectID": "R-installR.html#check-quarto-is-properly-installed",
    "href": "R-installR.html#check-quarto-is-properly-installed",
    "title": "Install and Setup R",
    "section": "4.2 Check Quarto is properly installed",
    "text": "4.2 Check Quarto is properly installed\n\nStep 1: Click the green plus sign circled below, and select Quarto Document\n\n\nStep 2: Select pdf and click Create.\n\n\nStep 3: Click the Render button. You may be asked to save the qmd file to a location. Pick any location on your computer. R will then render the qmd document and generate a PDF file, named “untitled.pdf” in the same location.\n\n\n\nStep 4: The PDF file should look like below. If you can generate the PDF file without issues, Quarto has successfully run on your computer!"
  },
  {
    "objectID": "R-installR.html#why-do-we-need-to-install-rtools-and-commandline-tools",
    "href": "R-installR.html#why-do-we-need-to-install-rtools-and-commandline-tools",
    "title": "Install and Setup R",
    "section": "5.1 Why do we need to install Rtools and Commandline tools?",
    "text": "5.1 Why do we need to install Rtools and Commandline tools?\nMany R packages are written in R. Since R is an interpreted language, source code written in R doesn’t have to be translated into system-specific machine language before running. However, some R packages have significant portions written in compiled languages, such as C/C++ or Fortran. These languages need accessory software tools to translate (“compile”) their source code into machine language that can run on a particular system.\nPackage developers have two choices when distributing code for compiled languages:\n\nThey can prepare compiled, realdy-to-use “binaries” matched against common systems, so that people can simply download the binaries and directly use their packages code without having to know how to compile it.\nThey can distribute source code (i.e., the raw C++ codes) only, and expect the user to have the right compiler software to build system-specific runnable code themselves. Rtools and Commandline tools are the compliers that do the job, therefore needed as an additional installation step.\n\nOn UNIX/Linux, only source code is distributed and all packages are compiled from source during installation (for packages written entirely in R, this is trivial!). For Windows and Mac, CRAN makes pre-compiled binaries available. On Windows, install.packages() will only install precompiled binaries, unless explicitly forced to install from source (you can read a lot more about this in the R Installation and Administration guide)."
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html",
    "href": "Case-PreliminaryCustomerAnalysis.html",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "",
    "text": "The amount of data created worldwide has been increasing exponentially over the past decade with some estimates placing the total at 59 zettabytes as of 2020 (Statista 2020). Data without analytics, however, is of little value to business decision-makers aiming to improve performance and increase growth. It is therefore no surprise that top-tier consulting companies, analytics ﬁrms and business schools have been promoting the positive returns to greater usage of analytics technology. It also explains why an increasing number of data analytics enthusiasts are willing to pay up to £40k tuition fee (that is 10,000 bubble teas!) to join the prestigious MSc Business Analytics program at the UCL School of Management (hmm, it’s now week 3, too late to ask for a refund!).\nBy identifying patterns and trends in massive amounts of data, business analytics enables organizations to make better decisions and improve performance. Descriptive analytics is the simplest and most widely used type of analytics; it is used to generate key performance indicators (KPIs) and metrics for business reports and dashboards. The latest research shows that, even with the adoption of very simple descriptive analytics, businesses can improve their performance by a large extent — Berman and Israeli (2022) use the synthetic difference-in-differences method to analyze the staggered adoption of a retail analytics dashboard by more than 1,500 e-commerce websites,1 and find an increase of 4%–10% in average weekly revenues postadoption. The increase in revenue is not explained by price changes or advertising optimization. Instead, it is consistent with the addition of customer relationship management, personalization, and prospecting technologies to retailer websites. The adoption and usage of descriptive analytics also increases the diversity of products sold, the number of transactions, the numbers of website visitors and unique customers, and the revenue from repeat customers. These findings are consistent with a complementary effect of descriptive analytics that serve as a monitoring device that helps retailers control additional martech tools and amplify their value. Without using the descriptive dashboard, retailers are unable to reap the benefits associated with these technologies.\nIn practice, businesses use descriptive analytics to assess how well they are performing and if they are on pace to meet business objectives. Business leaders and financial specialists monitor common financial measures generated by descriptive analytics, such as revenue and spending growth on a regular basis. Marketing teams utilize descriptive analytics to analyze the efficacy of marketing campaigns by tracking data such as conversion rates and social media followers, and manage customer relationship by keeping track of customer lifetime values. Manufacturing organizations track indicators such as line throughput and downtime. Descriptive analytics enables everyone in the organization to make more informed decisions that move the business forward. It reveals trends that would otherwise remain buried in raw data, allowing marketing managers to quickly assess how well the firm is operating and identify areas for improvement. Additionally, descriptive analytics enables firms to convey information within departments and to external parties.\nIn the remaining of this case, we will explore (1) how to consolidate multiple databases from various sources using R and (2) how to conduct preliminary customer analysis using descriptive analytics."
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#demographic-information",
    "href": "Case-PreliminaryCustomerAnalysis.html#demographic-information",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "2.1 Demographic Information",
    "text": "2.1 Demographic Information\nKnowing your consumer is a vital concept of running any business. Is the business selling fertilizer to farmers, apparel to teenage girls, or vacations to senior citizens? The distinctions are readily apparent in this comparison.\nDemographics define the qualities of clients. To be successful, business owners must understand the demographics of their clients and the trends or changes that are occurring within those specific traits.\nThe following demographic information is usually of interest to business managers:\nAge: Consumer behavior is strongly influenced by age. Younger consumers are more affluent and willing to spend more on entertainment, fashion, and movies. Seniors spend less on these items; they are less active, spend more time indoors, and require more medical treatment. Additionally, market segments can be defined by age groups. For instance, digital devices such as iPhones are targeted more towards millennials than at seniors. While older adults are increasingly utilizing technology, they remain less digitally savvy than millennials and purchase fewer digital products.\nGender: Gender also matters. Males and females have vastly diverse demands and tastes, which influence their purchasing decisions. As a result, some products are created with a specific gender in mind. Macy’s, Nordstrom, and The Gap all have departments dedicated to teenage girls’ clothes, while Seiko has a specific line of diver watches for men only.\nIncome: Income has a substantial influence on consumer behavior and product purchases. Middle-income customers make purchases with due regard for the utility of money. They do not have unlimited money to spend, and hence the money spent on one item may be used on something else. On the other hand, consumers with higher incomes tend to be less price sensitive and have a higher willingness to pay.\nEducation: Consumers’ level of education has an effect on their impressions of the world around them and on the amount of research they conduct prior to making a purchase. Individuals with a higher level of education will spend more time educating themselves before investing their money. Education has an impact on fashion, film, and television programming. Consumers with a higher level of education can be more distrustful of commercials and the facts offered.\nTesco has collected rich customer demographic information through its loyalty program, Tesco Clubcard membership. In the demograhpics.csv dataset, the data scientist team has the following demographic variables:\n\nID: Customer’s unique identifier\nYear_Birth: Customer’s birth year\nEducation: Customer’s education level\nMarital_Status: Customer’s marital status\nIncome: Customer’s yearly household income\nKidhome: Number of children in customer’s household\nTeenhome: Number of teenagers in customer’s household\nDt_Customer: Date of customer’s enrollment with the company"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#purchase-history",
    "href": "Case-PreliminaryCustomerAnalysis.html#purchase-history",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "2.2 Purchase History",
    "text": "2.2 Purchase History\n“History doesn’t repeat itself, but it often rhymes.”\nThis popular aphorism, frequently (and perhaps incorrectly) attributed to Mark Twain, is frequently invoked to demonstrate that, while past events do not always provide a clear indication of future events, they do provide valuable context. This sentiment is especially true for marketing managers, where a consumer’s purchase history provides invaluable insight into their future purchasing habits.\nTesco’s data engineering team has assembled a cross-sectional customer purchase history data, with variables including\n\nID: Customer’s unique identifier\nMntWines: Amount spent on wine in last 2 years\nMntFruits: Amount spent on fruits in last 2 years\nMntMeatProducts: Amount spent on meat in last 2 years\nMntFishProducts: Amount spent on fish in last 2 years\nMntSweetProducts: Amount spent on sweets in last 2 years\nNumDealsPurchases: Number of purchases made with a discount\nNumWebPurchases: Number of purchases made through the company’s web site\nNumCatalogPurchases: Number of purchases made using a catalogue\nNumStorePurchases: Number of purchases made directly in stores\nNumWebVisitsMonth: Number of visits to company’s web site in the last month\nComplain: 1 if customer complained in the last 2 years, 0 otherwise\nResponse: 1 if customer accepted the offer in the last campaign, 0 otherwise\nRecency: Number of days since customer’s last purchase"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#data-loading",
    "href": "Case-PreliminaryCustomerAnalysis.html#data-loading",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "3.1 Data Loading",
    "text": "3.1 Data Loading\nTo work on the datasets, we first need to load the raw data into R. The demographic information data are stored as csv files. In R, we can use read.csv(filepath) to load the data into R environment.\nFor your convenience, I have stored the demographic.csv and purchase.csv files on my Dropbox. We can directly feed the url links to read.csv() to download and create the dataset.\n\n## use read.csv() to download and load the data, assign it to an R data object\n## header = T argument is to tell read.csv() to keep the dataset header (first row)\n\n# Load demograhpic data, and call it data_demo\ndata_demo &lt;- read.csv(file = \"https://www.dropbox.com/s/a0v38lpydls2emy/demographics.csv?dl=1\",\n                      header = T)\n\n# Load purchase history data, and call it data_purchase\ndata_purchase &lt;- read.csv(file = \"https://www.dropbox.com/s/de435r8zdxydnhg/purchase.csv?dl=1\" , header = T)\n\nAfter running the above code blocks, you should see two datasets in your RStudio environment.\nNow, click into each dataset, take a look, and get a sense of how these two datasets look like."
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#data-consolidation",
    "href": "Case-PreliminaryCustomerAnalysis.html#data-consolidation",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "3.2 Data Consolidation",
    "text": "3.2 Data Consolidation\nIn reality, to accomplish a data analytics task, data scientists often need to collect data from various sources, and assemble them into a larger dataset as needed.\nNow we have two Tesco datasets at hand, and we should assemble them into a larger data frame.\n\nMerge the demographic information into purchase history data. Name the joined data as “data_full”\n\ntry left_join(), right_join(), inner_join(), and full_join().\nDo they give you the same results? Why? When would you get different results?\n\n\n\npacman::p_load(dplyr)\n# left join\ndata_full &lt;- data_purchase %&gt;%\n  left_join(data_demo, by = \"ID\")\n\n\n# right join\ndata_full_right_join &lt;- data_purchase %&gt;%\n  right_join(data_demo, by = \"ID\")\n\n\n# inner_join \ndata_full_inner_join &lt;- data_purchase %&gt;%\n  inner_join(data_demo, by = \"ID\")\n\n\n# full_join \ndata_full_full_join &lt;- data_purchase %&gt;%\n  full_join(data_demo, by = \"ID\")"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#data-types",
    "href": "Case-PreliminaryCustomerAnalysis.html#data-types",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "3.3 Data Types",
    "text": "3.3 Data Types\n\nTask: Check all data types in data_full are correct and as expected\n\n\n# can use str() to get the structure of data\n# Lina covered this in the induction week\nstr(data_full)\n\n'data.frame':   2000 obs. of  22 variables:\n $ ID                 : int  5524 2174 4141 6182 5324 7446 965 6177 4855 5899 ...\n $ MntWines           : int  635 11 426 11 173 520 235 76 14 28 ...\n $ MntFruits          : int  88 1 49 4 43 42 65 10 0 0 ...\n $ MntMeatProducts    : int  546 6 127 20 118 98 164 56 24 6 ...\n $ MntFishProducts    : int  172 2 111 10 46 0 50 3 3 1 ...\n $ MntSweetProducts   : int  88 1 21 3 27 42 49 1 3 1 ...\n $ MntGoldProds       : int  88 6 42 5 15 14 27 23 2 13 ...\n $ NumDealsPurchases  : int  3 2 1 2 5 2 4 2 1 1 ...\n $ NumWebPurchases    : int  8 1 8 2 5 6 7 4 3 1 ...\n $ NumCatalogPurchases: int  10 1 2 0 3 4 3 0 0 0 ...\n $ NumStorePurchases  : int  4 2 10 4 6 10 7 4 2 0 ...\n $ NumWebVisitsMonth  : int  7 5 4 6 5 6 6 8 9 20 ...\n $ Complain           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Response           : int  1 0 0 0 0 0 0 0 1 0 ...\n $ Year_Birth         : int  1957 1954 1965 1984 1981 1967 1971 1985 1974 1950 ...\n $ Education          : chr  \"Graduation\" \"Graduation\" \"Graduation\" \"Graduation\" ...\n $ Marital_Status     : chr  \"Single\" \"Single\" \"Together\" \"Together\" ...\n $ Income             : int  58138 46344 71613 26646 58293 62513 55635 33454 30351 5648 ...\n $ Kidhome            : int  0 1 0 1 1 0 0 1 1 1 ...\n $ Teenhome           : int  0 1 0 0 0 1 1 0 0 1 ...\n $ Dt_Customer        : chr  \"04/09/2012\" \"08/03/2014\" \"21/08/2013\" \"10/02/2014\" ...\n $ Recency            : int  58 38 26 26 94 16 34 32 19 68 ...\n\n\n\nDiscussion: If the variables types are incorrect, think about how would you make it right using dplyr?\n\nFor instance, we can observe that Dt_Customer is of character type, we need to convert it to date type in R.\n\n\n\n# use mutate to overwrite the previous Dt_Customer\n# use as.Date() to make the conversion from character to date type\ndata_full &lt;- data_full %&gt;%\n  mutate(Dt_Customer = as.Date(Dt_Customer, format = \"%d/%m/%Y\" ))\n\nNow, if we check the data type of Dt_Customer using class(), it is date type now!\n\nclass(data_full$Dt_Customer)\n\n[1] \"Date\""
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#missing-values",
    "href": "Case-PreliminaryCustomerAnalysis.html#missing-values",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "3.4 Missing Values",
    "text": "3.4 Missing Values\n\nTasks: Are there any missing values in the data?\n\ntip: use datasummary_skim(), which reports the number of missing values\n\n\n\npacman::p_load(modelsummary)\ndatasummary_skim(data_full)\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\n\nID\n2000\n0\n5599.2\n3242.0\n0.0\n5492.0\n11191.0\n\n\n\nMntWines\n738\n0\n306.1\n338.3\n0.0\n176.5\n1493.0\n\n\n\nMntFruits\n157\n0\n26.4\n39.9\n0.0\n8.0\n199.0\n\n\n\nMntMeatProducts\n532\n0\n167.9\n225.3\n0.0\n68.0\n1725.0\n\n\n\nMntFishProducts\n179\n0\n37.6\n54.6\n0.0\n12.0\n259.0\n\n\n\nMntSweetProducts\n175\n0\n27.5\n41.8\n0.0\n8.0\n263.0\n\n\n\nMntGoldProds\n207\n0\n43.8\n51.7\n0.0\n24.0\n362.0\n\n\n\nNumDealsPurchases\n15\n0\n2.3\n2.0\n0.0\n2.0\n15.0\n\n\n\nNumWebPurchases\n15\n0\n4.1\n2.8\n0.0\n4.0\n27.0\n\n\n\nNumCatalogPurchases\n14\n0\n2.7\n3.0\n0.0\n2.0\n28.0\n\n\n\nNumStorePurchases\n14\n0\n5.8\n3.3\n0.0\n5.0\n13.0\n\n\n\nNumWebVisitsMonth\n15\n0\n5.3\n2.5\n0.0\n6.0\n20.0\n\n\n\nComplain\n2\n0\n0.0\n0.1\n0.0\n0.0\n1.0\n\n\n\nResponse\n2\n0\n0.2\n0.4\n0.0\n0.0\n1.0\n\n\n\nYear_Birth\n59\n0\n1968.8\n12.0\n1893.0\n1970.0\n1996.0\n\n\n\nIncome\n1783\n1\n52139.7\n21492.4\n1730.0\n51518.0\n162397.0\n\n\n\nKidhome\n3\n0\n0.4\n0.5\n0.0\n0.0\n2.0\n\n\n\nTeenhome\n3\n0\n0.5\n0.5\n0.0\n0.0\n2.0\n\n\n\nRecency\n100\n0\n49.2\n29.0\n0.0\n50.0\n99.0\n\n\n\n\n\n\n\n\n\nTasks: Clean missing values in the dataset.\n\ntip: use mean(var, na.rm = T) to get the average income; and then replace missing values in data_full with the average income using replace() function. See below an example or check replace() help file to find out its syntax.\n\n\n\n# the below code generates a vector a with 2 NAs (not relevant to case study)\n# the 2nd and 4th elements are missing values\n# the last line of code replaces any missing value with 2\na &lt;- c(1,NA,3,NA)\nreplace(a, is.na(a), 2)\n\n[1] 1 2 3 2\n\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm = T)))"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#footnotes",
    "href": "Case-PreliminaryCustomerAnalysis.html#footnotes",
    "title": "Descriptive Analytics: Preliminary Customer Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe will cover the difference-in-differences technique to establish causal inference later in the module.↩︎\n“why something occurred” belongs to the scope of causal inference; “forecast what may occur in the future” falls in the scope of predictive analytics.↩︎"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html",
    "href": "Case-PredictiveAnalytics.html",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "",
    "text": "Marketing research has traditionally focused on causal inference. The focus on causation stems from the need to make counterfactual predictions. For example, will increasing advertising expenditure increase demand? Answering this question requires an unbiased estimate of advertising impact on demand. However, the need to make accurate predictions is also important to marketing practices. For example, which consumers to target, which product configuration a consumer is most likely to choose, which version of a banner advertisement will generate more clicks, and what the market shares and actions of competitors are likely to be. All of these are prediction problems. These problems do not require causation; rather, they require models with high out-of-sample predictive accuracy. Predictive analytics (machine learning, ML) tools can address these types of problems.\nMachine learning (ML) refers to the study of methods or algorithms designed to learn the underlying patterns in the data and make predictions based on these patterns. A key characteristic of predictive analytics techniques is their ability to produce accurate out-of-sample predictions. Consider the problem of predicting whether a user will click on an ad. We do not have a comprehensive theory of users’ clicking behavior. Predictive analytics methods can automatically learn which of these factors affect user behavior and how they interact with each other, potentially in a highly non-linear fashion, to derive the best functional form that explains user behavior virtually in real time. Predictive analytics methods typically assume a model or structure to learn, but they use a general class of models that can be very rich.\nPredictive analytics models can be divided into two groups: supervised learning and unsupervised learning. Supervised learning requires input data that has both predictor (independent) variables and a target (dependent) variable whose value is to be estimated. If the goal of an analysis is to predict the value of some variable (e.g., whether customer responds to our marketing offers; whether customers churn at some point in time), then supervised learning is used.\nOn the other hand, unsupervised learning does not identify a target (dependent) variable, but rather treats all of the variables equally. In this case, the goal is not to predict the value of a variable, but rather to look for patterns, groupings, or other ways to characterize the data that may lead to an understanding of the way the data interrelate. Cluster analysis is an example of unsupervised learning, which helps data analysts find customer segments based on provided characteristics.\nIn this case study, we are going to analyze the same dataset as in Week 2 Preliminary Customer Analysis. Our task is to use predictive analytics tools to help Tesco conduct more effective targeted marketing.\nAs a quick recap, the variable definitions are as follows:\nDemographic Variables\n\nID: Customer’s unique identifier\nYear_Birth: Customer’s birth year\nEducation: Customer’s education level\nMarital_Status: Customer’s marital status\nIncome: Customer’s yearly household income\nKidhome: Number of children in customer’s household\nTeenhome: Number of teenagers in customer’s household\nDt_Customer: Date of customer’s enrollment with the company\n\nCustomer Purchase History Data\n\nID: Customer’s unique identifier\nMntWines: Amount spent on wine in last 2 years\nMntFruits: Amount spent on fruits in last 2 years\nMntMeatProducts: Amount spent on meat in last 2 years\nMntFishProducts: Amount spent on fish in last 2 years\nMntSweetProducts: Amount spent on sweets in last 2 years\nNumDealsPurchases: Number of purchases made with a discount\nNumWebPurchases: Number of purchases made through the company’s web site\nNumCatalogPurchases: Number of purchases made using a catalogue\nNumStorePurchases: Number of purchases made directly in stores\nNumWebVisitsMonth: Number of visits to company’s web site in the last month\nComplain: 1 if customer complained in the last 2 years, 0 otherwise\nResponse: 1 if customer accepted the offer in the last campaign, 0 otherwise\nRecency: Number of days since customer’s last purchase"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#break-even-analysis-for-making-marketing-offers",
    "href": "Case-PredictiveAnalytics.html#break-even-analysis-for-making-marketing-offers",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "3.1 Break-Even Analysis for Making Marketing Offers",
    "text": "3.1 Break-Even Analysis for Making Marketing Offers\nFirst, we need to compute the profit margin (the net profit) if a customer responds to the marketing offer and buys from Tesco.\n\ncost_per_offer &lt;- 1.5 + 0.5\n\nCOGS &lt;- 0.6\nprofit_per_customer &lt;- 20 * (1 - COGS)\n\nSecond, compute the break-even response rate for a customer.\n\nThe cost is the marketing offer we send, cost_per_offer\nThe benefit is the profit margin if a customer responds, profit_per_customer\nIn order to break-even, we can calculate the break-even response rate from customers:\n\n\nbreak_even_response &lt;- cost_per_offer/profit_per_customer\nbreak_even_response\n\n[1] 0.25\n\n\nWhich means, only if a customer responds to us with at least 25% response rate can we recover the costs of making an marketing offer."
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#data-wrangling-and-cleaning",
    "href": "Case-PredictiveAnalytics.html#data-wrangling-and-cleaning",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "4.1 Data wrangling and cleaning",
    "text": "4.1 Data wrangling and cleaning\nTasks: Merge the demographic information into purchase history data.\n\npacman::p_load(dplyr)\n# Load both datasets\ndata_purchase &lt;- read.csv(file = \"https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1\", \n                      header = T)\n\ndata_demo &lt;- read.csv(\"https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1\",\n                      header = T)\n\n# Left join demographic data into purchase data\ndata_full &lt;- data_purchase %&gt;%\n  left_join(data_demo, by = \"ID\")\n\n# Handle Missing Values of Income\ndata_full &lt;- data_full %&gt;%\n  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T)))"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#select-featurespredictors",
    "href": "Case-PredictiveAnalytics.html#select-featurespredictors",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "4.2 Select features/predictors",
    "text": "4.2 Select features/predictors\nTasks: select meaningful features/predictors from data_full, named data_full_small\n\nSince ID is customer ID only, so should be removed from final data\nSince Dt_Customer is a character string, which cannot be directly used in the model, we should also remove it\n\n\n# Use select to remove the above two variables\n# Tip: a minus sign before the variable name can remove that variable\n\ndata_full_smaller &lt;- data_full %&gt;%\n  select(-ID) %&gt;%\n  select(-Dt_Customer)"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#construct-a-training-set-and-a-test-set",
    "href": "Case-PredictiveAnalytics.html#construct-a-training-set-and-a-test-set",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "4.3 Construct a training set and a test set",
    "text": "4.3 Construct a training set and a test set\nTasks: randomly divide data_full into a training set and a test set\n\n# use nrow() to count the number of rows in data_full_smaller\nn_rows_data_full &lt;- nrow(data_full_smaller)\n\n# set a seed, so that we can get the same set of results every time we rerun the model\nset.seed(8888)\n\n# use sample() to randomly draw row index from data_full\ntraining_index &lt;- sample(x = 1:n_rows_data_full, # draw from 1 until 2000\n                         size  = 0.7 * n_rows_data_full, # size is 70% of 2000\n                         replace = FALSE) # do not sample with replacement\n\n# think of data_full_smaller as a matrix, we can select rows based on training_index\ndata_training &lt;- data_full_smaller[training_index,]\n\n# minus sign means deselecting rows\ndata_test &lt;- data_full_smaller[-training_index,]"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#train-a-decision-tree",
    "href": "Case-PredictiveAnalytics.html#train-a-decision-tree",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "4.4 Train a decision tree",
    "text": "4.4 Train a decision tree\nTasks: load the rpart and rpart.plot packages. Follow the code examples in the lecture notes and try to train a decision tree on data_training\n\npacman::p_load(rpart,rpart.plot)\n\n# write your codes below from what we learned in class.\n\ndecision_tree &lt;- rpart(\n  formula = Response ~ MntWines + MntFruits + MntMeatProducts + MntFishProducts + \n    MntSweetProducts + MntGoldProds + NumDealsPurchases + NumWebPurchases + NumCatalogPurchases + \n    NumStorePurchases + NumWebVisitsMonth + Complain + Year_Birth + Education + \n    Marital_Status + Income + Kidhome + Teenhome + Recency,\n  data =  data_training,\n  method  = \"anova\" )\n\n\n# visualize the decision tree we have built\nrpart.plot(decision_tree)"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#train-a-random-forest",
    "href": "Case-PredictiveAnalytics.html#train-a-random-forest",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "4.5 Train a random forest",
    "text": "4.5 Train a random forest\nTasks: load the ranger packages. Follow the code examples in the lecture notes and try to train a random forest on data_training\n\npacman::p_load(ranger)\n\n# write your codes below:\n\nset.seed(888)\nrandomforest &lt;- ranger(\n  formula = Response ~  MntWines + MntFruits + MntMeatProducts + MntFishProducts + \n    MntSweetProducts + MntGoldProds + NumDealsPurchases + NumWebPurchases + NumCatalogPurchases + \n    NumStorePurchases + NumWebVisitsMonth + Complain + Year_Birth + Education + \n    Marital_Status + Income + Kidhome + Teenhome + Recency,\n  data = data_training,\n  num.trees = 500\n)\n\n# make prediction on the test set\nprediction_from_randomforest &lt;- predict(randomforest, data_test)\n\n# mutate a new column in data_test for the predicted probability from random forest\ndata_test &lt;- data_test %&gt;%\n  mutate(predicted_prob_randomforest = prediction_from_randomforest$predictions)"
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#blanket-marketing",
    "href": "Case-PredictiveAnalytics.html#blanket-marketing",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "5.1 Blanket marketing",
    "text": "5.1 Blanket marketing\nIf Tesco does blanket marketing and sends marketing offers to all 600 customers in the test set, the total marketing costs would be\n\ntotal_costs_of_mailing_blanket &lt;- cost_per_offer * nrow(data_test)\ntotal_costs_of_mailing_blanket\n\n[1] 1200\n\n\nAnd the total profits from the blanket marketing would be\n\n# below is the total number of responding customers in the test set\nsum(data_test$Response)\n\n[1] 99\n\n# Multiply it with profit_per_customer, we get total profits from the marketing campaign\ntotal_profit_blanket &lt;- sum(data_test$Response) * profit_per_customer\n\nTherefore, the Return on Investment (ROI) on the marketing offer would be\n\n\n\n\n\n\nReturn on Investment\n\n\n\nROI = Net Profits / Initial Investments, which means the return rate of an investment activity.\n\nROI needs to be a positive number in order for the company to make profits from the investment.\n\n\n\n\n# net profits = total_profit_blanket - total_costs_of_mailing_blanket\n# initial investment = total_costs_of_mailing_blanket\n# So ROI is below:\n\nROI_blanket &lt;- (total_profit_blanket - total_costs_of_mailing_blanket)/total_costs_of_mailing_blanket\n\nROI_blanket\n\n[1] -0.34\n\n\nA negative ROI from blanket marketing means, the company makes a loss from sending offers to all customers in the test set. The reason is that, not all customers are responsive to our marketing offers. It does Tesco no good from sending offers to those customers who would not respond anyway."
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#targeted-marketing-using-predictive-analytics",
    "href": "Case-PredictiveAnalytics.html#targeted-marketing-using-predictive-analytics",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "5.2 Targeted marketing using predictive analytics",
    "text": "5.2 Targeted marketing using predictive analytics\nHowever, if Tesco uses the trained decision tree model to conduct targeted marketing, and only target those customers who are predicted to be more responsive than the break-even response rate by decision trees, what would happen to the ROI?\n\n5.2.1 Predict response rate from decision tree model\nFirst, we have already trained the decision tree model, named decision_tree, from the training set. We can predict the probability of test set customer responding to our marketing offer, using predict().\n\n# use predict() to make prediction on the test set\n# Note that prediction_from_decision_tree is already a vector,\n# which we can directly mutate into \nprediction_from_decision_tree &lt;- predict(decision_tree, data_test)\n\n# mutate a new column in data_test for the predicted probability\ndata_test &lt;- data_test %&gt;%\n  mutate(predicted_prob_decisiontree = prediction_from_decision_tree)\n\n\n\n5.2.2 Select customers to target\nTasks: We should only send marketing offers to consumers whose expected or predicted response rate is larger than the break-even response rate. This is called targeted marketing.\n\n# mutate a new binary indicator for whether to target a customer based on predicted prob from decision tree model\ndata_test &lt;- data_test %&gt;%\n  mutate(is_target_decisiontree = ifelse(predicted_prob_decisiontree &gt; break_even_response,1L,0L))\n\n\n\n5.2.3 Compute ROI for decision tree targeted marketing\nFinally, we have decided to send marketing offers to selected responsive customers. We can then compute the ROI for targeted marketing as in the blanket marketing case.\n\n# total marketing costs\ntotal_costs_of_mailing_decisiontree &lt;- cost_per_offer * sum(data_test$is_target_decisiontree)\n\n# total profits from responding customers\ntotal_profit_decisiontree &lt;- sum((data_test%&gt;%filter(is_target_decisiontree==1))$Response) * profit_per_customer\n\n# Compute ROI\nROI_decisiontree&lt;- (total_profit_decisiontree - total_costs_of_mailing_decisiontree)/total_costs_of_mailing_decisiontree\n\nROI_decisiontree\n\n[1] 0.8550725\n\n\nFinally, if Tesco uses random forest, an arguably better supervised learning model, to conduct targeted marketing, we can follow a similar logic as above, and compute the ROI from using random forest.\n\n# mutate a new binary indicator for whether to target a customer based on predicted prob from random forest model\ndata_test &lt;- data_test %&gt;%\n  mutate(is_target_randomforest = ifelse(predicted_prob_randomforest &gt; break_even_response,1,0))\n\n# total marketing costs\ntotal_costs_of_mailing_randomforest &lt;- cost_per_offer * sum(data_test$is_target_randomforest)\n\n# total profits from responding customers\ntotal_profit_randomforest &lt;- sum((data_test%&gt;%filter(is_target_randomforest==1))$Response) * profit_per_customer\n\n# Compute ROI\nROI_randomforest&lt;- (total_profit_randomforest - total_costs_of_mailing_randomforest)/total_costs_of_mailing_randomforest\n\nROI_randomforest\n\n[1] 1.031746\n\n\nPredictive analytics model can help the company boost the marketing ROI by allowing Tesco to target customers who are more likely to respond to the marketing offers than the break-even response rate. By doing so, Tesco saves unnecessary marketing costs on those unresponsive customers and therefore improves its marketing efficiency."
  },
  {
    "objectID": "Case-PredictiveAnalytics.html#footnotes",
    "href": "Case-PredictiveAnalytics.html#footnotes",
    "title": "Improving Marketing Efficiency Using Predictive Analytics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis section is heavily borrowed from Chapter 11 of Handbook of Marketing Analytics. For the full article, you can have assess via UCL library service.↩︎"
  },
  {
    "objectID": "RTip-BLAS.html",
    "href": "RTip-BLAS.html",
    "title": "Speed Up Matrix Operations",
    "section": "",
    "text": "If your codes include lots of vector/matrix operations, especially those computation intensive tasks such as matrix inverse,1 you will probably feel that the default R is slow. Your intuition is likely correct! The reason is that the default R distributions from CRAN make use of the default BLAS/LAPACK implementation for linear algebra operations. The purpose of using such reference BLAS libraries by the development team is good: These implementations are built to be stable and cross platform compatible. However, the price that comes with stability is the sacrifice of speed. Is there a way to tweak your R settings to significantly boost the running speed? The answer is yes, and the steps are actually quite simple. We will review the steps to install highly optimized libraries and benchmark their performance."
  },
  {
    "objectID": "RTip-BLAS.html#how-to-switch-to-veclib-blas",
    "href": "RTip-BLAS.html#how-to-switch-to-veclib-blas",
    "title": "Speed Up Matrix Operations",
    "section": "2.1 How to Switch to VecLib BLAS",
    "text": "2.1 How to Switch to VecLib BLAS\nOn MacOS, the R’s framework path is /Library/Frameworks/R.framework/Resources/lib. To go to the folder, open terminal on your Mac, and enter the following command:\n\ncd /Library/Frameworks/R.framework/Resources/lib\nls -l\n\ntotal 16336\n-rwxrwxr-x  1 root     admin  3988192 24 Jun 11:57 libR.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libR.dylib.dSYM\n-rwxrwxr-x  1 root     admin   193440 24 Jun 11:57 libRblas.0.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRblas.0.dylib.dSYM\nlrwxr-xr-x  1 weimiao  admin       48 28 Aug 21:10 libRblas.dylib -&gt; /opt/homebrew/opt/openblas/lib/libopenblas.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRblas.dylib.dSYM\n-rwxrwxr-x  1 root     admin   154464 24 Jun 11:57 libRblas.vecLib.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRblas.vecLib.dylib.dSYM\n-rwxrwxr-x  1 root     admin  1624976 24 Jun 11:57 libRlapack.0.dylib\nlrwxr-xr-x  1 weimiao  admin       46 28 Aug 21:11 libRlapack.dylib -&gt; /opt/homebrew/opt/openblas/lib/liblapack.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRlapack.dylib.dSYM\n-rw-rw-r--  1 root     admin   157792 24 Jun 11:57 libgcc_s.1.1.dylib\n-rwxrwxr-x  1 root     admin  1865904 24 Jun 11:57 libgfortran.5.dylib\n-rwxrwxr-x  1 root     admin   367040 24 Jun 11:57 libquadmath.0.dylib\n\n\nwhich will change (c) the directory (d) to R’s framework folder. In this folder, you will see a few files by hitting ls:\n\nlibRblas.0.dylib is the default BLAS library.\nlibRblas.vecLib.dylib is the more efficient vecLib BLAS, which we are going to switch to.\nlibRblas.dylib is of alias file type. This means it’s kind of like a shortcut and points to a another file we set. By default, libRblas.dylib is pointed to libRblas.0.dylib, so R uses the default BLAS library. All we need to do is to relink the libRblas.0.dylib to libRblas.vecLib.dylib, such that R will use the vecLib.\n\nTo do so, type the following command in terminal:\n\nln -sf libRblas.vecLib.dylib libRblas.dylib\n\nIf vecLib has issues on your computer3 and you would like to switch back to the default BLAS/LAPACK, simply link the libRblas.dylib back to libRblas.0.dylib, by entering the following command in terminal:\n\nln -sf libRblas.0.dylib libRblas.dylib"
  },
  {
    "objectID": "RTip-BLAS.html#performance-comparison",
    "href": "RTip-BLAS.html#performance-comparison",
    "title": "Speed Up Matrix Operations",
    "section": "2.2 Performance Comparison",
    "text": "2.2 Performance Comparison\nWe will compare the performance before and after switching to vecLib using the following R codes. The code generates a 1000 by 1000 matrix, and obtains the inverse of that matrix. I use microbenchmark package to run the inverse operation for 100 times, and capture the performance metrics.\n\nset.seed(888)\nnd = 1000\na &lt;- matrix(rnorm(nd^2), nd, nd)\nlibrary(microbenchmark)\nmb &lt;- microbenchmark(\n  solve(a),\n  times = 100,\n  unit = \"ms\"\n)\n\nWhen I use the default BLAS/LAPACK, the benchmark result is as follows:\n\n\nUnit: milliseconds\n     expr     min      lq     mean   median       uq      max neval\n solve(a) 383.953 388.517 400.0499 392.0261 402.7341 629.8773   100\n\n\nAfter we switch to vecLib, the benchmarks are as follows. As can be seen, the speed has increased dramatically!\n\n\nUnit: milliseconds\n     expr     min       lq     mean   median       uq      max neval\n solve(a) 27.6078 30.08605 31.93745 30.75814 31.26055 53.32608   100"
  },
  {
    "objectID": "RTip-BLAS.html#footnotes",
    "href": "RTip-BLAS.html#footnotes",
    "title": "Speed Up Matrix Operations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor instance, in my research projects that involve structural modelling, I often need to write my own codes in matrix form to compute the value functions, policy functions, equlibrium computation, etc.↩︎\nApple’s Accelerate framework provides high-performance, energy-efficient computations on the CPU by leveraging its vector-processing capability. For more details, refer to Apple’s documentation here.↩︎\nAs warned by CRAN, “Although fast, it is not under our control and may possibly deliver inaccurate results.”↩︎"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "",
    "text": "The amount of data created worldwide has been increasing exponentially over the past decade with some estimates placing the total at 59 zettabytes as of 2020 (Statista 2020). Data without analytics, however, is of little value to business decision-makers aiming to improve performance and increase growth. It is therefore no surprise that top-tier consulting companies, analytics ﬁrms and business schools have been promoting the positive returns to greater usage of analytics technology. It also explains why an increasing number of data analytics enthusiasts are willing to pay up to £40k tuition fee (that is 10,000 bubble teas!) to join the prestigious MSc Business Analytics program at the UCL School of Management (hmm, it’s now week 3, too late to ask for a refund!).\nBy identifying patterns and trends in massive amounts of data, business analytics enables organizations to make better decisions and improve performance. Descriptive analytics is the simplest and most widely used type of analytics; it is used to generate key performance indicators (KPIs) and metrics for business reports and dashboards. The latest research shows that, even with the adoption of very simple descriptive analytics, businesses can improve their performance by a large extent — Berman and Israeli (2022)2 use the synthetic difference-in-differences method to analyze the staggered adoption of a retail analytics dashboard by more than 1,500 e-commerce websites,3 and find an increase of 4%–10% in average weekly revenues postadoption. The increase in revenue is not explained by price changes or advertising optimization. Instead, it is consistent with the addition of customer relationship management, personalization, and prospecting technologies to retailer websites. The adoption and usage of descriptive analytics also increases the diversity of products sold, the number of transactions, the numbers of website visitors and unique customers, and the revenue from repeat customers. These findings are consistent with a complementary effect of descriptive analytics that serve as a monitoring device that helps retailers control additional martech tools and amplify their value. Without using the descriptive dashboard, retailers are unable to reap the benefits associated with these technologies.\nIn practice, businesses use descriptive analytics to assess how well they are performing and if they are on pace to meet business objectives. Business leaders and financial specialists monitor common financial measures generated by descriptive analytics, such as revenue and spending growth on a regular basis. Marketing teams utilize descriptive analytics to analyze the efficacy of marketing campaigns by tracking data such as conversion rates and social media followers, and manage customer relationship by keeping track of customer lifetime values. Manufacturing organizations track indicators such as line throughput and downtime. Descriptive analytics enables everyone in the organization to make more informed decisions that move the business forward. It reveals trends that would otherwise remain buried in raw data, allowing marketing managers to quickly assess how well the firm is operating and identify areas for improvement. Additionally, descriptive analytics enables firms to convey information within departments and to external parties.\nIn the remaining of this case, we will explore (1) how to consolidate multiple databases from various sources using R and (2) how to conduct preliminary customer analysis using descriptive analytics."
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#demographic-information",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#demographic-information",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "2.1 Demographic Information",
    "text": "2.1 Demographic Information\nKnowing your consumer is a vital concept of running any business. Is the business selling fertilizer to farmers, apparel to teenage girls, or vacations to senior citizens? The distinctions are readily apparent in this comparison.\nDemographics define the qualities of clients. To be successful, business owners must understand the demographics of their clients and the trends or changes that are occurring within those specific traits.\nThe following demographic information is usually of interest to business managers:\nAge: Consumer behavior is strongly influenced by age. Younger consumers are more affluent and willing to spend more on entertainment, fashion, and movies. Seniors spend less on these items; they are less active, spend more time indoors, and require more medical treatment. Additionally, market segments can be defined by age groups. For instance, digital devices such as iPhones are targeted more towards millennials than at seniors. While older adults are increasingly utilizing technology, they remain less digitally savvy than millennials and purchase fewer digital products.\nGender: Gender also matters. Males and females have vastly diverse demands and tastes, which influence their purchasing decisions. As a result, some products are created with a specific gender in mind. Macy’s, Nordstrom, and The Gap all have departments dedicated to teenage girls’ clothes, while Seiko has a specific line of diver watches for men only.\nIncome: Income has a substantial influence on consumer behavior and product purchases. Middle-income customers make purchases with due regard for the utility of money. They do not have unlimited money to spend, and hence the money spent on one item may be used on something else. On the other hand, consumers with higher incomes tend to be less price sensitive and have a higher willingness to pay.\nEducation: Consumers’ level of education has an effect on their impressions of the world around them and on the amount of research they conduct prior to making a purchase. Individuals with a higher level of education will spend more time educating themselves before investing their money. Education has an impact on fashion, film, and television programming. Consumers with a higher level of education can be more distrustful of commercials and the facts offered.\nTesco has collected rich customer demographic information through its loyalty program, Tesco Clubcard membership. In the demograhpics.csv dataset, the data scientist team has the following demographic variables:\n\nID: Customer’s unique identifier\nYear_Birth: Customer’s birth year\nEducation: Customer’s education level\nMarital_Status: Customer’s marital status\nIncome: Customer’s yearly household income\nKidhome: Number of children in customer’s household\nTeenhome: Number of teenagers in customer’s household\nDt_Customer: Date of customer’s enrollment with the company"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#purchase-history",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#purchase-history",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "2.2 Purchase History",
    "text": "2.2 Purchase History\n“History doesn’t repeat itself, but it often rhymes.”\nThis popular aphorism, frequently (and perhaps incorrectly) attributed to Mark Twain, is frequently invoked to demonstrate that, while past events do not always provide a clear indication of future events, they do provide valuable context. This sentiment is especially true for marketing managers, where a consumer’s purchase history provides invaluable insight into their future purchasing habits.\nTesco’s data engineering team has assembled a cross-sectional customer purchase history data, with variables including\n\nID: Customer’s unique identifier\nMntWines: Amount spent on wine in last 2 years\nMntFruits: Amount spent on fruits in last 2 years\nMntMeatProducts: Amount spent on meat in last 2 years\nMntFishProducts: Amount spent on fish in last 2 years\nMntSweetProducts: Amount spent on sweets in last 2 years\nNumDealsPurchases: Number of purchases made with a discount\nNumWebPurchases: Number of purchases made through the company’s web site\nNumCatalogPurchases: Number of purchases made using a catalogue\nNumStorePurchases: Number of purchases made directly in stores\nNumWebVisitsMonth: Number of visits to company’s web site in the last month\nComplain: 1 if customer complained in the last 2 years, 0 otherwise\nResponse: 1 if customer accepted the offer in the last campaign, 0 otherwise\nRecency: Number of days since customer’s last purchase"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#data-loading",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#data-loading",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "3.1 Data Loading",
    "text": "3.1 Data Loading\nTo work on the datasets, we first need to load the raw data into R. The demographic information data are stored as csv files. In R, we can use read.csv(filepath) to load the data into R environment.\nFor your convenience, I have stored the demographic.csv and purchase.csv files on my Dropbox. We can directly feed the url links to read.csv() to download and create the dataset.\nAfter running the above code blocks, you should see two datasets in your RStudio environment.\nNow, click into each dataset, take a look, and get a sense of how these two datasets look like."
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#data-consolidation",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#data-consolidation",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "3.2 Data Consolidation",
    "text": "3.2 Data Consolidation\nIn reality, to accomplish a data analytics task, data scientists often need to collect data from various sources, and assemble them into a larger dataset as needed.\nNow we have two Tesco datasets at hand, and we should assemble them into a larger data frame.\n\nMerge the demographic information into purchase history data. Name the joined data as “data_full”\n\ntry left_join(), right_join(), inner_join(), and full_join().\nDo they give you the same results? Why? When would you get different results?"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#data-types",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#data-types",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "3.3 Data Types",
    "text": "3.3 Data Types\n\nTask: Check all data types in data_full are correct and as expected\n\n\nDiscussion: If the variables types are incorrect, think about how would you make it right using dplyr?"
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#missing-values",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#missing-values",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "3.4 Missing Values",
    "text": "3.4 Missing Values\n\nTasks: Are there any missing values in the data?\n\ntip: use datasummary_skim(), which reports the number of missing values\n\n\n\nTasks: Clean missing values in the dataset.\n\ntip: use mean(var, na.rm = T) to get the average income; and then replace missing values in data_full with the average income using replace() function. See below an example or check replace() help file to find out its syntax."
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis-stu.html#footnotes",
    "href": "Case-PreliminaryCustomerAnalysis-stu.html#footnotes",
    "title": "Descriptive Analytics: Preliminary Customer Analysis1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis case was prepared by Dr. Wei Miao, UCL School of Management, University College London for MSIN0094 Marketing Analytics module. This case was developed to provide material for class discussion rather than to illustrate either effective or ineffective handling of a business situation. Names and data may have been disguised to assure confidentiality. Please do not circulate without permission.↩︎\nBerman, Ron, and Ayelet Israeli. 2022. “The Value of Descriptive Analytics: Evidence from Online Retailers.” Marketing Science, March, mksc.2022.1352. https://doi.org/10.1287/mksc.2022.1352.↩︎\nWe will cover the difference-in-differences technique to establish causal inference later in the module.↩︎\n“why something occurred” belongs to the scope of causal inference; “forecast what may occur in the future” falls in the scope of predictive analytics.↩︎\nThis case was prepared by Dr. Wei Miao, UCL School of Management, University College London for MSIN0094 Marketing Analytics module. This case was developed to provide material for class discussion rather than to illustrate either effective or ineffective handling of a business situation. Names and data may have been disguised to assure confidentiality. Please do not circulate without permission.↩︎\nThis case was prepared by Dr. Wei Miao, UCL School of Management, University College London for MSIN0094 Marketing Analytics module. This case was developed to provide material for class discussion rather than to illustrate either effective or ineffective handling of a business situation. Names and data may have been disguised to assure confidentiality. Please do not circulate without permission.↩︎\nThis case was prepared by Dr. Wei Miao, UCL School of Management, University College London for MSIN0094 Marketing Analytics module. This case was developed to provide material for class discussion rather than to illustrate either effective or ineffective handling of a business situation. Names and data may have been disguised to assure confidentiality. Please do not circulate without permission.↩︎"
  },
  {
    "objectID": "R-errors.html",
    "href": "R-errors.html",
    "title": "Troubleshooting R",
    "section": "",
    "text": "In this page, I summarize the common issues running R and the troubleshooting tips. If you run into any R issues, please refer to this page as the first step for any solution."
  },
  {
    "objectID": "R-errors.html#could-not-find-function-error",
    "href": "R-errors.html#could-not-find-function-error",
    "title": "Troubleshooting R",
    "section": "1 ‘could not find function’ Error",
    "text": "1 ‘could not find function’ Error\nThis error arises when (1) an R package is not loaded properly, or (2) due to misspelling of the functions.\nAs you can see in the screenshot below, when we run the code, we get a could not find function “praise” error in the console. This is because we have not loaded the package “praise” to which the praise() function belongs.\n\nWe need to first load the package that contains the function we want to run using library() as shown below:\n\nlibrary(praise)\n\nand then use the function praise() to run it error-free.\n\npraise()\n\n[1] \"You are fantabulous!\"\n\n\nMeanwhile, if we misspell the praise() function, for instance, to prais(), this will also throw up a could not find function “prais” error.\n\nprais()\n\nError in prais(): could not find function \"prais\""
  },
  {
    "objectID": "R-errors.html#object-not-found-error",
    "href": "R-errors.html#object-not-found-error",
    "title": "Troubleshooting R",
    "section": "2 ‘object not found’ error",
    "text": "2 ‘object not found’ error\nThis error occurs when the particular object used in the code is not yet created or existing in the R environment.\nIn the example below we are trying to compute x plus 4. As you can see, we get an ‘object ’x’ not found’ error as the “x” object is not yet created and missing in our R environment.\n\nx+4\n\nError in eval(expr, envir, enclos): object 'x' not found\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nBased on the missing object, go back to your previous codes and check why the object is missing. Did you forget to create it? Did you accidentally delete it?"
  },
  {
    "objectID": "R-errors.html#banner-packages-xxx-required-but-are-not-installed",
    "href": "R-errors.html#banner-packages-xxx-required-but-are-not-installed",
    "title": "Troubleshooting R",
    "section": "3 Banner: “Packages XXX required but are not installed”",
    "text": "3 Banner: “Packages XXX required but are not installed”\nIf you see this banner in your RStudio in the screenshot, it means RStudio detects some packages mentioned in the R Markdown file or R script but are not yet installed on your computer. So it prompts you for installation.\n\nFor instance, in this case, the error message means, knitr and pacman are found in the .Rmd file, but they are not installed, so RStudio is smart enough to prompt you to install.\n\n\n\n\n\n\nSolution\n\n\n\nJust click install button in the banner. RStudio will install the missing components."
  },
  {
    "objectID": "R-errors.html#prompt-window-r-packages-not-up-to-date",
    "href": "R-errors.html#prompt-window-r-packages-not-up-to-date",
    "title": "Troubleshooting R",
    "section": "4 Prompt Window: “R packages not up-to-date”",
    "text": "4 Prompt Window: “R packages not up-to-date”\nSince the R packages are being updated every day (just like our mobile apps, there could be bugs so that the developers have to update R packages to fix those bugs), sometimes, though we may have installed some packages, they are too old to run the functions. And you may see this prompted message:\n\nIt means, the aforementioned packages, base64enc, digest, etc. are outdated and must be updated to function.\n\n\n\n\n\n\nSolution\n\n\n\nClick Yes and RStudio will update all the packages for you."
  },
  {
    "objectID": "R-errors.html#latex-not-found-when-kniting-the-.rmd.qmd-file.",
    "href": "R-errors.html#latex-not-found-when-kniting-the-.rmd.qmd-file.",
    "title": "Troubleshooting R",
    "section": "5 Latex not found when kniting the .rmd/.qmd file.",
    "text": "5 Latex not found when kniting the .rmd/.qmd file.\nIf this is your first time to knit the PDF document, you may see an error message as below:\n\nThe error message has usually told us everything on how to troubleshoot (that’s what an error message is for!).\nIn this screenshot, if you read along, you will find the cause of problem:\n\n[…] LaTex failed to compile,\n\nbecause\n\n[…] No LaTeX installation detected (LaTeX is required to create PDF output).\n\nand the solution is also consideratebly given in this error message:\n\n[…] You should install a LaTeX distribution for your platform: https://www.latex-project.org/get/\n\n\nIf you are not sure, you may install TinyTeX in R: tinytex::install_tinytex()\n\nSo this error message tells us the solution:\n\n\n\n\n\n\nSolution\n\n\n\n\nAlternative solution 1:\nRun the following command in Console\ntinytex::install_tinytex()\nin order to install tinytex, a simplied version of LaTex, on your laptop.\n\n\nAlternative solution 2:\nInstall LaTex on your computer following the R installation guide\n\n\n\nIf you run the command in R Console, you will see that Latex is being installed\n\nAfter this progress bar finishes, you will be able to knit the PDF document!"
  },
  {
    "objectID": "R-errors.html#error-connection-not-found",
    "href": "R-errors.html#error-connection-not-found",
    "title": "Troubleshooting R",
    "section": "6 Error: Connection Not Found",
    "text": "6 Error: Connection Not Found\nConnection Not Found error is usually caused by RStudio being unable to locate your files on your hard disk.\nIf you don’t know how to find the path names for a file on your computer, please refer to\n\nthis link for Windows\n\nthis link for Mac"
  },
  {
    "objectID": "R-errors.html#more-questions",
    "href": "R-errors.html#more-questions",
    "title": "Troubleshooting R",
    "section": "7 More Questions",
    "text": "7 More Questions\nPlease leave a screenshot of error message in the MSTeams channel named “R QnA”. I will keep updating this webpage as more questions come in."
  },
  {
    "objectID": "ClassPreparation.html",
    "href": "ClassPreparation.html",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "We will have a 3-hour session each week and I will aim to cover a new marketing analytics model. Whenever we learn a new technique (e.g., a new statistical model or a new analytics tool), the subsequent week will often start with some warm-up exercise and a seminar workshop (with a case study) for you to review and practice the new technique learned in the previous week. This way, you would have time to digest what you’ve learned and can further reflect on your understanding of the technique by practicing your skills with a real-life application.\nFor instance, in week 1, I will first introduce the concepts of marketing and marketing process, and then will cover the concept of customer lifetime value (CLV) and how to compute CLV with R. In week 2, we will therefore begin with a case study that helps you practice your knowledge of CLV, so you can understand how to use CLV for better marketing decisions in your future projects/jobs.\nIn the remaining time of week 2, I will then introduce a new technique: the dplyr package in R, which helps us clean and manipulate datasets in R. Following a similar logic, then in week 3, we will start with a case study for you to practice the dplyr package. So on and so forth.\n\n\n\n\nEssential: contents core to this week’s materials. All pre-class preparations should be completed before class.\nOptional: supplemental readings for those interested in learning more\n\nAll materials, including the lecture slides and before-class readings will be released a few days before each Thursday’s class.\n\n\n\n\nModule Outline\n\n\n\n\n\n\n\n\n\nWeek\nAnalytics/Methodology Topic\nSubstantive Topic\nCase Study1\nImportant dates\n\n\n\n\nInduction\nIntroduction to R\n\n\n\n\n\n1\nComputation with R\nCustomer lifetime value\nCustomer Lifetime Value\n\n\n\n2\nData wrangling with R\nPreliminary customer analysis\nPreliminary Customer Analysis\n\n\n\n3\nUnsupervised learning\nSegmentation\nImproving Marketing Efficiency with Predictive Analytics I\n\n\n\n4\nSupervised learning\nTargeting\nImproving Marketing Efficiency with Predictive Analytics II\nFriday, 27 October 2023\n1st assignment due\n\n\n5\nRubin Causal Model and Potential Outcome Framework\nCausal Inference\n\n\n\n\n6\nRCT\nPromotion Analytics\nImprove User Engagement on Social Media Platforms using A/B Testings\n\n\n\n7\nLinear regression\nPricing analytics\nOptimizing Marketing Mixes Using Regression Models\n\n\n\n8\nInstrumental variable\nPlatform Design\nEvaluating the Impact of COVID-19 on Ride-sharing Market\nFriday, 24 November 2023\n2nd assignment due\n\n\n9\nQuasi-experiments\nDifference-in-Differences\nRegression Discontinuity Design\n\n\n\n\n10\nCausal forest\nCausal machine learning\n\n\n\n\n\n\n\n\nFriday, 15 December 2023\n3rd assignment due"
  },
  {
    "objectID": "ClassPreparation.html#arrangements-each-week",
    "href": "ClassPreparation.html#arrangements-each-week",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "We will have a 3-hour session each week and I will aim to cover a new marketing analytics model. Whenever we learn a new technique (e.g., a new statistical model or a new analytics tool), the subsequent week will often start with some warm-up exercise and a seminar workshop (with a case study) for you to review and practice the new technique learned in the previous week. This way, you would have time to digest what you’ve learned and can further reflect on your understanding of the technique by practicing your skills with a real-life application.\nFor instance, in week 1, I will first introduce the concepts of marketing and marketing process, and then will cover the concept of customer lifetime value (CLV) and how to compute CLV with R. In week 2, we will therefore begin with a case study that helps you practice your knowledge of CLV, so you can understand how to use CLV for better marketing decisions in your future projects/jobs.\nIn the remaining time of week 2, I will then introduce a new technique: the dplyr package in R, which helps us clean and manipulate datasets in R. Following a similar logic, then in week 3, we will start with a case study for you to practice the dplyr package. So on and so forth."
  },
  {
    "objectID": "ClassPreparation.html#about-the-labels",
    "href": "ClassPreparation.html#about-the-labels",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "Essential: contents core to this week’s materials. All pre-class preparations should be completed before class.\nOptional: supplemental readings for those interested in learning more\n\nAll materials, including the lecture slides and before-class readings will be released a few days before each Thursday’s class."
  },
  {
    "objectID": "ClassPreparation.html#module-outline",
    "href": "ClassPreparation.html#module-outline",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "Module Outline\n\n\n\n\n\n\n\n\n\nWeek\nAnalytics/Methodology Topic\nSubstantive Topic\nCase Study1\nImportant dates\n\n\n\n\nInduction\nIntroduction to R\n\n\n\n\n\n1\nComputation with R\nCustomer lifetime value\nCustomer Lifetime Value\n\n\n\n2\nData wrangling with R\nPreliminary customer analysis\nPreliminary Customer Analysis\n\n\n\n3\nUnsupervised learning\nSegmentation\nImproving Marketing Efficiency with Predictive Analytics I\n\n\n\n4\nSupervised learning\nTargeting\nImproving Marketing Efficiency with Predictive Analytics II\nFriday, 27 October 2023\n1st assignment due\n\n\n5\nRubin Causal Model and Potential Outcome Framework\nCausal Inference\n\n\n\n\n6\nRCT\nPromotion Analytics\nImprove User Engagement on Social Media Platforms using A/B Testings\n\n\n\n7\nLinear regression\nPricing analytics\nOptimizing Marketing Mixes Using Regression Models\n\n\n\n8\nInstrumental variable\nPlatform Design\nEvaluating the Impact of COVID-19 on Ride-sharing Market\nFriday, 24 November 2023\n2nd assignment due\n\n\n9\nQuasi-experiments\nDifference-in-Differences\nRegression Discontinuity Design\n\n\n\n\n10\nCausal forest\nCausal machine learning\n\n\n\n\n\n\n\n\nFriday, 15 December 2023\n3rd assignment due"
  },
  {
    "objectID": "ClassPreparation.html#module-introduction",
    "href": "ClassPreparation.html#module-introduction",
    "title": "Weekly Arrangements",
    "section": "Module Introduction",
    "text": "Module Introduction\n\nWhat you will learn\n\nAn overview of the course topics and requirements\nConcept of marketing and marketing process\nHow marketing analytics can empower marketers in the digital era\n\nAfter-class reading\n\n(optional) The Definitive Guide to Strategic Marketing Planning. Highly recommended if you didn’t take marketing undergrad courses and would like to know more about the conventional marketing process."
  },
  {
    "objectID": "ClassPreparation.html#customer-profitability-and-lifetime-value",
    "href": "ClassPreparation.html#customer-profitability-and-lifetime-value",
    "title": "Weekly Arrangements",
    "section": "Customer Profitability and Lifetime Value",
    "text": "Customer Profitability and Lifetime Value\n\nWhat you will learn\n\nHow to conduct break-even analyses (static and dynamic)\nThe concept of customer life cycle\nHow to compute customer acquisition cost (CAC)\nHow to compute customer lifetime value (CLV)\n\nAfter-class reading\n\n(optional) “Hubspot: How to compute CLV”. This article introduces alternative ways to compute CLV, which are used in many companies.\n(optional) “Important lessons for embracing customer lifetime value”\n(essential) “Common R Programming Errors Faced by Beginners”. This guide will be updated constantly and summarizes the common R errors you may encounter as beginners. For any questions, come here first for solutions."
  },
  {
    "objectID": "ClassPreparation.html#workshop-customer-lifetime-social-value",
    "href": "ClassPreparation.html#workshop-customer-lifetime-social-value",
    "title": "Weekly Arrangements",
    "section": "Workshop: Customer Lifetime Social Value",
    "text": "Workshop: Customer Lifetime Social Value\n\n\n\n\n\n\nPre-class preparation\n\n\n\n\nHBS 9-518-077: Customer Lifetime Social Value (CLSV).\n\nIn the first class this week, we will go through how to compute the customer lifetime value for i-basket in this case study. We will only discuss the CLV calculation part and you can finish the CLSV as an after-class exercise. Please read pages 1 - 8, focusing on pages 5 - 8.\nWhen reading the case, please fill in the table at the beginning of the quarto document to find out all key information, including the variable, the value, and the page where it shows up. This can help you quickly find that information in class.\nPrepare for the following questions, which we will discuss in class.\n\nWhat would be the time unit of analyses? monthly or yearly? How many years of customer lifetime to consider for CLV calculation?\nWhat is the information needed to calculate the net cashflow of a customer in each period? Where can you find the M and c in the CLV formula? Highlight these key numbers so that we can create them in R directly. \nHow do we incorporate customer churn in CLV calculation?\nWhat are the costs i-basket needs to incur to acquire a customer? Based on this information, how to compute the customer acquisition costs (CAC)?\nCan you figure out how to compute CLV with R before class? You can try your best to use what we have learned in Week 1 to compute the CLV for i-basket on your own with R. The exercise qmd file is already given.\n\n\n\n\n\n\nWhat you will learn\n\nHow to apply CLV calculation in a real-life scenario\nHow to extend the concept of CLV to CLSV as needed\nDiscuss how CLV can be used by marketers to guide marketing decisions\n\nAfter-class reading\n\n(optional) The Dangerous Seduction of the Lifetime Value (LTV) Formula"
  },
  {
    "objectID": "ClassPreparation.html#data-wrangling-with-r-part-i",
    "href": "ClassPreparation.html#data-wrangling-with-r-part-i",
    "title": "Weekly Arrangements",
    "section": "Data Wrangling with R: Part I",
    "text": "Data Wrangling with R: Part I\n\nWhat you will learn\n\nProcess of a typical data analytics task\nHow to use filter, mutate, and arrange for data manipulation with dplyr package in R\n\nAfter-class reading\n\n(essential) Finish the practice exercise on data camp: Data Manipulation with dplyr: 1.Transforming Data with dplyr\n(essential) Cheatsheet for dplyr\n(optional) Python Pandas vs. R Dplyr. In David’s class, you will learn how to use Python Pandas for data wrangling. This cheat sheet provides a one-to-one comparison of the two libraries for your reference."
  },
  {
    "objectID": "ClassPreparation.html#data-wrangling-with-r-part-ii",
    "href": "ClassPreparation.html#data-wrangling-with-r-part-ii",
    "title": "Weekly Arrangements",
    "section": "Data Wrangling with R: Part II",
    "text": "Data Wrangling with R: Part II\n\n\n\n\n\n\nPre-class preparation\n\n\n\n\nRead the variable description of mtcars here\nCase Study: Preliminary Customer Analysis\n\nPlease carefully read the case background before class; we will be discussing the case in class\n\n\n\n\n\nWhat you will learn\n\nThe usage of group_by and join for data aggregation and merge with dplyr package\nHow to use dplyr to conduct preliminary customer analyses\n\nAfter-class reading\n\n(essential) Finish the practice exercise on data camp: Data Manipulation with dplyr: 2.Aggregating Data\n(optional) Go through the tutorial of modelsummary package to learn more features"
  },
  {
    "objectID": "ClassPreparation.html#predictive-analytics-unsupervised-learning",
    "href": "ClassPreparation.html#predictive-analytics-unsupervised-learning",
    "title": "Weekly Arrangements",
    "section": "Predictive Analytics: Unsupervised Learning",
    "text": "Predictive Analytics: Unsupervised Learning\n\nWhat you will learn\n\nImportant concepts in predictive analytics\nConcept of unsupervised learning\nHow to run K-means clustering in R\n\nAfter-class reading\n\n(recommended) K-means Cluster Analysis, which provides more details on the maths behind the K-means clustering"
  },
  {
    "objectID": "ClassPreparation.html#supervised-learning-with-tree-based-models",
    "href": "ClassPreparation.html#supervised-learning-with-tree-based-models",
    "title": "Weekly Arrangements",
    "section": "Supervised Learning with Tree-based Models",
    "text": "Supervised Learning with Tree-based Models\n\nWhat you will learn\n\nImportant concepts of supervised learning\nIntuition behind decision tree and random forest models\nHow to build random forest models in R\n\nAfter-class reading\n\n(optional) Varian, Hal R. “Big data: New tricks for econometrics.” Journal of Economic Perspectives 28, no. 2 (2014): 3-28\n(recommended) Decision tree in R and Random forest in R. Both tutorials introduce the detailed maths behind the two models if you would like to learn more\n(optional) Available machine learning model packages in R. In class, we have learned how to use R packages to run random forest models. This link lists all other machine learning packages that can be used in R. You can learn how to use these models following their manuals."
  },
  {
    "objectID": "ClassPreparation.html#workshop-targeted-marketing-with-predictive-analytics",
    "href": "ClassPreparation.html#workshop-targeted-marketing-with-predictive-analytics",
    "title": "Weekly Arrangements",
    "section": "Workshop: Targeted Marketing with Predictive Analytics",
    "text": "Workshop: Targeted Marketing with Predictive Analytics\n\n\n\n\n\n\nPre-class preparation\n\n\n\n\nCase Study: Improving Marketing Efficiency Using Targeted Marketing\n\nPlease carefully read the case background before class\n\n\n\n\n\nWhat you will learn\n\nThe application predictive analytics (supervised learning) in targeted marketing\nHow targeted marketing improves ROI of marketing campaigns"
  },
  {
    "objectID": "ClassPreparation.html#causal-inference",
    "href": "ClassPreparation.html#causal-inference",
    "title": "Weekly Arrangements",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nWhat you will learn\n\nConcept of causal inference\nConcept of Rubin’s potential outcome framework and various treatment effects\nConcept of fundamental problem of causal inference and basic identity of causal inference\n\nAfter-class reading\n\n(optional) Varian, Hal R. ‘Causal Inference in Economics and Marketing’. Proceedings of the National Academy of Sciences 113, no. 27 (5 July 2016): 7310–15.\n(optional) Angrist, Joshua & Pischke, Jörn-Steffen. (2009). Mostly Harmless Econometrics: An Empiricist’s Companion."
  },
  {
    "objectID": "ClassPreparation.html#randomized-experiments",
    "href": "ClassPreparation.html#randomized-experiments",
    "title": "Weekly Arrangements",
    "section": "Randomized Experiments",
    "text": "Randomized Experiments\n\n\n\n\n\n\nPre-class preparation\n\n\n\n\nThis and next week, we will be conducting t-tests with R in class to estimate treatment effects from randomized experiments. If you’re unfamiliar with t-tests, please go through this Review of Statistics with R before the next week’s class.\n\n\n\n\nWhat you will learn\n\nWhy RCT is the gold standard of causal inference\nSteps to design and conduct an RCT\n\nAfter-class reading\n\n(optional) Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press, 2020., by Ronny Kohavi (Airbnb, formerly Microsoft and Amazon), Diane Tang (Google), and Ya Xu (LinkedIn). This is a very practically oriented guide to experimentation, with many examples relevant to marketing and product management."
  },
  {
    "objectID": "ClassPreparation.html#workshop-improve-user-engagement-on-social-media-platforms-using-ab-testings",
    "href": "ClassPreparation.html#workshop-improve-user-engagement-on-social-media-platforms-using-ab-testings",
    "title": "Weekly Arrangements",
    "section": "Workshop: Improve User Engagement on Social Media Platforms Using A/B Testings",
    "text": "Workshop: Improve User Engagement on Social Media Platforms Using A/B Testings\n\n\n\n\n\n\nPre-class preparation\n\n\n\n\nCase study: Improve User Engagement on Social Media Platforms Using A/B Testings\n\nPlease carefully read the case background before class; we will be discussing the case in class\n\n\n\n\n\nWhat you will learn\n\nThe business model of the advertising industry and mobile marketing industry\nDesign and analyze A-B testing to solve real-life marketing problems\n\nAfter-class reading\n\n(recommended) Test and learn: How a culture of experimentation can help grow your business\n(optional) A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook"
  },
  {
    "objectID": "ClassPreparation.html#linear-regression-model-basics",
    "href": "ClassPreparation.html#linear-regression-model-basics",
    "title": "Weekly Arrangements",
    "section": "Linear Regression Model: Basics",
    "text": "Linear Regression Model: Basics\n\nWhat you will learn\n\nReview of concept for Data Generating Process (DGP) and a model\nThe intuition behind coefficient estimation of linear regression models\nRun linear regression models in R\nHow to interpret the regression coefficients and statistics\n\nAfter-class reading\n\n(highly recommended) Introduction to Econometrics with R, Chapters 4-7. These 4 chapters cover very detailed applied knowledge of linear regressions. Due to limited time, we cannot cover all contents in class, so it would be great if you can take time to go through these chapters thoroughly."
  },
  {
    "objectID": "ClassPreparation.html#linear-regression-model-advanced-topics",
    "href": "ClassPreparation.html#linear-regression-model-advanced-topics",
    "title": "Weekly Arrangements",
    "section": "Linear Regression Model: Advanced Topics",
    "text": "Linear Regression Model: Advanced Topics\n\nWhat you will learn\n\nHow to model non-linear relationship using linear regression\nHow to interpret the coefficients of categorical variables"
  },
  {
    "objectID": "ClassPreparation.html#workshop-marketing-mix-modelling-with-linear-regression",
    "href": "ClassPreparation.html#workshop-marketing-mix-modelling-with-linear-regression",
    "title": "Weekly Arrangements",
    "section": "Workshop: Marketing Mix Modelling with Linear Regression",
    "text": "Workshop: Marketing Mix Modelling with Linear Regression\n\n\n\n\n\n\nPre-class preparation\n\n\n\n\nCase study: Optimizing Marketing Mixes Using Regression Models\n\nPlease carefully read the case background before class; we will be discussing the case in class\n\n\n\n\n\nWhat you will learn\n\nBuild regression-based marketing mix models to solve pricing and promotion optimization problems\n\nAfter-class reading\n\nA complete guide to Marketing Mix Modeling"
  },
  {
    "objectID": "ClassPreparation.html#endogeneity",
    "href": "ClassPreparation.html#endogeneity",
    "title": "Weekly Arrangements",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nWhat you will learn\n\nEndogeneity and its consequence in causal inference\nOmitted variable bias\nReverse causality"
  },
  {
    "objectID": "ClassPreparation.html#instrumental-variables",
    "href": "ClassPreparation.html#instrumental-variables",
    "title": "Weekly Arrangements",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables\n\nWhat you will learn\n\nIntuition of why instrumental variables solve endogeneity problems\nThe two requirements of a valid instrumental variable and how to find good instruments\nApply two-stage least square method to estimate the causal effects using instrumental variables\nApplication of instrumental variables in the marketing and business field\n\nAfter-class reading\n\n(optional) Econometrics with R: Instrumental Variables Regression"
  },
  {
    "objectID": "ClassPreparation.html#difference-in-differences-design",
    "href": "ClassPreparation.html#difference-in-differences-design",
    "title": "Weekly Arrangements",
    "section": "Difference-in-Differences Design",
    "text": "Difference-in-Differences Design\n\nWhat you will learn\n\nConcept of difference-in-differences (DiD) design\nEstimation of causal effects using the DiD design\nApplication of DiD design in the marketing and business field"
  },
  {
    "objectID": "ClassPreparation.html#regression-discontinuity-design",
    "href": "ClassPreparation.html#regression-discontinuity-design",
    "title": "Weekly Arrangements",
    "section": "Regression Discontinuity Design",
    "text": "Regression Discontinuity Design\n\nWhat you will learn\n\nConcept of regression discontinuity design (RDD)\nEstimation of causal effects using the RDD design\nApplication of RDD designs in the marketing and business field\n\nAfter-class reading\n\n(optional) Varian, Hal R. “Causal inference in economics and marketing.” Proceedings of the National Academy of Sciences 113, no. 27 (2016): 7310-7315.\n(optional) Econometrics with R: Quasi-experiments"
  },
  {
    "objectID": "ClassPreparation.html#frontiers-of-marketing-analytics",
    "href": "ClassPreparation.html#frontiers-of-marketing-analytics",
    "title": "Weekly Arrangements",
    "section": "Frontiers of Marketing Analytics",
    "text": "Frontiers of Marketing Analytics\n\nWhat you will learn\n\nCausal machine learning with causal forest\nHeterogeneous treatment effect estimation with causal forest in R\n\nAfter-class reading\n\nEstimate causal effects using ML by Microsoft Research\nAthey, Susan, and Stefan Wager. ‘Estimating Treatment Effects with Causal Forests: An Application’. ArXiv:1902.07409 [Stat], 20 February 2019. http://arxiv.org/abs/1902.07409."
  },
  {
    "objectID": "ClassPreparation.html#footnotes",
    "href": "ClassPreparation.html#footnotes",
    "title": "Weekly Arrangements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCase studies to be discussed in the subsequent week. For example, CLV is introduced in Week 1 and its associated case study “CSLV” will be discussed in Week 2.↩︎"
  },
  {
    "objectID": "Case-CLSV.html",
    "href": "Case-CLSV.html",
    "title": "CLV Case Study",
    "section": "",
    "text": "1 Step 1: Determine time unit of analysis\n\nTime unit of analysis\n\n[…] Because the typical customer contract is renewed annually, all relevant variables will be measured on a yearly basis.\n\nWhen should we use monthly analysis? When there is strong within-year seasonality of customer purchases\n\n\n\n\n\n2 Step 2: Determine number of years\n\n\\(N\\): the number of years over which the customer relationship is assessed\n\n[…] i-basket typically used a five-year horizon for revenue and profitability calculations\n\n\n\nN &lt;- 5\n\n\n\n3 Step 3: Compute profit margin for each period\n\\(CF = M - c\\): gross profit each year\n\nmost customers paid the $99 annual membership fee\n\n\nmembership &lt;- 99\n\n\n40 times each year; each time $100\n\n\nn_visit &lt;- 40\nrevenue_each_visit &lt;- 100\n\n\nprofit margin 7% (COGS 93%)\n\n\nprofit_margin &lt;- 0.07\nM &lt;- membership + revenue_each_visit * n_visit * profit_margin\n\n\nvariable delivery costs each order\n\n\ndeliverycost_each_visit &lt;- 5 + 100 * 0.035\nc &lt;- deliverycost_each_visit * n_visit\n\n\nthe annual CF from customers CF\n\n\nCF &lt;- M - c\nprofit_seq &lt;- rep(CF,N)\nprofit_seq\n\n[1] 39 39 39 39 39\n\n\n\n\n4 Step 4: Compute sequence of retention rate\n\n\\(r\\): retention rate\n\n\n[…] i-basket examined the proportion of customers who renewed their membership from one year to the next (using an average across three years). This proportion was 0.7\n\n\nretention_rate &lt;- 0.7\nretention_seq &lt;- retention_rate ^ (seq(1,N) - 1)\n\n\n\n5 Step 5: Compute sequence of discount factors\n\n\\(k\\): the discount rate\n\n\n[…] A yearly discount rate of 10%\n\n\ndiscount_rate &lt;- 0.1\ndiscount_factor_seq &lt;- (1 / (1+discount_rate)) ^ (seq(1,N))\ndiscount_factor_seq\n\n[1] 0.9090909 0.8264463 0.7513148 0.6830135 0.6209213\n\n\n\n[…] The team decided to take a conservative approach whereby all profits are booked at the end of year.\n\nAll profits earned per customer in year 1 need to be discounted once, the profits earned in year 2 need to be discounted twice, and so on\n\n\n\n\n6 Step 6: Compute customer acquisition costs\n\nCAC = total costs for customer ad clicks + total costs of $15 promo + total costs of free deliveries\nTotal costs for customer clicks\n\n\n[…] 20% of those that signed up for the free trial ended up becoming members\n[…] a fifth of those who clicked on an ad were willing to give the service a try\n\n\nconversion_rate &lt;- 0.2\nclick_through_rate &lt;- 0.2\n\n\nHow many customers need to click the ad to get 1 new customer?\n\n\nn_clicks_1newcustomer &lt;- 1/click_through_rate/conversion_rate\n\n\nTotal costs for customer clicks\n\n\ntotal_cost_clicks &lt;- 0.4 * n_clicks_1newcustomer\ntotal_cost_clicks\n\n[1] 10\n\n\n\ntotal costs of $15 promo for first order each trier customer\n\n\nHow many customers need to try the service to get 1 new customer?\n\n\nn_triers &lt;- 1/conversion_rate\n\n\nWhat is the total promo cost for these trier customers’ first order?\n\n\npromo_first_order_each_trier &lt;- 15\n\ntotal_cost_promo &lt;- promo_first_order_each_trier * (1 - profit_margin) * n_triers\ntotal_cost_promo\n\n[1] 69.75\n\n\n\ntotal costs from free deliveries\n\n\nAssume two visits, the delivery costs for each visit\n\n\ndeliverycost_1st &lt;- 5 + 115 * 0.035\ndeliverycost_2nd &lt;- 5 + 100 * 0.035\ndeliverycost_each_trier &lt;- deliverycost_1st + deliverycost_2nd\n\n\nWe also make a profit each trier\n\n\nprofit_each_trier &lt;-  revenue_each_visit * profit_margin * 2\n\n\nNet delivery costs each trier\n\n\nNetDeliveryCost_each_trier&lt;- deliverycost_each_trier - profit_each_trier\n\ntotal_cost_delivery &lt;- NetDeliveryCost_each_trier * n_triers\n\ntotal_cost_delivery\n\n[1] 17.625\n\n\n\nCAC\n\n\nCAC &lt;- total_cost_clicks + total_cost_promo + total_cost_delivery\nCAC\n\n[1] 97.375\n\n\n\n\n7 Step 7: Compute CLV\n\nCompute the CLV based on the CLV formula (Table A)\n\n\n\nRevenues, variables costs, and profit for the next 5 years\n\n\nprofit_seq\n\n[1] 39 39 39 39 39\n\n\n\nApply retention rate\n\n\nprofit_seq_after_churn &lt;- profit_seq * retention_seq\nprofit_seq_after_churn\n\n[1] 39.0000 27.3000 19.1100 13.3770  9.3639\n\n\n\nApply discount factor\n\n\nprofit_seq_after_churn_discount&lt;- profit_seq_after_churn * discount_factor_seq\n\nprofit_seq_after_churn_discount\n\n[1] 35.454545 22.561983 14.357626  9.136671  5.814245\n\n\n\nCompute CLV by summing up future expected profits\n\n\nsum(profit_seq_after_churn_discount)\n\n[1] 87.32507\n\nsum(profit_seq_after_churn_discount) - CAC\n\n[1] -10.04993"
  },
  {
    "objectID": "Week6-Lecture1.html",
    "href": "Week6-Lecture1.html",
    "title": "Class 11 Application of RCT",
    "section": "",
    "text": "Andrew and Hammond, 2 recently graduated MBA students, were tasked with developing an ad-serving learning algorithm for Vungle, a mobile ad-serving company.\nZain Jaffer, the firm’s CEO, planned to test the developed method in parallel with the existing Vungle algorithm.\nThe hope was that the new algorithm would increase conversion rates and, more specifically, profits (as measured by eRPM).\nTo test this, two conditions (A, Vungle’s existing algorithm, and B, the data science approach) were evaluated in parallel on randomly assigned users.\n\n\n\nThe case examines the results of an A/B test of the two algorithms during the month of June 2014. You will need to determine whether B outperformed A.\n\nHow might Jaffer conclude that B is better than A?\nIf it is, what would the financial benefits be?\nFinally, how long would Jaffer need to wait before declaring a winning algorithm?"
  },
  {
    "objectID": "Week6-Lecture1.html#case-background",
    "href": "Week6-Lecture1.html#case-background",
    "title": "Class 11 Application of RCT",
    "section": "",
    "text": "Andrew and Hammond, 2 recently graduated MBA students, were tasked with developing an ad-serving learning algorithm for Vungle, a mobile ad-serving company.\nZain Jaffer, the firm’s CEO, planned to test the developed method in parallel with the existing Vungle algorithm.\nThe hope was that the new algorithm would increase conversion rates and, more specifically, profits (as measured by eRPM).\nTo test this, two conditions (A, Vungle’s existing algorithm, and B, the data science approach) were evaluated in parallel on randomly assigned users."
  },
  {
    "objectID": "Week6-Lecture1.html#case-core-question",
    "href": "Week6-Lecture1.html#case-core-question",
    "title": "Class 11 Application of RCT",
    "section": "",
    "text": "The case examines the results of an A/B test of the two algorithms during the month of June 2014. You will need to determine whether B outperformed A.\n\nHow might Jaffer conclude that B is better than A?\nIf it is, what would the financial benefits be?\nFinally, how long would Jaffer need to wait before declaring a winning algorithm?"
  },
  {
    "objectID": "Week6-Lecture1.html#company",
    "href": "Week6-Lecture1.html#company",
    "title": "Class 11 Application of RCT",
    "section": "2.1 Company",
    "text": "2.1 Company\n\nBusiness model of Vungle?\n\nPlatform model\n\nWhat are the key players in this mobile video ads market (i.e., Vungle)? Find the info in the case.\n\nAs a comparison, what are the key players in website ads case?\n\n\n\n\n\n\n\n\n“Four parties participated in the in-app mobile advertisement channel. The user of the mobile device (user), the owner of the app being used (publisher), the sponsor of the video ad the user was exposed to (advertiser), and the platform that matched the choice of ad to a specific user (e.g., Vungle).”"
  },
  {
    "objectID": "Week6-Lecture1.html#company-1",
    "href": "Week6-Lecture1.html#company-1",
    "title": "Class 11 Application of RCT",
    "section": "2.2 Company",
    "text": "2.2 Company\n\nHow does Vungle make money? Find the info in the case.\n\n\n\n\n\n\n\n“In most cases, payment was made by the advertiser upon installation. Publishers typically received 60% of the revenues and the ad provider the remaining 40%. See Figure 1 for the conversion funnel depicting how an install is achieved. Of all ad requests, most were served and became impressions. When at least 80% of a video ad was watched, it was considered complete. When the user clicked on the ad to get more information, it was counted as a click. The process could then result in an install.”\n\n\nHow can Vungle improve its revenue using what we’ve learned in Marketing Analytics so far?\n\n\nVungle should use predictive analytics to fill the ads that a customer is most likely to click and install. This is essentially a supervised learning problem."
  },
  {
    "objectID": "Week6-Lecture1.html#customer",
    "href": "Week6-Lecture1.html#customer",
    "title": "Class 11 Application of RCT",
    "section": "2.3 Customer",
    "text": "2.3 Customer\n\nDue to the nature of business model (multi-sided market), who are Vungle’s customers? Find the info in the case.\n\n\n“In the mobile advertising domain, supply was considered to be the slots available for showing ads, and demand consisted of the advertisers willing to buy the supply by placing ads.”"
  },
  {
    "objectID": "Week6-Lecture1.html#collaborators",
    "href": "Week6-Lecture1.html#collaborators",
    "title": "Class 11 Application of RCT",
    "section": "2.4 Collaborators",
    "text": "2.4 Collaborators\n\nWho are the collaborators of Vungle?"
  },
  {
    "objectID": "Week6-Lecture1.html#competitors",
    "href": "Week6-Lecture1.html#competitors",
    "title": "Class 11 Application of RCT",
    "section": "2.5 Competitors",
    "text": "2.5 Competitors\n\n\n\n\n\n\nDirect competitors\nIndirect competitors\nPotential competitors"
  },
  {
    "objectID": "Week6-Lecture1.html#context",
    "href": "Week6-Lecture1.html#context",
    "title": "Class 11 Application of RCT",
    "section": "2.6 Context",
    "text": "2.6 Context\n\n\n\n\n\n\nLegal: GDPR\nTechnological: penetration of mobile phones\n…"
  },
  {
    "objectID": "Week6-Lecture1.html#step-1-decide-on-the-unit-of-randomization",
    "href": "Week6-Lecture1.html#step-1-decide-on-the-unit-of-randomization",
    "title": "Class 11 Application of RCT",
    "section": "3.1 Step 1: Decide on the Unit of Randomization",
    "text": "3.1 Step 1: Decide on the Unit of Randomization\n\nWhat would be the best unit of randomization?\n\n\nThe best level would be user level. User-device level would be too granular and can easily cause crossover effects.\n\n\nHow about website-based online ads, say Google ads?"
  },
  {
    "objectID": "Week6-Lecture1.html#step-2-ensure-no-spillover-and-crossover-effects",
    "href": "Week6-Lecture1.html#step-2-ensure-no-spillover-and-crossover-effects",
    "title": "Class 11 Application of RCT",
    "section": "3.2 Step 2: Ensure No Spillover and Crossover Effects",
    "text": "3.2 Step 2: Ensure No Spillover and Crossover Effects\n\nWhat are the potential problems for spillover and crossover?\n\n\n\nA user may use multiple devices, causing crossover effects\nA user may talk to family members/friends, causing spillover effects.\n\n\n\nHow about website-based online ads, say Google ads?"
  },
  {
    "objectID": "Week6-Lecture1.html#step-3-decide-on-randomization-allocation-scheme",
    "href": "Week6-Lecture1.html#step-3-decide-on-randomization-allocation-scheme",
    "title": "Class 11 Application of RCT",
    "section": "3.3 Step 3: Decide on Randomization Allocation Scheme",
    "text": "3.3 Step 3: Decide on Randomization Allocation Scheme\n\nHow did Vungle implement the randomization scheme? Is it sensible?\n\n\nChan’s team thought it would make sense to direct only 1/16th of the users to the B condition. The other randomly assigned 15/16ths of users would receive an ad based on the existing algorithm (i.e., the A condition)."
  },
  {
    "objectID": "Week6-Lecture1.html#step-4-collect-data",
    "href": "Week6-Lecture1.html#step-4-collect-data",
    "title": "Class 11 Application of RCT",
    "section": "3.4 Step 4: Collect Data",
    "text": "3.4 Step 4: Collect Data\n\nWhat data did Vungle collect?\n\n\nAfter two weeks, B was looking pretty good. Its daily eRPM was on average $0.131 higher than A’s. Would this translate into annual revenues worthy of the necessary data science investment? Exhibits 2 and 3 provide the daily results of the A/B test.\n\n\nCan you do better?\n\n\nUser-level data rather than day-level data would be more granular and can provide higher statistical power."
  },
  {
    "objectID": "Week6-Lecture1.html#step-5-interpreting-results-from-a-field-experiment",
    "href": "Week6-Lecture1.html#step-5-interpreting-results-from-a-field-experiment",
    "title": "Class 11 Application of RCT",
    "section": "3.5 Step 5: Interpreting Results from a Field Experiment",
    "text": "3.5 Step 5: Interpreting Results from a Field Experiment\n\nWhich step is missing in Vungle A-B testing?\n\n\nVungle didn’t do randomization check.\n\n\nHow to draw statistical conclusions from the Exhibits A and B?\n\n\nWe can conduct a paired t-test to statistically test the effectiveness of B."
  },
  {
    "objectID": "Week6-Lecture1.html#step-5-paired-t-test",
    "href": "Week6-Lecture1.html#step-5-paired-t-test",
    "title": "Class 11 Application of RCT",
    "section": "3.6 Step 5: Paired t-test",
    "text": "3.6 Step 5: Paired t-test\n\npacman::p_load(dplyr)\n\n\ndata_vungle &lt;- read.csv(\"https://www.dropbox.com/s/nsxnworjggreh4s/UV6968-XLS-ENG.csv?dl=1\")\n\nt.test((data_vungle%&gt;%filter(Strategy == \"Vungle A\"))$eRPM,\n       (data_vungle%&gt;%filter(Strategy == \"Vungle B\"))$eRPM,\n       paired = TRUE) \n\n\n    Paired t-test\n\ndata:  (data_vungle %&gt;% filter(Strategy == \"Vungle A\"))$eRPM and (data_vungle %&gt;% filter(Strategy == \"Vungle B\"))$eRPM\nt = -3.2837, df = 29, p-value = 0.002677\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.17959566 -0.04173767\nsample estimates:\nmean difference \n     -0.1106667"
  },
  {
    "objectID": "Week6-Lecture1.html#conclusion",
    "href": "Week6-Lecture1.html#conclusion",
    "title": "Class 11 Application of RCT",
    "section": "3.7 Conclusion",
    "text": "3.7 Conclusion\nGuerin was curious to see how the superior condition would be chosen. How would one conclude that B was better than A?"
  },
  {
    "objectID": "Week6-Lecture2.html",
    "href": "Week6-Lecture2.html",
    "title": "Class 12 OLS Regression Basics",
    "section": "",
    "text": "In causal inference, we often care about the expected mean of the outcome variable (\\(Y\\)) conditional on treatment variables (\\(X\\)).\nFor example, in an RCT, Y is the outcome variable (e.g., purchase rate), X is whether or not customers receive the treatment (e.g., BMW ads), then from the basic identity of causal inference, we have \\[\n    ATE = E[Y|X=1] - E[Y|X=0]\n\\]\nQuestion: how can we model the expected mean of outcome variable conditional on \\(X\\), \\(E[Y|X = x]\\)?\n\n\n\n\n\nIf we assume a linear, additive function for \\(E[Y|X=x]\\), we have a simple linear regression model, as follows, \\[\nY_i = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k + \\epsilon_i\n\\]\n\\(y_i\\): Outcome variable/dependent variable/regressand/response variable/LHS variable\n\\(\\beta\\): Regression coefficients/estimates/parameters; \\(\\beta_0\\): intercept\n\\(x_k\\): control variable/independent variable/regressor/explanatory variable/RHS variable\n\nLower case such as \\(x_1\\) usually indicates a single variable while upper case such as \\(X_{ik}\\) indicates several variables\n\n\\(\\epsilon_i\\): error term/disturbance, which has the expected mean of 0, i.e., \\(E[\\epsilon|X] = 0\\)\nIf we take the expectation of \\(Y\\), we have: \\[\nE[Y|X] = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k\n\\]\n\n\n\n\n\nThe term “regression” was coined by Francis Galton to describe a biological phenomenon: The heights of descendants of tall ancestors tend to regress down towards a normal average.\nThe term “regression” was later extended by Udny Yule and Karl Pearson to a more general statistical context (Pearson, 1903).\nIn supervised learning models, “regression” can have different meanings:1\n\nThe regression-class models (OLS, Lasso, Ridge, etc.)\nRegression task\n\nTo establish causal inference, OLS regression model is all we need."
  },
  {
    "objectID": "Week6-Lecture2.html#conditional-mean-in-causal-inference",
    "href": "Week6-Lecture2.html#conditional-mean-in-causal-inference",
    "title": "Class 12 OLS Regression Basics",
    "section": "",
    "text": "In causal inference, we often care about the expected mean of the outcome variable (\\(Y\\)) conditional on treatment variables (\\(X\\)).\nFor example, in an RCT, Y is the outcome variable (e.g., purchase rate), X is whether or not customers receive the treatment (e.g., BMW ads), then from the basic identity of causal inference, we have \\[\n    ATE = E[Y|X=1] - E[Y|X=0]\n\\]\nQuestion: how can we model the expected mean of outcome variable conditional on \\(X\\), \\(E[Y|X = x]\\)?"
  },
  {
    "objectID": "Week6-Lecture2.html#linear-regression-models",
    "href": "Week6-Lecture2.html#linear-regression-models",
    "title": "Class 12 OLS Regression Basics",
    "section": "",
    "text": "If we assume a linear, additive function for \\(E[Y|X=x]\\), we have a simple linear regression model, as follows, \\[\nY_i = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k + \\epsilon_i\n\\]\n\\(y_i\\): Outcome variable/dependent variable/regressand/response variable/LHS variable\n\\(\\beta\\): Regression coefficients/estimates/parameters; \\(\\beta_0\\): intercept\n\\(x_k\\): control variable/independent variable/regressor/explanatory variable/RHS variable\n\nLower case such as \\(x_1\\) usually indicates a single variable while upper case such as \\(X_{ik}\\) indicates several variables\n\n\\(\\epsilon_i\\): error term/disturbance, which has the expected mean of 0, i.e., \\(E[\\epsilon|X] = 0\\)\nIf we take the expectation of \\(Y\\), we have: \\[\nE[Y|X] = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k\n\\]"
  },
  {
    "objectID": "Week6-Lecture2.html#why-the-name-regression",
    "href": "Week6-Lecture2.html#why-the-name-regression",
    "title": "Class 12 OLS Regression Basics",
    "section": "",
    "text": "The term “regression” was coined by Francis Galton to describe a biological phenomenon: The heights of descendants of tall ancestors tend to regress down towards a normal average.\nThe term “regression” was later extended by Udny Yule and Karl Pearson to a more general statistical context (Pearson, 1903).\nIn supervised learning models, “regression” can have different meanings:1\n\nThe regression-class models (OLS, Lasso, Ridge, etc.)\nRegression task\n\nTo establish causal inference, OLS regression model is all we need."
  },
  {
    "objectID": "Week6-Lecture2.html#how-to-run-regression-in-r",
    "href": "Week6-Lecture2.html#how-to-run-regression-in-r",
    "title": "Class 12 OLS Regression Basics",
    "section": "2.1 How to Run Regression in R",
    "text": "2.1 How to Run Regression in R\n\nIn R, there are tons of packages that can run OLS regression.\nIn this module, we will be using the fixest package, because it’s able to estimate high-dimensional fixed effects.\n\n\npacman::p_load(modelsummary,fixest)\n\nOLS_result &lt;- feols( \n   fml = total_spending ~ Income, # Y ~ X\n   data = data_full, # dataset from Tesco\n   )"
  },
  {
    "objectID": "Week6-Lecture2.html#report-regression-results",
    "href": "Week6-Lecture2.html#report-regression-results",
    "title": "Class 12 OLS Regression Basics",
    "section": "2.2 Report Regression Results",
    "text": "2.2 Report Regression Results\n\nmodelsummary(OLS_result,\n    stars = TRUE  # export statistical significance\n  )\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−552.235***\n\n\n\n(20.722)\n\n\nIncome\n0.021***\n\n\n\n(0.000)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.630\n\n\nR2 Adj.\n0.630\n\n\nAIC\n29130.1\n\n\nBIC\n29141.3\n\n\nRMSE\n351.63\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Week6-Lecture2.html#parameter-estimation-univariate-regression-case",
    "href": "Week6-Lecture2.html#parameter-estimation-univariate-regression-case",
    "title": "Class 12 OLS Regression Basics",
    "section": "2.3 Parameter Estimation: Univariate Regression Case",
    "text": "2.3 Parameter Estimation: Univariate Regression Case\n\nLet’s take a univariate regression2 as an example\n\n\\[\n    y = a + b x_1  + \\epsilon\n\\]\n\nFor each guess of a and b, we can compute the error for customer \\(i\\), \\[\ne_i = y_{i}-a-b x_{1i}\n\\]\nWe can compute the sum of squared residuals (SSR) across all customers\n\n\\[\n        SSR =\\sum_{i=1}^{n}\\left(y_{i}-a-b x_{1i}\\right)^{2}\n\\]\n\nObjective of estimation: Search for the unique set of \\(a\\) and \\(b\\) that can minimize the SSR.\nThis estimation method that minimizes SSR is called Ordinary Least Square (OLS)."
  },
  {
    "objectID": "Week6-Lecture2.html#visualization-estimation-of-univariate-regression",
    "href": "Week6-Lecture2.html#visualization-estimation-of-univariate-regression",
    "title": "Class 12 OLS Regression Basics",
    "section": "2.4 Visualization: Estimation of Univariate Regression",
    "text": "2.4 Visualization: Estimation of Univariate Regression\n\nIf in the Tesco dataset, if we regress total spending (Y) on income (X)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nColor\nSum of Squared Error\n\n\n\n\n\\(Y = -552 + 0.06 * X\\)\nPurple\n1.6176047^{13}\n\n\n\\(Y = 0 + 0.004 * X\\)\nRed\n5.093683^{11}\n\n\n\\(Y = -552 + 0.021 * X\\)\nGreen\n2.0205681^{9}"
  },
  {
    "objectID": "Week6-Lecture2.html#multivariate-regression",
    "href": "Week6-Lecture2.html#multivariate-regression",
    "title": "Class 12 OLS Regression Basics",
    "section": "2.5 Multivariate Regression",
    "text": "2.5 Multivariate Regression\n\nThe OLS estimation also applies to multivariate regression with multiple regressors.\n\n\\[\ny_i = b_0 + b_1 x_{1} + ... + b_k x_{k}+\\epsilon_i\n\\]\n\nObjective of estimation: Search for the set of \\(b\\) that can minimize the sum of squared residuals.\n\n\\[\n    SSR= \\sum_{i=1}^{n}\\left(y_{i}-b_0 - b_1 x_{1} - ... - b_k x_{k} \\right)^{2}\n\\]"
  },
  {
    "objectID": "Week6-Lecture2.html#coefficients-interpretation",
    "href": "Week6-Lecture2.html#coefficients-interpretation",
    "title": "Class 12 OLS Regression Basics",
    "section": "3.1 Coefficients Interpretation",
    "text": "3.1 Coefficients Interpretation\n\nNow on your Quarto document, let’s run a new regression, where the DV is \\(total\\_spending\\), and X includes \\(Income\\) and \\(Kidhome\\).\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−316.878***\n\n\n\n(26.972)\n\n\nIncome\n0.019***\n\n\n\n(0.000)\n\n\nKidhome\n−210.613***\n\n\n\n(16.282)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.658\n\n\nR2 Adj.\n0.658\n\n\nAIC\n28971.2\n\n\nBIC\n28988.0\n\n\nRMSE\n337.77\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nControlling for Kidhome / everything else being equal / ceteris paribus, one unit increase in Income increases total spending by 0.019 pounds."
  },
  {
    "objectID": "Week6-Lecture2.html#standard-errors-and-p-values",
    "href": "Week6-Lecture2.html#standard-errors-and-p-values",
    "title": "Class 12 OLS Regression Basics",
    "section": "3.2 Standard Errors and P-values",
    "text": "3.2 Standard Errors and P-values\n\nDue to randomness of the error term, all coefficients estimates follow a \\(t\\) distribution.\nTherefore, we need p-values to check whether the coefficients are statistically different from 0.\nIncome/Kidhome is statistically significant at the 1% level."
  },
  {
    "objectID": "Week6-Lecture2.html#r-squared",
    "href": "Week6-Lecture2.html#r-squared",
    "title": "Class 12 OLS Regression Basics",
    "section": "3.3 R-squared",
    "text": "3.3 R-squared\n\nR-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable or variables in a regression model.\nInterpretation: 65.8% of the variation in Spending can be explained by Income and Kidhome.\nAs the number of variables increases, the \\(R^2\\) will naturally increase.\nIn causal inference tasks, \\(R^2\\) does not mean much."
  },
  {
    "objectID": "Week6-Lecture2.html#footnotes",
    "href": "Week6-Lecture2.html#footnotes",
    "title": "Class 12 OLS Regression Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nML models are developed by computer science; causal inference models are developed by economists.↩︎\nRegressions with a single regressor is called univariate regressions.↩︎"
  },
  {
    "objectID": "Week1-Lecture2.html",
    "href": "Week1-Lecture2.html",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "",
    "text": "How to conduct break-even analyses for marketing campaigns\n\nBreak-even quantity\nNet present value\n\nConcept of customer life cycle\nConcept of and how to compute customer acquisition cost (CAC)\nConcept of and how to compute customer lifetime value (CLV)\nPractice R basic calculations and vector operations in the case study"
  },
  {
    "objectID": "Week1-Lecture2.html#learning-objectives",
    "href": "Week1-Lecture2.html#learning-objectives",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "",
    "text": "How to conduct break-even analyses for marketing campaigns\n\nBreak-even quantity\nNet present value\n\nConcept of customer life cycle\nConcept of and how to compute customer acquisition cost (CAC)\nConcept of and how to compute customer lifetime value (CLV)\nPractice R basic calculations and vector operations in the case study"
  },
  {
    "objectID": "Week1-Lecture2.html#decisions-for-marketing-managers",
    "href": "Week1-Lecture2.html#decisions-for-marketing-managers",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.1 Decisions for Marketing Managers",
    "text": "2.1 Decisions for Marketing Managers\n\nUltimate goal in the marketing process: create value and improve profitability for firms\nAs any marketing activity comes with a cost, marketers need to correctly evaluate whether a campaign creates or destroys value to the company. Such analyses are called break-even analyses (BEA).\nIn this class, we will learn how to conduct BEA from the following points of view\n\nCampaign-centric or customer-centric\nStatic or dynamic\n\n\n\n\n\n\nStatic View\nDynamic View\n\n\nCampaign-centric\nBreak-Even Quantity\nNet Present Value\n\n\nCustomer-centric\n-\nCustomer Lifetime Value"
  },
  {
    "objectID": "Week1-Lecture2.html#break-even-quantity",
    "href": "Week1-Lecture2.html#break-even-quantity",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.2 Break-Even Quantity",
    "text": "2.2 Break-Even Quantity\n\nMarketing managers often use break-even analyses to evaluate the financial feasibility of marketing investments. One commonly used way is to compute the break-even quantity.\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe break-even quantity (BEQ) calculates the number of incremental units the firm needs to sell to cover the cost of the marketing campaign."
  },
  {
    "objectID": "Week1-Lecture2.html#break-even-quantity-steps-and-decision-rule",
    "href": "Week1-Lecture2.html#break-even-quantity-steps-and-decision-rule",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.3 Break-Even Quantity: Steps and Decision Rule",
    "text": "2.3 Break-Even Quantity: Steps and Decision Rule\n\nSteps to conduct break-even analysis\n\nStep 1: Compute the BEQ based on the company’s product demand and production cost structure\nStep 2: Evaluate whether the campaign can guarantee incremental sales to that quantity\n\nThe decision rule\n\nif incremental sales &gt; BEQ, the company makes money so accept the campaign; otherwise, reject the campaign"
  },
  {
    "objectID": "Week1-Lecture2.html#compute-beq",
    "href": "Week1-Lecture2.html#compute-beq",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.4 Compute BEQ",
    "text": "2.4 Compute BEQ\nMarketers often refer to the difference between the price per unit and variable costs per unit as the contribution margin per unit. That is,\n\\[\nContribution Margin Per Unit = Price Per Unit - Variable Costs Per Unit\n\\]\n\nPrice per unit: retail price or transaction price\nVariable costs per unit: Costs of goods sold (COGS)1 + any other variable costs\nMarketing expenditure: total costs of marketing investment\n\nThis gives the second formula for computing BEQ:\n\\[\nBEQ = Marketing Expenditure / Contribution Margin Per Unit\n\\]"
  },
  {
    "objectID": "Week1-Lecture2.html#pineapple-inc-background",
    "href": "Week1-Lecture2.html#pineapple-inc-background",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.5 Pineapple Inc: Background",
    "text": "2.5 Pineapple Inc: Background\n\n\n\n\n\n\n\n\n\nTom Cooper, is looking to launch a series of marketing campaigns to promote its new product PinePhone 14 against its competitor iPhone 14. Tom was a proud graduate from UCL MSc BA program in 2020, and he remembered learning from Marketing Analytics module that, marketers often use break-even analysis to help evaluate different types of marketing decisions.\n\nprice &lt;- 600 # retail price\nquantity &lt;- 10 # sales \nCOGS &lt;- 0.6 # cost of goods sold\nRD_costs &lt;- 100 # operating and marketing costs\nendorsement_fee &lt;- 50 # endorsement\n\nCase objectives:\n\nPractice how to conduct BEA and compute BEQ\nPractice R basic computations and vector operations"
  },
  {
    "objectID": "Week1-Lecture2.html#pineapple-inc-key-information",
    "href": "Week1-Lecture2.html#pineapple-inc-key-information",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.6 Pineapple Inc: Key Information",
    "text": "2.6 Pineapple Inc: Key Information\nFrom the case: The marketing analytics team at Pinapple Inc had applied predictive analytics models on historical sales data and predicted that the sales this year will reach 10 million units at the retail price of £600, without any additional marketing activities. The team had also collected the information on the Cost of Goods Sold of Pineapple 14, which is 60%. The Research and Development (R&D) costs for PinePhone 14 is 100 million pounds.\n\nOpen the .qmd answer sheet downloaded from Moodle. And let’s solve this case using the R basics we learned last week!"
  },
  {
    "objectID": "Week1-Lecture2.html#pineapple-inc-bea",
    "href": "Week1-Lecture2.html#pineapple-inc-bea",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "2.7 Pineapple Inc: BEA",
    "text": "2.7 Pineapple Inc: BEA\n\nQuestion 1: Compute the contribution margin\n\nDo we need to consider R&D costs?\n\n\n\ncontribution_margin &lt;- (1 - COGS) * price\ncontribution_margin\n\n[1] 240\n\n\n\nQuestion 2: Based on the information at hand, should Tom approve the influencer marketing plan?"
  },
  {
    "objectID": "Week1-Lecture2.html#definition-of-npv",
    "href": "Week1-Lecture2.html#definition-of-npv",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "3.1 Definition of NPV",
    "text": "3.1 Definition of NPV\n\nWhen the effect of the marketing campaign is expected to have a long-term effect or when time value of money is important to the question at hand, we need to take the future into account.\n\n\n\n\n\n\n\nDefinition\n\n\n\nNet present value (NPV) is the difference between the present value of cash inflows and the present value of cash outflows over a period of time."
  },
  {
    "objectID": "Week1-Lecture2.html#formula-of-npv",
    "href": "Week1-Lecture2.html#formula-of-npv",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "3.2 Formula of NPV",
    "text": "3.2 Formula of NPV\n\\[\nN P V=-I_{0}+\\frac{CF_{1}}{(1+k)}+\\frac{C F_{2}}{(1+k)^{2}}+\\cdots+\\frac{C F_{n}}{(1+k)^{n}}\n\\]\n\n\\(I_{0}\\) is the initial marketing expense\n\\(C F_{n}\\) is the incremental sales in period \\(n\\): it must be the additional sales due to the marketing campaign\n\\(k\\) is the discount rate: reflects the value of time: the same £1 today is more valuable than £1 tomorrow\nThe decision rule\n\nif NPV &gt; 0, then the marketing campaign can bring in more values to the company, accept\nif NPV &lt; 0, then the marketing campaign will decrease the company’s value, reject"
  },
  {
    "objectID": "Week1-Lecture2.html#pineapple-inc-npv-influencer-marketing-i",
    "href": "Week1-Lecture2.html#pineapple-inc-npv-influencer-marketing-i",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "3.3 Pineapple Inc: NPV Influencer Marketing I",
    "text": "3.3 Pineapple Inc: NPV Influencer Marketing I\nQuestion 3: Based on the information at hand, should Tom approve the influencer marketing plan based on Net Present Value method?\n\nCompute the sequence of monthly cash flows\n\nGenerate a sequence of incremental sales for 12 months (a vector with 12 elements)\n\nHint: use rep(), c(), and vector element-wise multiplication\n\n\n\n\nmonthly_sales_increase_1stmonth &lt;- 0.003\nmonthly_sales_increase_after &lt;- 0.002\n# incremental profit each month\nmonthly_incremental_sales &lt;- c(monthly_sales_increase_1stmonth,\n                     rep(monthly_sales_increase_after,11))\n\nCF &lt;-  monthly_incremental_sales * \n                    quantity * \n                    contribution_margin \n\nThe resulting monthly CFs are: 7.2, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8"
  },
  {
    "objectID": "Week1-Lecture2.html#pineapple-inc-npv-influencer-marketing-ii",
    "href": "Week1-Lecture2.html#pineapple-inc-npv-influencer-marketing-ii",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "3.4 Pineapple Inc: NPV Influencer Marketing II",
    "text": "3.4 Pineapple Inc: NPV Influencer Marketing II\n\nCompute the sequence of discount factors\n\nGenerate a sequence of WACC for 12 months (a vector with 12 elements)\nGenerate a sequence of discount factor for 12 months (a vector with 12 elements)\n\nHint: use seq() to generate geometric sequence with patterns\n\n\n\n\nmonthly_WACC &lt;- 0.1/12 \ndiscount_factor &lt;- (1/(1+monthly_WACC))^c(1:12)\n\nThe resulting monthly discount factors are: 0.9917355, 0.9835394, 0.975411, 0.9673497, 0.9593551, 0.9514265, 0.9435635, 0.9357654, 0.9280319, 0.9203622, 0.9127559, 0.9052124"
  },
  {
    "objectID": "Week1-Lecture2.html#pineapple-inc-npv-influencer-marketing-iii",
    "href": "Week1-Lecture2.html#pineapple-inc-npv-influencer-marketing-iii",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "3.5 Pineapple Inc: NPV Influencer Marketing III",
    "text": "3.5 Pineapple Inc: NPV Influencer Marketing III\n\nCompute the NPV\n\nGenerate a sequence of discounted CFs for 12 months\nSum up all discounted CFs across the 12 months using sum()\nSubtract endorsement fee from the sum to get NPV\n\n\n\nNetPresentValue &lt;- sum(CF * discount_factor) - endorsement_fee\n\n\nThe NPV is 6.9778057"
  },
  {
    "objectID": "Week1-Lecture2.html#from-campaign-centric-to-customer-centric-marketing",
    "href": "Week1-Lecture2.html#from-campaign-centric-to-customer-centric-marketing",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.1 From Campaign-Centric to Customer-Centric Marketing",
    "text": "4.1 From Campaign-Centric to Customer-Centric Marketing\n\n\n\n\n\nCustomer Life Cycle"
  },
  {
    "objectID": "Week1-Lecture2.html#customer-acquisition-cost",
    "href": "Week1-Lecture2.html#customer-acquisition-cost",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.2 Customer Acquisition Cost",
    "text": "4.2 Customer Acquisition Cost\n\n\n\n\n\n\nDefinition\n\n\n\nCustomer Acquisition Cost (CAC) is the cost of winning a customer to purchase a product or service.\n\n\n\nWhy should we care about CAC? Having a new customer may not always be a good thing. For example, no company would spend £500 to acquire a new customer worth £300"
  },
  {
    "objectID": "Week1-Lecture2.html#how-to-acquire-new-customers",
    "href": "Week1-Lecture2.html#how-to-acquire-new-customers",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.3 How to Acquire New Customers",
    "text": "4.3 How to Acquire New Customers\n\nFree sampling/trials"
  },
  {
    "objectID": "Week1-Lecture2.html#how-to-acquire-new-customers-1",
    "href": "Week1-Lecture2.html#how-to-acquire-new-customers-1",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.4 How to Acquire New Customers",
    "text": "4.4 How to Acquire New Customers\n\nReferral Programs: Customer Lifetime Social Value (CLSV, week 2)"
  },
  {
    "objectID": "Week1-Lecture2.html#customer-acquisition-cost-calculation",
    "href": "Week1-Lecture2.html#customer-acquisition-cost-calculation",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.5 Customer Acquisition Cost: Calculation",
    "text": "4.5 Customer Acquisition Cost: Calculation\n\n\n\n\n\n\nDefinition\n\n\n\nWhen the marketing cost can be attributed to each individual customer\n\nCAC = (# of offers needed to acquire 1 customer) * (cost of making a marketing offer)\nCAC = (cost of making a marketing offer) / (customer response rate)\n\n\n\n\nAfter we study predictive analytics later in this module, we will be able to predict response rate for each individual customer and compute individual-specific CAC."
  },
  {
    "objectID": "Week1-Lecture2.html#customer-acquisition-cost-an-example",
    "href": "Week1-Lecture2.html#customer-acquisition-cost-an-example",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.6 Customer Acquisition Cost: An Example",
    "text": "4.6 Customer Acquisition Cost: An Example\nA new Bubble Tea shop in Canary Wharf is contemplating whether or not to attract new customers by sending ads leaflets to nearby residents.\nThe cost of sending a leaflet, which includes production and labor costs, is £0.5.\n\nrandomly sending out leaflets\n\nexpected response rate of 1%\n\nusing names purchased from a marketing agency\n\neach name costs £0.2\nexpected response rate of 4% by analyzing the buying behavior and demographics of current customers\n\n\nCompute the CAC for each choice.\n\ncost_per_random_offer &lt;- 0.5\nresponse_rate_random_offer &lt;- 0.01\n# Following the formula in the previous slide\nCAC_random_offer &lt;- cost_per_random_offer / response_rate_random_offer\nCAC_random_offer\n\n[1] 50\n\n\n\ncost_per_targeted_offer &lt;- 0.5 + 0.2 \nresponse_rate_targeted_offer &lt;- 0.04\nCAC_targeted_offer &lt;- cost_per_targeted_offer/response_rate_targeted_offer\nCAC_targeted_offer\n\n[1] 17.5\n\n\n\n\n\n\n\n\nTips for assignment 1\n\n\n\nThe cost structures for sending out offers are different from the simple example here, but the logic of calculation is the same. Think carefully about how to compute the cost per offer (the components) in the assignment."
  },
  {
    "objectID": "Week1-Lecture2.html#customer-lifetime-value-clv",
    "href": "Week1-Lecture2.html#customer-lifetime-value-clv",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.7 Customer Lifetime Value (CLV)",
    "text": "4.7 Customer Lifetime Value (CLV)\n\n\n\n\n\n\nDefinition\n\n\n\nCustomer lifetime value (CLV or LTV) is the total worth to a business of a customer over the whole period of their relationship.\n\n\n\nThe underlying idea of CLV is essentially NPV, but at the customer level.\n\nThink of acquiring a new customer as an investment in an “asset” that can generate future cash flows"
  },
  {
    "objectID": "Week1-Lecture2.html#clv-calculation",
    "href": "Week1-Lecture2.html#clv-calculation",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.8 CLV: Calculation",
    "text": "4.8 CLV: Calculation\n\\[\n\\mathrm{CLV} = - CAC + \\sum_{t=1}^{N} \\frac{CF_t * r^{(t-1)}}{(1+k)^{t}},\nwhere \\space CF_t = M_t - c_t\n\\]\n\n\\(r\\) is the average annual retention rate; \\(r^{(t-1)}\\) is the cumulative retention rate in year \\(t\\)\n\\(N\\) is the number of years over which the relationship is calculated\n\\(M_{t}\\) is the margin the customer generates in year \\(t\\)\n\\(c_{t}\\) is the expected cost of marketing communications or promotions targeted to the customer in year \\(t\\)\n\\(k\\) is the rate for discounting future cash flows"
  },
  {
    "objectID": "Week1-Lecture2.html#retention-rate",
    "href": "Week1-Lecture2.html#retention-rate",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.9 Retention Rate",
    "text": "4.9 Retention Rate\n\n\n\n\n\n\nDefinition\n\n\n\nThe churn rate, also known as the rate of attrition or customer churn, is the rate at which customers stop doing business with an entity.\n\n\n\nretention rate = 1 - churn rate\nHow to compute individual churn rate: machine learning models to predict the churn rate of an individual customer (Week 4)"
  },
  {
    "objectID": "Week1-Lecture2.html#number-of-years-of-customer-relationship",
    "href": "Week1-Lecture2.html#number-of-years-of-customer-relationship",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.10 Number of Years of Customer Relationship",
    "text": "4.10 Number of Years of Customer Relationship\n\nIf we assume infinite customer economic life, we can simplify the formula into the following using the property of geometric sequence.\n\n\\[\nC L V_{N} = \\sum_{t=1}^{N} \\frac{g r^{(t-1)}}{(1+k)^{t}} =&gt; C L V_{N}=\\mathrm{g} \\cdot \\frac{1-\\left(\\frac{r}{1+k}\\right)^{N}}{1+k-r}  =&gt;\nC L V_{\\infty}=\\frac{g}{(1+k-r)}\n\\]\n\nHowever, most of the time, we are more comfortable to assume finite customer economic life; we need to decide on a cutoff date for CLV calculation\n\nRule 1: until the year when the \\(g = M-c\\) becomes negative\nRule 2: industry’s average customer lifespan"
  },
  {
    "objectID": "Week1-Lecture2.html#after-class",
    "href": "Week1-Lecture2.html#after-class",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "4.11 After-class",
    "text": "4.11 After-class\n\nReview the coding practice in today’s class.\nWe will solve Harvard Business Case: Customer Lifetime Social Value. Remember to read the case background before next week’s class.\nYou can try to solve the case on your own using what we’ve covered today.\nOptional readings are for alternative ways to compute CLV in the industry."
  },
  {
    "objectID": "Week1-Lecture2.html#footnotes",
    "href": "Week1-Lecture2.html#footnotes",
    "title": "Class 2 Break-Even Analysis and Customer Lifetime Value",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaterial and production labor costs for producing a unit of product↩︎"
  },
  {
    "objectID": "Week1-Lecture1.html",
    "href": "Week1-Lecture1.html",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "Hi there, I’m Wei!\nI did my PhD in quant Marketing at NUS Sinagpore\nI love messing with musical instruments, video games, travelling, and food (bubble tea is my fav)!\nMy research focuses on digital marketing, sharing economy, and platform design.\n\n\n\n\n\nDetailed weekly arrangements can be found in this link\n\nAdd bookmark for easier reference\n\nEach week, we have a 3-hour lecture on Thursday, usually divided into two sessions\n\nA methodology session, in which we cover a new marketing analytics model/tool\nA case study workshop session, to practice the tools learned in the previous week\n\nClass recordings are available on Moodle “LectureCast Recordings” section.\n\n\n\n\n\nNo exams; 3 individual assignments, which are similar to case studies, and you will need to use what you learned in class to solve real-life marketing analytics problems.\n\n1st assignment, 30% weight, 1500 words, due on Oct 27, Friday\n2nd assignment, 30% weight, 1500 words, due on Nov 24, Friday\n3rd assignment, 40% weight, 2000 words, due on Dec 15\n\nHow to submit?\n\nOnly submit PDF report rendered from R Quarto with codes printed; don’t submit any other file format or your submission won’t be marked.\nThe quarto-based answer sheets (qmd file) will be given to you\n\nWord count and late submission penalties will be applied by admin. For related queries and EC applications, please directly contact BA admin.\nYour marks will be released in due time (no later than 4 weeks) after the submission date per school requirement.\nWe have random second marking in place to mitigate marking errors. So please refrain from emailing teaching assistants for any re-marking as this is not allowed by school."
  },
  {
    "objectID": "Week1-Lecture1.html#about-me",
    "href": "Week1-Lecture1.html#about-me",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "Hi there, I’m Wei!\nI did my PhD in quant Marketing at NUS Sinagpore\nI love messing with musical instruments, video games, travelling, and food (bubble tea is my fav)!\nMy research focuses on digital marketing, sharing economy, and platform design."
  },
  {
    "objectID": "Week1-Lecture1.html#weekly-arrangements",
    "href": "Week1-Lecture1.html#weekly-arrangements",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "Detailed weekly arrangements can be found in this link\n\nAdd bookmark for easier reference\n\nEach week, we have a 3-hour lecture on Thursday, usually divided into two sessions\n\nA methodology session, in which we cover a new marketing analytics model/tool\nA case study workshop session, to practice the tools learned in the previous week\n\nClass recordings are available on Moodle “LectureCast Recordings” section."
  },
  {
    "objectID": "Week1-Lecture1.html#assignments",
    "href": "Week1-Lecture1.html#assignments",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "No exams; 3 individual assignments, which are similar to case studies, and you will need to use what you learned in class to solve real-life marketing analytics problems.\n\n1st assignment, 30% weight, 1500 words, due on Oct 27, Friday\n2nd assignment, 30% weight, 1500 words, due on Nov 24, Friday\n3rd assignment, 40% weight, 2000 words, due on Dec 15\n\nHow to submit?\n\nOnly submit PDF report rendered from R Quarto with codes printed; don’t submit any other file format or your submission won’t be marked.\nThe quarto-based answer sheets (qmd file) will be given to you\n\nWord count and late submission penalties will be applied by admin. For related queries and EC applications, please directly contact BA admin.\nYour marks will be released in due time (no later than 4 weeks) after the submission date per school requirement.\nWe have random second marking in place to mitigate marking errors. So please refrain from emailing teaching assistants for any re-marking as this is not allowed by school."
  },
  {
    "objectID": "Week1-Lecture1.html#role-of-marketing",
    "href": "Week1-Lecture1.html#role-of-marketing",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.1 Role of Marketing",
    "text": "2.1 Role of Marketing\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinance (finance a company’s business activities)\nAccounting (bookkeeping of transactions)\nOperations (supply chain, manufacturing, inventory management)\nMarketing (directly deal with consumer; value exchange and realization)"
  },
  {
    "objectID": "Week1-Lecture1.html#what-is-marketing-1",
    "href": "Week1-Lecture1.html#what-is-marketing-1",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.2 What is Marketing?",
    "text": "2.2 What is Marketing?\n\nKotler (1991): “Marketing is a social and managerial process by which individuals and groups obtain what they want and need through creating, offering and exchanging products of value with others.”\nBritish Chartered Institute of Marketing (2000s): “Marketing is the management process responsible for identifying, anticipating and satisfying customers’ requirements profitably.”\nAmerican Marketing Association (2017): “Marketing is the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large.”"
  },
  {
    "objectID": "Week1-Lecture1.html#what-is-marketing-a-word-cloud-approach",
    "href": "Week1-Lecture1.html#what-is-marketing-a-word-cloud-approach",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.3 What is Marketing? A Word Cloud Approach",
    "text": "2.3 What is Marketing? A Word Cloud Approach\n[link to code]\n\n\n\n\n\n\n\n\n\n\ninstall.packages('pacman')\npacman::p_load(tm, wordcloud, RColorBrewer,\n               wordcloud2, data.table)\n# generate text corpus\ndf_mkt &lt;- 'Marketing is a social and \nmanagerial process by which individuals and groups \nobtain what they  want and need through creating, \noffering and exchanging products of value with others.\nMarketing is the management process responsible for \nidentifying, anticipating and satisfying customers \nrequirements profitably.\nMarketing is the activity, set of institutions, \nand processes for creating, communicating, delivering, \nand exchanging offerings that have value for customers, \nclients, partners, and society at large.'\ndf_mkt_corpus &lt;- Corpus(VectorSource(df_mkt))\ndf_mkt_corpus &lt;- df_mkt_corpus |&gt; \n  tm_map(removePunctuation) |&gt;\n  tm_map(stripWhitespace) |&gt;\n  tm_map(content_transformer(tolower)) |&gt;\n  tm_map(removeWords, stopwords(\"english\"))\n# Create a document-term-matrix\ndf_mkt_dtm &lt;- TermDocumentMatrix(df_mkt_corpus)\ndf_mkt_matrix &lt;-  as.matrix(df_mkt_dtm)\ndf &lt;- data.table(words = rownames(df_mkt_matrix),\n                 freq = df_mkt_matrix[,1])\n# draw wordcloud\nset.seed(888)\nwordcloud(words = df$words, freq = df$freq, \n          min.freq = 1, max.words=200, random.order=FALSE, \n          colors=brewer.pal(8, \"Dark2\"))\n\n\nMarketing is a management process that creates and exchanges value for the company by selling the right product to the right customer. - Wei, 2023"
  },
  {
    "objectID": "Week1-Lecture1.html#marketing-process",
    "href": "Week1-Lecture1.html#marketing-process",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.4 Marketing Process",
    "text": "2.4 Marketing Process\n\n\n\nMarketing Process\n\n\n\nWe will go through the above concepts quickly in the case of Apple Inc."
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-5-cs",
    "href": "Week1-Lecture1.html#situation-analysis-5-cs",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.5 Situation Analysis: 5 C’s",
    "text": "2.5 Situation Analysis: 5 C’s\n\nAny marketing decision can benefit from a deep understanding of the players within the market ecosystem—your own company, current and potential customers, collaborators and competitors—and the context they interact within: the 5Cs for short.\n\n\n\n\n5Cs of Marketing"
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-competitors",
    "href": "Week1-Lecture1.html#situation-analysis-competitors",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.6 Situation Analysis: Competitors",
    "text": "2.6 Situation Analysis: Competitors\n\nManagers tend to exhibit a supply-side bias, largely paying attention towards more salient direct (industry) competitors.\n\nIndirect competitors are defined by who your customers consider to satisfy the same goals.\nPotential competitors are those who might pose a competitive threat in the future; who possess equivalent resources that would allow them to enter the market\n\n\n\n\n\nSiutation Analysis: Competitors"
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-contextclimate",
    "href": "Week1-Lecture1.html#situation-analysis-contextclimate",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.7 Situation Analysis: Context/Climate",
    "text": "2.7 Situation Analysis: Context/Climate\n\nContext/Climate analysis is a strategic planning method used to assess major external factors that influence the market ecosystem, and is often referred to as PESTLE analysis.\n\n\n\n\nSituation Analysis: Context"
  },
  {
    "objectID": "Week1-Lecture1.html#strategy-stp",
    "href": "Week1-Lecture1.html#strategy-stp",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.8 Strategy: STP",
    "text": "2.8 Strategy: STP\n\nSituation analysis is a critical input into marketing strategy design, the sequential application of the processes of segmentation, targeting, and positioning. Marketing strategy is inherently related to the concepts of the business model and business strategy—who you are serving, with what value proposition, and how you do so.\n\n\n\n\nSegmentation, Targetting, and Positioning"
  },
  {
    "objectID": "Week1-Lecture1.html#tactics-4ps",
    "href": "Week1-Lecture1.html#tactics-4ps",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.9 Tactics: 4P’s",
    "text": "2.9 Tactics: 4P’s\n\nThe marketing mix provides an implementation of your positioning. Segmentation is here applied at the tactical level, to optimally design the marketing mix or 4Ps.\n\n\n\n\n4Ps of Marketing"
  },
  {
    "objectID": "Week1-Lecture1.html#big-data-era",
    "href": "Week1-Lecture1.html#big-data-era",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "3.1 Big Data Era",
    "text": "3.1 Big Data Era\nWith the advancement in information and communication technologies (ICTs), data scientists nowadays are equipped with data analytics tools powerful than ever!\nFirms now have access to enormously rich information trail of customers\n\nDemographic profiles (DoB, gender, ethnicity, income)\nPurchase history (recency, frequency, monetary value, spending behavior)\nOnline browsing and search history (browsing, click through, add to cart, purchase)\nGPS data from mobile phones for offline store visits\nSocial media (location, consumer preference, social network)"
  },
  {
    "objectID": "Week1-Lecture1.html#what-marketing-analytics-can-do",
    "href": "Week1-Lecture1.html#what-marketing-analytics-can-do",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "3.2 What Marketing Analytics Can Do?",
    "text": "3.2 What Marketing Analytics Can Do?"
  },
  {
    "objectID": "Week1-Lecture1.html#unique-position-of-marketing-analytics",
    "href": "Week1-Lecture1.html#unique-position-of-marketing-analytics",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "3.3 Unique Position of Marketing Analytics",
    "text": "3.3 Unique Position of Marketing Analytics"
  },
  {
    "objectID": "LectureSlides.html",
    "href": "LectureSlides.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Lecture slides of subsequent weeks will be released on Monday each week. Stay tuned!"
  },
  {
    "objectID": "Case-COVID-IV.html",
    "href": "Case-COVID-IV.html",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "",
    "text": "The sharing economy has been booming in recent years, leading to a rapid increase in jobs in the “gig” economy. According to Hossain (2020), in the US alone, the sharing economy sector has created 6.23 million jobs with 78 million service providers, and 800 million people engage with it. The transportation sector is one of the most salient beneficiaries of the burgeoning sharing economy. For instance, commuting to work by shared bicycle (e.g., Citi Bike) has become an increasingly popular transportation option (Ford et al. 2019). The ride-sharing service (e.g., Uber) allows drivers to enjoy more flexibility in work, which is proven valuable to drivers and has improved capacity utilization (Cramer and Krueger 2016).\nHowever, the COVID-19 pandemic has brought unprecedented disruptions to many industries, and the transportation industry is among the most disrupted ones. Further, the COVID-19 has raised concerns about the survivability of the sharing economy in general. It is reported that gross bookings on Uber rides were down by 75% in the three months through June 2020, and that Lyft’s April ridership was down by 75% from April 2019.\nUnlike the traditional taxi market, where taxi drivers rent vehicles from taxi companies and then directly provide transportation services to consumers, modern ride-sharing platforms typically serve as the matching intermediary between drivers and passengers. Due to such two-sided market nature, the profitability of modern ride-sharing platforms (and sharing economy in general) highly depends on the interdependence or externality between the two sides of economic agents (Rysman 2009). Therefore, a ride-sharing platform would benefit from the network effect if more drivers work for them. It is thus managerially important for the ride-sharing platform to understand whether COVID-19 has affected drivers’ labor supply patterns and if yes, the magnitude of the effect across drivers and over time.\nIn this case study, we will answer the above question using the instrumental variable method."
  },
  {
    "objectID": "Case-COVID-IV.html#driver-daily-trip-data",
    "href": "Case-COVID-IV.html#driver-daily-trip-data",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "2.1 Driver Daily Trip Data",
    "text": "2.1 Driver Daily Trip Data\nThe data science team has aggregated the raw trip-level data into a driver-day level panel data. This first data set summarizes drivers’ daily shift each day in April 2020, right during the period when the pandemic began in the UK. The data set consists of a random sample of around 4000 drivers in 3 UK cities (anonymized as g, s, and c) in 2020.\n\nCheck the data types of each variable\n\nIdentify any variables that need to be converted\nIdentify all economically meaningful variables\n\n\n\nstr(data_driver)\n\n'data.frame':   93467 obs. of  7 variables:\n $ driver_id   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ booking_date: chr  \"2020-04-01\" \"2020-04-02\" \"2020-04-03\" \"2020-04-04\" ...\n $ is_work     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ income      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ n_order     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ avg_distance: num  0 0 0 0 0 0 0 0 0 0 ...\n $ city        : chr  \"g\" \"g\" \"g\" \"g\" ..."
  },
  {
    "objectID": "Case-COVID-IV.html#data-wrangling",
    "href": "Case-COVID-IV.html#data-wrangling",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\n\nConvert the booking date data from characters to date type using the lubridate package.\n\nFor more details on how to work on date objects in R, refer to this tutorial\n\n\n\npacman::p_load(lubridate)\n\n# function ymd(), dmy(), mdy() can convert characters into date format\n# lubridate is super powerful, refer to the tutorial for more usages\ndata_driver &lt;- data_driver %&gt;%\n  mutate(booking_date = ymd(booking_date) )\n\n# check the class of booking_date now\nclass(data_driver$booking_date)\n\n[1] \"Date\"\n\n\n\nPlease report the summary statistics of the trip data\n\n\npacman::p_load(modelsummary)\n# report the summary statistics below\ndatasummary_skim(data_driver)\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\n\ndriver_id\n3223\n0\n4378.2\n2503.8\n1.0\n4420.0\n8640.0\n\n\n\nis_work\n2\n0\n0.2\n0.4\n0.0\n0.0\n1.0\n\n\n\nincome\n12127\n0\n5.9\n20.7\n0.0\n0.0\n948.2\n\n\n\nn_order\n38\n0\n0.9\n3.0\n0.0\n0.0\n41.0\n\n\n\navg_distance\n9676\n0\n1.7\n5.0\n0.0\n0.0\n181.8"
  },
  {
    "objectID": "Case-COVID-IV.html#covid-19-data",
    "href": "Case-COVID-IV.html#covid-19-data",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "2.3 COVID-19 Data",
    "text": "2.3 COVID-19 Data\nTo measure the severity of COVID-19, the data science team collected daily number of new cases in each city from the government database.\n\ndata_covid &lt;- read.csv(\"https://www.dropbox.com/s/q72b6j8nm8kkla0/data_covid.csv?dl=1\")\n\n\nPlease check the data types and correct the data types as needed\n\nTips: need to convert the booking_date into a date type.\n\n\n\n# check the structure of data below\nstr(data_covid)\n\n'data.frame':   87 obs. of  4 variables:\n $ city                : chr  \"g\" \"g\" \"g\" \"g\" ...\n $ booking_date        : chr  \"2020-04-01\" \"2020-04-02\" \"2020-04-03\" \"2020-04-04\" ...\n $ new_cases           : int  1 1 1 0 3 1 0 2 0 3 ...\n $ other_city_new_cases: int  0 0 0 0 0 0 0 0 0 0 ...\n\n# convert the data types of booking_date below\n\ndata_covid &lt;- data_covid %&gt;%\n  mutate(booking_date = ymd(booking_date))"
  },
  {
    "objectID": "Case-COVID-IV.html#data-wrangling-1",
    "href": "Case-COVID-IV.html#data-wrangling-1",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "2.4 Data Wrangling",
    "text": "2.4 Data Wrangling\n\nJoin the two datasets using appropriate functions\n\nTip: In Week 2 and 3, we have learned dplyr data joining. Please observe the data structure of the two datasets and carefully think about how we should do the data join in this case.\n\n\n\ndata_driver &lt;- data_driver %&gt;%\n  left_join(data_covid, by = c('city' = 'city','booking_date'='booking_date'))"
  },
  {
    "objectID": "Case-COVID-IV.html#key-outcome-variables",
    "href": "Case-COVID-IV.html#key-outcome-variables",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "3.1 Key Outcome Variables",
    "text": "3.1 Key Outcome Variables\nTo facilitate the empirical analysis of drivers’ responses to COVID-19, the data science team has followed the literature (e.g., Farber 2008) and further aggregate trips into a higher level for each driver so that we can measure both extensive margin (i.e., whether to work) and intensive margin (i.e., how much to work) of drivers’ labor supply. As the COVID-19 measures vary at the daily level, the team aggregated the trip level data into driver-day level. Specifically, Uber cares about the following driver-day level KPI measures which serve as the dependent variables in the subsequent empirical analysis.\n\nWhether or not to work, a binary outcome variable which equals 1 if a driver has at least one ride request on the day and 0 otherwise. We can use this variable to measure drivers’ shift decision, i.e., willingness to work on a day, which proxies for the extensive margin of drivers’ labor supply. It is ambiguous ex-ante how the number of new cases affects a driver’s shift decision. On the one hand, more new cases may increase the risk of infection, which decease drivers’ expected wellbeing, and therefore discourage drivers from working on a specific day; on the other hand, fewer drivers on the street suggest less competition among drivers and therefore higher chances of getting a passenger and potentially higher hourly earnings, which may motivate drivers to work. It is important for the ride-sharing company to understand how the severity of COVID-19 affects drivers’ willingness to work, so that the company can adjust their stimulus plans for drivers accordingly.\nTotal number of completed orders, which contain three aspects of information which are of policy and managerial interest. First, the variable can proxy for the length of drivers’ daily labor supply. Conditional on working, if a driver decides to work for longer hours, then we expect the driver to have a larger number of requests/orders. Second, both variables contain information on consumer demand. We expect the total number of requests/orders to decrease if there is a lower demand for ride-sharing service from consumers due to the COVID-19 outbreak. Finally, both variables can measure the intensity of competition among drivers. Keeping the level of demand fixed, the total number of requests/orders would be larger when there are fewer drivers working on the day. Due to the complexity of information contained, ex-ante, it is not straightforward how the COVID-19 measures affect the total number of orders for individual drivers.\nEarnings. Earnings measure the driver’s income from providing ride-sharing services, which is highly correlated with the number of completed orders and total trip distance. It allows us to directly assess the impact of the COVID-19 on drivers’ financial wellbeing.\nAverage trip distance. In our empirical context, drivers cannot reject a booking request once being matched with a passenger, therefore, the trip distance is largely determined by passengers. Since passengers may be reluctant to take long distance trips during the pandemic, we expect a negative impact of the number of new cases on the average trip distance."
  },
  {
    "objectID": "Case-COVID-IV.html#univariate-ols-regression-analyses",
    "href": "Case-COVID-IV.html#univariate-ols-regression-analyses",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "3.2 Univariate OLS regression Analyses",
    "text": "3.2 Univariate OLS regression Analyses\nTo empirically investigate the causal impact of COVID-19 cases on driver behavior, we can use simple OLS regression to regress the labor supply measures of driver \\(i\\), in city \\(j\\), on day \\(t\\) on the COVID-19 measure and other covariates as follows:\n\\[\nOutcome_{ijt}=\\beta_0+\\alpha NewCases_{ijt}+X\\beta+\\varepsilon_{ijt}\n\\]\n\nPlease run three univariate simple linear regressions, with the outcome being the 3 aforementioned dependent variables and explanatory variable being new cases only. Interpret the results.\n\n\npacman::p_load(fixest)\nOLS_is_work &lt;- feols(fml = is_work ~ new_cases,\n      data = data_driver)\nOLS_income &lt;- feols(fml = income ~ new_cases,\n      data = data_driver)\nOLS_n_order &lt;- feols(fml = n_order ~ new_cases,\n      data = data_driver)\nOLS_avg_distance &lt;- feols(fml = avg_distance ~ new_cases,\n      data = data_driver)\n\nmodelsummary(list(OLS_is_work,OLS_income,OLS_n_order,OLS_avg_distance),\n             stars = TRUE)\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\n(Intercept)\n0.182***\n5.957***\n0.885***\n1.656***\n\n\n\n(0.001)\n(0.071)\n(0.010)\n(0.017)\n\n\nnew_cases\n0.0002\n−0.122***\n−0.012*\n−0.003\n\n\n\n(0.0007)\n(0.036)\n(0.005)\n(0.009)\n\n\nNum.Obs.\n93467\n93467\n93467\n93467\n\n\nR2\n0.000001\n0.0001\n0.00006\n0.000001\n\n\nR2 Adj.\n−0.000009\n0.0001\n0.00005\n−0.00001\n\n\nRMSE\n0.39\n20.68\n2.95\n5.04\n\n\nStd.Errors\nIID\nIID\nIID\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-COVID-IV.html#fixed-effect-ols-regressions",
    "href": "Case-COVID-IV.html#fixed-effect-ols-regressions",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "3.3 Fixed Effect OLS Regressions",
    "text": "3.3 Fixed Effect OLS Regressions\nWhat confounding factors do we need to control in the above OLS regressions? Specifically, since this data is a panel data, what fixed effects do you need to control in the OLS regression?\nWe first need to include driver fixed effects to control for driver-specific characteristics that may affect drivers’ labor supply patterns. Such characteristics include, but are not limited to, the driver’s sociodemographic characteristics (e.g., gender and age), the driver’s degree of risk aversion, whether a driver is driving full-time or part-time, and the driver’s innate abilities to search for passengers, etc.\nFor instance, less risk-averse drivers may prefer to work on days when there are more new cases because they expect less competition from peer drivers and potentially higher profitability on such days. Another example is that, full-time drivers can be more subject to the impact of new cases compared to part-time drivers, because full-time drivers’ income largely comes from providing ride-sharing services via the focal company. Driver fixed effects can mitigate such driver-specific time-invariant confounding effects and help us obtain more accurate estimates for our focal explanatory variable NewCases.\nIn addition to driver fixed effects that remove cross-sectional confounding effects across drivers, we also include time fixed effects in Equation (1) to mitigate the intertemporal confounding effects. We consider time fixed effects at the day level. Moreover, given that the local government in each city may have enacted different policies on fighting COVID-19 and/or stimulating economy (e.g., subsidizing drivers) during our data period, we further control for city fixed effects.\n\nFE_is_work &lt;- feols(fml = is_work ~ new_cases|\n                       driver_id + booking_date + city,\n      data = data_driver)\nFE_income &lt;- feols(fml = income ~ new_cases|\n                       driver_id + booking_date + city,\n      data = data_driver)\nFE_n_order &lt;- feols(fml = n_order ~ new_cases|\n                       driver_id + booking_date + city,\n      data = data_driver)\nFE_avg_distance &lt;- feols(fml = avg_distance ~ new_cases|\n                       driver_id + booking_date + city,\n      data = data_driver)\n\nmodelsummary(list(FE_is_work,FE_income,FE_n_order,FE_avg_distance),\n             stars = TRUE)\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\nnew_cases\n0.0002\n0.029\n0.003\n0.002\n\n\n\n(0.0005)\n(0.022)\n(0.003)\n(0.008)\n\n\nNum.Obs.\n93467\n93467\n93467\n93467\n\n\nR2\n0.662\n0.555\n0.607\n0.420\n\n\nR2 Adj.\n0.650\n0.539\n0.593\n0.400\n\n\nR2 Within\n0.000001\n0.00001\n0.000007\n0.0000006\n\n\nR2 Within Adj.\n−0.00001\n−0.0000009\n−0.000004\n−0.00001\n\n\nRMSE\n0.22\n13.79\n1.85\n3.84\n\n\nStd.Errors\nby: driver_id\nby: driver_id\nby: driver_id\nby: driver_id\n\n\nFE: driver_id\nX\nX\nX\nX\n\n\nFE: booking_date\nX\nX\nX\nX\n\n\nFE: city\nX\nX\nX\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-COVID-IV.html#potential-endogeneity",
    "href": "Case-COVID-IV.html#potential-endogeneity",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "4.1 Potential Endogeneity",
    "text": "4.1 Potential Endogeneity\nAfter including the driver, city, date fixed effects fixed effects in the above Regression (1), the only challenge to obtaining causal inference is the potential endogeneity of NewCases.\nEquation (1) could be subject to simultaneity issues because drivers’ labor supply decisions and number of new cases may be interdependent. On the one hand, drivers may adjust their labor supply accordingly to the number of new cases. On the other hand, prior research has demonstrated the potential effect of mobility on the COVID-19 case growth rate. If a city has a higher volume of private transportation through ride-sharing services, given the highly contagious nature of COVID-19, the city may have a higher number of new cases."
  },
  {
    "objectID": "Case-COVID-IV.html#instrumental-variables",
    "href": "Case-COVID-IV.html#instrumental-variables",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "4.2 Instrumental Variables",
    "text": "4.2 Instrumental Variables\nTo tackle the potential endogeneity issue, we use the instrumental variable (IV) method, leveraging exogenous sources of variation in the explanatory variable that are uncorrelated with the error term in Equation (1) using two-stage least squares (2SLS). We can potentially select two instrumental variables. The first instrumental variable is imported new cases, which measures the number of infected travelers from overseas in each city as disclosed by local government. Because the imported cases relate to travelers from overseas, it should be exogenous to local confirmed cases and meet the exogeneity requirement. The second instrumental variable is other city new cases, which is the number of new cases confirmed in neighboring cities. Since confirmed cases in other cities should not directly affect the focal city’s ride-sharing market, the variable other city new cases should also satisfy the exogeneity requirement."
  },
  {
    "objectID": "Case-COVID-IV.html#manual-iv-regression",
    "href": "Case-COVID-IV.html#manual-iv-regression",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "4.3 Manual IV Regression",
    "text": "4.3 Manual IV Regression\nThe first-stage regression is specified below in Equation (2), where the definitions of variables are the same as in Equation (1).\n\\[\nNewCase_{ijt}=\\beta_0+\\alpha OtherCityNewCase_{ijt}+X\\beta+\\varepsilon_{ijt}\n\\]\n\n# Run first stage regression: new_cases ~ other_city_new_cases + controls\nIV_is_work_1ststage &lt;- feols(fml = new_cases ~ other_city_new_cases|\n                       driver_id + booking_date + city,\n      data = data_driver)\n\n# mutate predicted new_cases in data_driver\ndata_driver &lt;- data_driver %&gt;%\n  mutate(predicted_new_cases = predict(IV_is_work_1ststage))\n\nIn the second stage regression, we regress the outcome variables on the predicted new cases from the 1st stage, controlling for the same set of control variables.\n\n# Run second stage regression: is_work ~ predicted_new_cases + controls\nIV_is_work_2ndstage &lt;- feols(fml = is_work ~ predicted_new_cases|\n                       driver_id + booking_date + city,\n      data = data_driver)\n\nmodelsummary(list(IV_is_work_1ststage,IV_is_work_2ndstage),\n             stars = TRUE)\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n\nother_city_new_cases\n−0.466***\n\n\n\n\n(0.012)\n\n\n\npredicted_new_cases\n\n0.0005\n\n\n\n\n(0.001)\n\n\nNum.Obs.\n93467\n93467\n\n\nR2\n0.491\n0.662\n\n\nR2 Adj.\n0.473\n0.650\n\n\nR2 Within\n0.225\n0.000002\n\n\nR2 Within Adj.\n0.225\n−0.000009\n\n\nRMSE\n1.35\n0.22\n\n\nStd.Errors\nby: driver_id\nby: driver_id\n\n\nFE: driver_id\nX\nX\n\n\nFE: booking_date\nX\nX\n\n\nFE: city\nX\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-COVID-IV.html#iv-regression-using-feols",
    "href": "Case-COVID-IV.html#iv-regression-using-feols",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "4.4 IV Regression Using feols()",
    "text": "4.4 IV Regression Using feols()\nIn fact, the feols() function in the fixest package is powerful to help us estimate IV regression in a single step.\n\nIV_is_work &lt;- feols(fml = is_work ~ 1|\n                       driver_id + booking_date + city|\n                      new_cases ~ other_city_new_cases,\n      data = data_driver)\n\nIV_no_order &lt;- feols(fml = n_order ~ 1|\n                       driver_id + booking_date + city|\n                      new_cases ~ other_city_new_cases,\n      data = data_driver)\n\nIV_income &lt;- feols(fml = income ~ 1|\n                       driver_id + booking_date + city|\n                      new_cases ~ other_city_new_cases,\n      data = data_driver)\n\nIV_avg_distance &lt;- feols(fml = avg_distance ~ 1|\n                       driver_id + booking_date + city|\n                      new_cases ~ other_city_new_cases,\n      data = data_driver)\n\nmodelsummary(list(IV_is_work,IV_no_order,IV_income,IV_avg_distance),\n             stars = TRUE)\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\nfit_new_cases\n0.0005\n0.019**\n0.122*\n0.023\n\n\n\n(0.001)\n(0.007)\n(0.053)\n(0.015)\n\n\nNum.Obs.\n93467\n93467\n93467\n93467\n\n\nR2\n0.662\n0.607\n0.555\n0.420\n\n\nR2 Adj.\n0.650\n0.593\n0.539\n0.399\n\n\nR2 Within\n−0.000002\n−0.0002\n−0.0001\n−0.00007\n\n\nR2 Within Adj.\n−0.00001\n−0.0002\n−0.0001\n−0.00008\n\n\nRMSE\n0.22\n1.85\n13.79\n3.84\n\n\nStd.Errors\nby: driver_id\nby: driver_id\nby: driver_id\nby: driver_id\n\n\nFE: driver_id\nX\nX\nX\nX\n\n\nFE: booking_date\nX\nX\nX\nX\n\n\nFE: city\nX\nX\nX\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-COVID-IV.html#footnotes",
    "href": "Case-COVID-IV.html#footnotes",
    "title": "The Causal Impact of COVID-19 on Uber Driver Labor Supply",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis case was prepared by Wei Miao, UCL School of Management, University College London for MSIN0094 Marketing Analytics module based on . This case was developed to provide material for class discussion rather than to illustrate either effective or ineffective handling of a business situation. Names and data may have been disguised or fabricated. Please do not circulate without permission. Copyrights reserved.↩︎"
  },
  {
    "objectID": "Week7-Lecture2.html",
    "href": "Week7-Lecture2.html",
    "title": "Class 14 Workshop: Marketing Mix Modeling",
    "section": "",
    "text": "Marketing Mix Modeling (MMM) is the use of statistical analysis to estimate the causal impact of various marketing mix variables (especially pricing and promotions) on sales.\n\n\nCore idea: find an appropriate statistical model that can characterize the relationship (DGP) between sales and marketing mix variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat functional forms and specifications to use for each variable?\n\nMore of an art than science\nquadratic terms when diminishing returns are expected\n\nHow to determine the “best” model\n\npredictive accuracy (error of predicted sales)\nmodel fit (\\(R^2\\))\n\n\n\n\n\nExample 1: Model the relationship between sales and price as follows:\n\\[\nsales_t = \\beta_0 + \\beta_1Price_t + X_t\\beta+ \\epsilon_t\n\\]\nExample 2: Model the relationship between sales and number of influencers as follows.\n\nWe would normally consider diminishing marginal return of marketing activities\n\n\\[\nsales_t = \\beta_0 + \\beta_1NumInflu + \\beta_2NumInflu^2 + X_t\\beta+ \\epsilon_t\n\\]\n\n\n\nThe outputs from your MMM project – that is, the data and estimates that come out of your statistical model – need to address the profit maximization problem.\n\nThe MMM model will produce a host of outputs that measure how each tactic (e.g., price) affects sales.\nWe can then use the outputs to guide our marketing decisions.\n\n\n\n\n\nWe can utilize the outputs to compute the optimal pricing\n\n\\[\nsales_t = 1000 - 20 Price_t + X_t\\beta+ \\epsilon_t\n\\]\n\nThen we know, conditional on \\(X_t\\), the total revenue would be\n\n\\[\nprofit = (Price - COGS) *sales = (Price - COGS)*(1000 + X_t\\beta - 20*Price)\n\\]\n\nWe can derive the optimal price to maximize profit.\n\n\\[\n\\partial profit/\\partial price = (1000 + X_t\\beta - 20 Price) - 20 (Price - COGS) = 0\n\\]\n\nThe optimal price that can maximize the revenue/profit is \\(\\frac{1000 + X_t\\beta+20COGS}{40}\\)\nIn term 2’s Operations Analytics module, you will systematically learn how to find the optimal pricing, given the estimated functional relationship between sales and marketing mix variables."
  },
  {
    "objectID": "Week7-Lecture2.html#what-is-marketing-mix-modeling",
    "href": "Week7-Lecture2.html#what-is-marketing-mix-modeling",
    "title": "Class 14 Workshop: Marketing Mix Modeling",
    "section": "",
    "text": "Marketing Mix Modeling (MMM) is the use of statistical analysis to estimate the causal impact of various marketing mix variables (especially pricing and promotions) on sales.\n\n\nCore idea: find an appropriate statistical model that can characterize the relationship (DGP) between sales and marketing mix variables"
  },
  {
    "objectID": "Week7-Lecture2.html#phase-2-statistical-modelling",
    "href": "Week7-Lecture2.html#phase-2-statistical-modelling",
    "title": "Class 14 Workshop: Marketing Mix Modeling",
    "section": "",
    "text": "What functional forms and specifications to use for each variable?\n\nMore of an art than science\nquadratic terms when diminishing returns are expected\n\nHow to determine the “best” model\n\npredictive accuracy (error of predicted sales)\nmodel fit (\\(R^2\\))"
  },
  {
    "objectID": "Week7-Lecture2.html#classic-examples-of-mmm",
    "href": "Week7-Lecture2.html#classic-examples-of-mmm",
    "title": "Class 14 Workshop: Marketing Mix Modeling",
    "section": "",
    "text": "Example 1: Model the relationship between sales and price as follows:\n\\[\nsales_t = \\beta_0 + \\beta_1Price_t + X_t\\beta+ \\epsilon_t\n\\]\nExample 2: Model the relationship between sales and number of influencers as follows.\n\nWe would normally consider diminishing marginal return of marketing activities\n\n\\[\nsales_t = \\beta_0 + \\beta_1NumInflu + \\beta_2NumInflu^2 + X_t\\beta+ \\epsilon_t\n\\]"
  },
  {
    "objectID": "Week7-Lecture2.html#phase-3-model-based-optimization",
    "href": "Week7-Lecture2.html#phase-3-model-based-optimization",
    "title": "Class 14 Workshop: Marketing Mix Modeling",
    "section": "",
    "text": "The outputs from your MMM project – that is, the data and estimates that come out of your statistical model – need to address the profit maximization problem.\n\nThe MMM model will produce a host of outputs that measure how each tactic (e.g., price) affects sales.\nWe can then use the outputs to guide our marketing decisions."
  },
  {
    "objectID": "Week7-Lecture2.html#optimal-pricing-to-maximize-profit",
    "href": "Week7-Lecture2.html#optimal-pricing-to-maximize-profit",
    "title": "Class 14 Workshop: Marketing Mix Modeling",
    "section": "",
    "text": "We can utilize the outputs to compute the optimal pricing\n\n\\[\nsales_t = 1000 - 20 Price_t + X_t\\beta+ \\epsilon_t\n\\]\n\nThen we know, conditional on \\(X_t\\), the total revenue would be\n\n\\[\nprofit = (Price - COGS) *sales = (Price - COGS)*(1000 + X_t\\beta - 20*Price)\n\\]\n\nWe can derive the optimal price to maximize profit.\n\n\\[\n\\partial profit/\\partial price = (1000 + X_t\\beta - 20 Price) - 20 (Price - COGS) = 0\n\\]\n\nThe optimal price that can maximize the revenue/profit is \\(\\frac{1000 + X_t\\beta+20COGS}{40}\\)\nIn term 2’s Operations Analytics module, you will systematically learn how to find the optimal pricing, given the estimated functional relationship between sales and marketing mix variables."
  },
  {
    "objectID": "Case-BreakEvenAnalysis-Solutions.html",
    "href": "Case-BreakEvenAnalysis-Solutions.html",
    "title": "PineApple Case Study",
    "section": "",
    "text": "The marketing analytics team at PineApple Inc had applied predictive analytics models on historical sales data and predicted that the sales this year will reach 10 million unit at the retail price of £600, without any additional marketing activities. The team had also collected the information on the Cost of Goods Sold of PinePhone 15, which is 60%. The Research and Development (R&D) costs for PinePhone 15 is 100 million pounds.\nBased on the above information, we first translate the necessary background information into the following R objects.\n\n\n\n\n\n\nAssigning Operations\n\n\n\n\nWhen assigning values to variables, the operation will not print the values of the new variable\nIf you would like to check the variable is created with the correct value, you can\n\ncheck its value in the RStudio Environment on the right hand side\nin a new line, type the variable name: this is to ask R to print out the value of the object\n\n\n\n\n\n# translate the above information into R variables\nprice &lt;- 600 # retail price\nquantity &lt;- 10 # sales \nCOGS &lt;- 0.6 # cost of goods sold\nRD_costs &lt;- 100 # Research and Development costs\nendorsement_fee &lt;- 50 # endorsement\nendorsement_sales_increase &lt;- 0.025\n\n\nQuestion 1\nCompute the contribution margin per unit\n\n# Create a variable called contribution_margin from price and COGS\n# Use variables but not the raw numbers. --- Why?\n\n# Following the definition of contribution margin per unit: price - cost\ncontribution_margin &lt;- price - price * COGS\ncontribution_margin\n\n[1] 240\n\n\n\n# equivalently, contribution margin per unit = price * contribution margin rate  \ncontribution_margin &lt;- price * (1 - COGS) \ncontribution_margin\n\n[1] 240\n\n\n\n\nQuestion 2\nBased on the information at hand, should Tom approve the influencer marketing plan?\nTo decide whether Tom should approve the marketing plan, we need to conduct break-even analyses.\nThe first step is to compute the break-even quantity, as shown in the following code.\n\n# numerator is the marketing expense\n# denominator is the \"extra profit\", or the contribution margin, from selling one more unit\nBEQ &lt;- endorsement_fee / contribution_margin\nBEQ\n\n[1] 0.2083333\n\n\nBreak even quantity is the incremental quantity sales needed in order to neither lose money nor make money from the marketing campaign, hence the name “break-even”. This is like the safe line the campaign must reach.\n\n\n\n\n\n\nSales\n\n\n\nIn this module (and in practice), when we talk about sales, we mean the quantity sales, the number of units sold. For instance, in the case study, the original sales without influencer marketing is 10 million units.\nThe total money made is often called revenue or revenue sales. For instance, in the case study, the original revenue is 6000 million pounds.\n\n\nThe next step is to compare BEQ with the estimated incremental sales from the campaign.\nBEQ is 0.208 million units, which is smaller than incremental sales is 10 * 0.025 million units. It means, to not lose any money, the influencer marketing compaign needs to bring in additional 0.208 million units, but in reality, the company can actually sell better at 0.25 million, so it’s profitable to continue with the influencer marketing campaign.\nTherefore, based on the above reasoning, Tom should approve the influencer marketing campaign.\n\n\nQuestion 3\n(Please follow the above example to finish both authoring and R codes for the NPV question)\n\nCompute the sequence of monthly cash flows\n\n\nFirst, we compute the incremental sales percentage for each month, relative to the 10 million. This is a 12-element vector, each element representing the incremental sales percentage.\n\n\nincremental.sales.percentage_1stmonth &lt;- 0.003\nincremental.sales.percentage_next11months &lt;- rep(0.002,11)\n\n# incremental profit each month\nvector_incremental.sales.percentage_12months &lt;- c(incremental.sales.percentage_1stmonth,incremental.sales.percentage_next11months)\n\nvector_incremental.sales.percentage_12months\n\n [1] 0.003 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n\n\n\nNext, we time the incremental sales percentage with quantity, to get the incremental sales in terms of units each month.\n\n\nvector_incremental.sales.units_12months &lt;- vector_incremental.sales.percentage_12months * quantity\n\nvector_incremental.sales.units_12months\n\n [1] 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n\n\n\nLastly, we multiply the incremental quantity sales with the contribution margin per unit, to get the total contribution margins (incremental profits) for each month, i.e., the CF\n\n\nvector_CF &lt;- vector_incremental.sales.units_12months * contribution_margin\n\nvector_CF\n\n [1] 7.2 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8\n\n\n\nCompute the sequence of discount factors\n\n\n# divide annual wacc to get monthly wacc\nmonthly_WACC &lt;- 0.1/12 \n\n# discount factor for 1 month is 1/(1+k)\ndiscount_factor &lt;- 1/ (1+monthly_WACC)\n\n# Generate a geometric sequence vector of discounted CFs for 12 months\n\nvector_discount_factor &lt;- discount_factor ^ c(1:12)\n\nvector_discount_factor\n\n [1] 0.9917355 0.9835394 0.9754110 0.9673497 0.9593551 0.9514265 0.9435635\n [8] 0.9357654 0.9280319 0.9203622 0.9127559 0.9052124\n\n\n\nCompute the NPV\n\n\nMultiply CF vector with discount factor vector, to get the discounted CF vector\n\n\nvector_discounted.CF &lt;- vector_CF * vector_discount_factor \n  \nvector_discounted.CF\n\n [1] 7.140496 4.720989 4.681973 4.643279 4.604904 4.566847 4.529105 4.491674\n [9] 4.454553 4.417738 4.381228 4.345020\n\n\n\nuse function sum() to get the sum of all elements in a vector. That is, the sum of discounted cash flows in all 12 months.\n\n\nsum(vector_discounted.CF)\n\n[1] 56.97781\n\n\n\nWe need to subtract the endorsement fee, which is the marketing expense, to get the next present value\n\n\nNPV &lt;- sum(vector_discounted.CF) - endorsement_fee\nNPV\n\n[1] 6.977806"
  },
  {
    "objectID": "Week7-Lecture1.html",
    "href": "Week7-Lecture1.html",
    "title": "Class 13 OLS Regression Advanced",
    "section": "",
    "text": "pacman::p_load(dplyr,ggplot2,ggthemes)\n# Load both datasets\ndata_purchase &lt;- read.csv(file = \"https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1\", \n                      header = T)\n\ndata_demo &lt;- read.csv(\"https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1\",\n                      header = T)\n\n# Left join demographic data into purchase data\ndata_full &lt;- data_purchase %&gt;%\n  left_join(data_demo, by = \"ID\")\n\n# Handle Missing Values of Income\ndata_full &lt;- data_full %&gt;%\n  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T))) %&gt;%\n  mutate(total_spending = MntFishProducts + MntFruits + MntGoldProds + MntMeatProducts + MntSweetProducts + MntWines)\n\n\nSo far, the independent variables we have used are Income and Kidhome, which are continuous variables.\nSome variables are intrinsically not countable; we need to treat them as categorical variables\n\ne.g., gender, education group, city.\n\n\n\n\n\n\nIn R, we need to use a function factor() to inform R that this variable is a categorical variable, such that statistical models will treat them differently from continuous variables.\n\nRefer to this link for more examples in datacamp.\n\nWe can use factor(Education) to indicate that, Education is a categorical variable.\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor = factor(Education))\n\n\nWe can use levels() to check how many categories are there in the factor variable.\n\n\n# check levels of a factor\nlevels(data_full$Education_factor)\n\n[1] \"2n Cycle\"   \"Basic\"      \"Graduation\" \"Master\"     \"PhD\"       \n\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor_2 = relevel(Education_factor, ref = \"Basic\") )\n\nlevels(data_full$Education_factor_2)\n\n[1] \"Basic\"      \"2n Cycle\"   \"Graduation\" \"Master\"     \"PhD\"       \n\n\n\n\n\n\npacman::p_load(fixest,modelsummary)\nfeols_categorical &lt;- feols(data = data_full,\n  fml = total_spending ~ Income + Kidhome + Education_factor_2)\nmodelsummary(feols_categorical,\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−180.297**\n\n\n\n(56.305)\n\n\nIncome\n0.020***\n\n\n\n(0.000)\n\n\nKidhome\n−227.761***\n\n\n\n(16.961)\n\n\nEducation_factor_22n Cycle\n−164.044**\n\n\n\n(60.448)\n\n\nEducation_factor_2Graduation\n−119.695*\n\n\n\n(56.176)\n\n\nEducation_factor_2Master\n−143.015*\n\n\n\n(58.443)\n\n\nEducation_factor_2PhD\n−153.190**\n\n\n\n(57.751)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.662\n\n\nR2 Adj.\n0.661\n\n\nAIC\n29128.2\n\n\nBIC\n29167.4\n\n\nRMSE\n350.59\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nInternally, R uses one-hot encoding to encode factor variables with K levels into K-1 binary variables.\n\nBecause we have the intercept, we can only have K-1 binary variables.\nThe intercept stands for the effects of the baseline group.\nIn the regression result table, Basic group is suppressed if we use Education_factor_2, because this group is chosen as the baseline group.\n\nThe interpretation template of coefficients for factor variables: Ceteris paribus, compared with the [baseline group], the [outcome variable] of [group XXX] is higher/lower by [coefficient], and the coefficient is statistically [significant/insignificant].\n\nCeteris paribus, compared with the basic education group, the total spending of PhD group is lower by 153.190 dollars. The coefficient is statistically significant at the 1% level.\n\n\n\n\n\nAfter-class exercise: change the baseline group to Master, rerun the regression, and interpret the coefficients."
  },
  {
    "objectID": "Week7-Lecture1.html#categorical-variables-1",
    "href": "Week7-Lecture1.html#categorical-variables-1",
    "title": "Class 13 OLS Regression Advanced",
    "section": "",
    "text": "pacman::p_load(dplyr,ggplot2,ggthemes)\n# Load both datasets\ndata_purchase &lt;- read.csv(file = \"https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1\", \n                      header = T)\n\ndata_demo &lt;- read.csv(\"https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1\",\n                      header = T)\n\n# Left join demographic data into purchase data\ndata_full &lt;- data_purchase %&gt;%\n  left_join(data_demo, by = \"ID\")\n\n# Handle Missing Values of Income\ndata_full &lt;- data_full %&gt;%\n  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T))) %&gt;%\n  mutate(total_spending = MntFishProducts + MntFruits + MntGoldProds + MntMeatProducts + MntSweetProducts + MntWines)\n\n\nSo far, the independent variables we have used are Income and Kidhome, which are continuous variables.\nSome variables are intrinsically not countable; we need to treat them as categorical variables\n\ne.g., gender, education group, city."
  },
  {
    "objectID": "Week7-Lecture1.html#handling-categorical-variables-using-factor",
    "href": "Week7-Lecture1.html#handling-categorical-variables-using-factor",
    "title": "Class 13 OLS Regression Advanced",
    "section": "",
    "text": "In R, we need to use a function factor() to inform R that this variable is a categorical variable, such that statistical models will treat them differently from continuous variables.\n\nRefer to this link for more examples in datacamp.\n\nWe can use factor(Education) to indicate that, Education is a categorical variable.\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor = factor(Education))\n\n\nWe can use levels() to check how many categories are there in the factor variable.\n\n\n# check levels of a factor\nlevels(data_full$Education_factor)\n\n[1] \"2n Cycle\"   \"Basic\"      \"Graduation\" \"Master\"     \"PhD\"       \n\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor_2 = relevel(Education_factor, ref = \"Basic\") )\n\nlevels(data_full$Education_factor_2)\n\n[1] \"Basic\"      \"2n Cycle\"   \"Graduation\" \"Master\"     \"PhD\""
  },
  {
    "objectID": "Week7-Lecture1.html#running-regression-with-factor-variables",
    "href": "Week7-Lecture1.html#running-regression-with-factor-variables",
    "title": "Class 13 OLS Regression Advanced",
    "section": "",
    "text": "pacman::p_load(fixest,modelsummary)\nfeols_categorical &lt;- feols(data = data_full,\n  fml = total_spending ~ Income + Kidhome + Education_factor_2)\nmodelsummary(feols_categorical,\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−180.297**\n\n\n\n(56.305)\n\n\nIncome\n0.020***\n\n\n\n(0.000)\n\n\nKidhome\n−227.761***\n\n\n\n(16.961)\n\n\nEducation_factor_22n Cycle\n−164.044**\n\n\n\n(60.448)\n\n\nEducation_factor_2Graduation\n−119.695*\n\n\n\n(56.176)\n\n\nEducation_factor_2Master\n−143.015*\n\n\n\n(58.443)\n\n\nEducation_factor_2PhD\n−153.190**\n\n\n\n(57.751)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.662\n\n\nR2 Adj.\n0.661\n\n\nAIC\n29128.2\n\n\nBIC\n29167.4\n\n\nRMSE\n350.59\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Week7-Lecture1.html#interpretation-of-coefficients-for-categorical-variables",
    "href": "Week7-Lecture1.html#interpretation-of-coefficients-for-categorical-variables",
    "title": "Class 13 OLS Regression Advanced",
    "section": "",
    "text": "Internally, R uses one-hot encoding to encode factor variables with K levels into K-1 binary variables.\n\nBecause we have the intercept, we can only have K-1 binary variables.\nThe intercept stands for the effects of the baseline group.\nIn the regression result table, Basic group is suppressed if we use Education_factor_2, because this group is chosen as the baseline group.\n\nThe interpretation template of coefficients for factor variables: Ceteris paribus, compared with the [baseline group], the [outcome variable] of [group XXX] is higher/lower by [coefficient], and the coefficient is statistically [significant/insignificant].\n\nCeteris paribus, compared with the basic education group, the total spending of PhD group is lower by 153.190 dollars. The coefficient is statistically significant at the 1% level.\n\n\n\n\n\nAfter-class exercise: change the baseline group to Master, rerun the regression, and interpret the coefficients."
  },
  {
    "objectID": "Week7-Lecture1.html#quadratic-terms",
    "href": "Week7-Lecture1.html#quadratic-terms",
    "title": "Class 13 OLS Regression Advanced",
    "section": "2.1 Quadratic Terms",
    "text": "2.1 Quadratic Terms\n\nIf we believe the relationship between the outcome variable and explanatory variable is a quadratic function, we can include an additional quadratic term in the regression to model such non-linear relationship.\n\n\\[\nSpending = \\beta_0 + \\beta_1Income + \\beta_2Income^2  + \\epsilon\n\\]\n\nggplot(data = data_full,\n       aes(x = Income, y = total_spending)) + \n  geom_point()+theme_stata()"
  },
  {
    "objectID": "Week7-Lecture1.html#quadratic-terms-1",
    "href": "Week7-Lecture1.html#quadratic-terms-1",
    "title": "Class 13 OLS Regression Advanced",
    "section": "2.2 Quadratic Terms",
    "text": "2.2 Quadratic Terms\n\nIf after estimation, the coefficient for \\(Income^2\\), \\(\\beta_2\\), is negative, then we have an down open parabola.\n\n\n\n\n\n\n\nThat is, as income increases, total spending first increases and then decreases, i.e., a non-linear effect."
  },
  {
    "objectID": "Week7-Lecture1.html#quadratic-terms-in-linear-regression",
    "href": "Week7-Lecture1.html#quadratic-terms-in-linear-regression",
    "title": "Class 13 OLS Regression Advanced",
    "section": "2.3 Quadratic Terms in Linear Regression",
    "text": "2.3 Quadratic Terms in Linear Regression\n\nLet’s run two regressions, with and without the quadratic term.\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Income_quadartic = Income^2 )\n\n# model 1: without quadratic term\nfeols_noquadratic &lt;- feols(data = data_full,\n  fml = total_spending ~ Income )\n\n# model 2: with quadratic term\nfeols_quadratic &lt;- feols(data = data_full,\n  fml = total_spending ~ Income  + Income_quadartic )\n\n\nmodelsummary(list(feols_noquadratic,feols_quadratic), \n             stars = T)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n−556.823***\n−627.040***\n\n\n\n(21.654)\n(36.522)\n\n\nIncome\n0.022***\n0.025***\n\n\n\n(0.000)\n(0.001)\n\n\nIncome_quadartic\n\n0.000*\n\n\n\n\n(0.000)\n\n\nNum.Obs.\n2000\n2000\n\n\nR2\n0.629\n0.630\n\n\nR2 Adj.\n0.629\n0.630\n\n\nAIC\n29306.1\n29302.4\n\n\nBIC\n29317.3\n29319.2\n\n\nRMSE\n367.45\n366.92\n\n\nStd.Errors\nIID\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Week7-Lecture1.html#quadratic-terms-compute-the-vertex",
    "href": "Week7-Lecture1.html#quadratic-terms-compute-the-vertex",
    "title": "Class 13 OLS Regression Advanced",
    "section": "2.4 Quadratic Terms: Compute the Vertex",
    "text": "2.4 Quadratic Terms: Compute the Vertex\n\nWe can compute the vertex point where total spending is maximized by income\n\n\n# extract the coeffcient vector\nfeols_coefficient &lt;- feols_quadratic$coefficients\nfeols_coefficient\n\n     (Intercept)           Income Income_quadartic \n   -6.270403e+02     2.533276e-02    -2.663682e-08 \n\n# Use b / (-2a) to get the vertex\n- feols_coefficient[2]/ \n  (2 * feols_coefficient[3])\n\n  Income \n475521.5"
  },
  {
    "objectID": "Week7-Lecture1.html#linear-probability-model-1",
    "href": "Week7-Lecture1.html#linear-probability-model-1",
    "title": "Class 13 OLS Regression Advanced",
    "section": "3.1 Linear Probability Model",
    "text": "3.1 Linear Probability Model\n\nIn Predictive Analytics, we learned how to use decision tree and random forest to make predictions. In fact, linear regression can also be used as another supervised learning model.\nOn the one hand, regression predicts the expectation of response \\(Y\\) conditional on \\(X\\); that is \\[\nE[Y|X]= X\\beta\n\\]\nOn the other hand, for a binary outcome variable, if the probability of outcome occurring is \\(p\\), then we can write the expectation of \\(Y\\) is \\[\nE[Y|X] = 1 * p + 0 * (1 - p) = p\n\\]\nAs a result, we have the following equation \\[\n   \\operatorname{Probability}[Y=1|X] = E[Y|X] = X \\beta\n\\]\nInterpretation of LPM: Everything else equal, a unit change in \\(x\\) will change the probability of the outcome occurring by \\(\\beta\\) units."
  },
  {
    "objectID": "Week7-Lecture1.html#pros-and-cons-of-lpm",
    "href": "Week7-Lecture1.html#pros-and-cons-of-lpm",
    "title": "Class 13 OLS Regression Advanced",
    "section": "3.2 Pros and Cons of LPM",
    "text": "3.2 Pros and Cons of LPM\n\nThe procedures of training LPM is similar to training a decision tree rpart()/random forest ranger(): we use linear regression function feols() to train the LPM on the training data and make predictions on the test data.\nAdvantages\n\nEasy and fast to run\nHigh interpretability: coefficients have clear economic meanings\n\nDisadvantages\n\nPredicted probabilities of occurring may fall out of the [0,1] range\nAccuracy tends to be low"
  },
  {
    "objectID": "Week10-Lecture2.html#week-1-marketing-process",
    "href": "Week10-Lecture2.html#week-1-marketing-process",
    "title": "Week 10 Module Wrap-up",
    "section": "1.1 Week 1: Marketing Process",
    "text": "1.1 Week 1: Marketing Process\n\n\n\n\n\n\nWhat is marketing\nSituation analysis (5C analysis)"
  },
  {
    "objectID": "Week10-Lecture2.html#week-1-break-even-analysis",
    "href": "Week10-Lecture2.html#week-1-break-even-analysis",
    "title": "Week 10 Module Wrap-up",
    "section": "1.2 Week 1: Break-Even Analysis",
    "text": "1.2 Week 1: Break-Even Analysis\n\nBreak-even analysis is essential to any business activity\n\nBreak-even quantity (BEQ)\nNet present value (NPV)\nCustomer lifetime value (CLV)\n\nBEA is essentially cost-benefit analysis\n\nPineapple case\ni-basket case"
  },
  {
    "objectID": "Week10-Lecture2.html#week-2-data-wrangling-with-dplyr",
    "href": "Week10-Lecture2.html#week-2-data-wrangling-with-dplyr",
    "title": "Week 10 Module Wrap-up",
    "section": "2.1 Week 2: Data Wrangling with dplyr",
    "text": "2.1 Week 2: Data Wrangling with dplyr\n\nData manipulation with dplyr\n\nbasic operations: filter, mutate, select, arrange\ngroup aggregation: group_by\nmulti-data joining: left_join\n\nTesco case study: Preliminary customer analysis using dplyr"
  },
  {
    "objectID": "Week10-Lecture2.html#week-2-nice-to-meet-you-im-wei-and-im-a-musician",
    "href": "Week10-Lecture2.html#week-2-nice-to-meet-you-im-wei-and-im-a-musician",
    "title": "Week 10 Module Wrap-up",
    "section": "2.2 Week 2: Nice to meet you, I’m Wei, and I’m a musician!",
    "text": "2.2 Week 2: Nice to meet you, I’m Wei, and I’m a musician!"
  },
  {
    "objectID": "Week10-Lecture2.html#week-2-nice-to-meet-you-im-wei-and-im-a-youtuber",
    "href": "Week10-Lecture2.html#week-2-nice-to-meet-you-im-wei-and-im-a-youtuber",
    "title": "Week 10 Module Wrap-up",
    "section": "2.3 Week 2: Nice to meet you, I’m Wei, and I’m a Youtuber!",
    "text": "2.3 Week 2: Nice to meet you, I’m Wei, and I’m a Youtuber!"
  },
  {
    "objectID": "Week10-Lecture2.html#week-3-unsupervised-learning-for-customer-segmentation",
    "href": "Week10-Lecture2.html#week-3-unsupervised-learning-for-customer-segmentation",
    "title": "Week 10 Module Wrap-up",
    "section": "3.1 Week 3: Unsupervised Learning for Customer Segmentation",
    "text": "3.1 Week 3: Unsupervised Learning for Customer Segmentation\n\nUnsupervised learning such as K-means clustering help classify individuals into different groups.\nK-means is usually supplementary to more complicated analyses."
  },
  {
    "objectID": "Week10-Lecture2.html#week-4-supervised-learning-for-customer-targeting",
    "href": "Week10-Lecture2.html#week-4-supervised-learning-for-customer-targeting",
    "title": "Week 10 Module Wrap-up",
    "section": "3.2 Week 4: Supervised Learning for Customer Targeting",
    "text": "3.2 Week 4: Supervised Learning for Customer Targeting\n\n\n\n\n\n\nSupervised learning models predict outcome \\(Y\\) based on predictors \\(X\\)\n\nlinear regression (high interpretability, low accuracy)\ndecision tree and random forest (good interpretability, good accuracy)\ndeep learning (no interpretability, high accuracy)\n\nWith targeted marketing from supervised learning, we can effectively reduce marketing costs and boost the ROI.\n\nImproving Marketing Efficiency Using Predictive Analytics for Tesco case"
  },
  {
    "objectID": "Week10-Lecture2.html#week-5-rubin-causal-framework-and-rcts",
    "href": "Week10-Lecture2.html#week-5-rubin-causal-framework-and-rcts",
    "title": "Week 10 Module Wrap-up",
    "section": "4.1 Week 5: Rubin Causal Framework and RCTs",
    "text": "4.1 Week 5: Rubin Causal Framework and RCTs\n\nWhy causal inference matters?\n\n\n\n\n\n\n\nWe can use RCT to get causal inference\n\nRandomization removes selection bias and pre-existing differences\n\nApplication of RCTs\n\nVungle case study"
  },
  {
    "objectID": "Week10-Lecture2.html#week-5-nice-to-meet-you-im-wei-and-im-from-hogwarts",
    "href": "Week10-Lecture2.html#week-5-nice-to-meet-you-im-wei-and-im-from-hogwarts",
    "title": "Week 10 Module Wrap-up",
    "section": "4.2 Week 5: Nice to meet You, I’m Wei and I’m from Hogwarts",
    "text": "4.2 Week 5: Nice to meet You, I’m Wei and I’m from Hogwarts"
  },
  {
    "objectID": "Week10-Lecture2.html#week-6-7-ols-regression-and-marketing-mix-modeling",
    "href": "Week10-Lecture2.html#week-6-7-ols-regression-and-marketing-mix-modeling",
    "title": "Week 10 Module Wrap-up",
    "section": "4.3 Week 6 & 7 : OLS Regression and Marketing Mix Modeling",
    "text": "4.3 Week 6 & 7 : OLS Regression and Marketing Mix Modeling\n\nLinear regression can give causal inference if and only if all confounding factors have been controlled in the regression.\n\n\n\n\n\n\n\nIn reality, this never happens, so linear regression can never give causal effects.\nIn practice, companies often use linear regression to build marketing mix modeling, in order to set optimal prices for profit maximization (profit is a quadratic function of price).\n\nZalora case study"
  },
  {
    "objectID": "Week10-Lecture2.html#week-8-endogeneity-and-instrument-variables",
    "href": "Week10-Lecture2.html#week-8-endogeneity-and-instrument-variables",
    "title": "Week 10 Module Wrap-up",
    "section": "4.4 Week 8: Endogeneity and Instrument Variables",
    "text": "4.4 Week 8: Endogeneity and Instrument Variables\n\nEndogeneity of simple linear regressions\n\nOmitted variable bias\nReverse causality\n[Measurement error]\n\nAn instrument variable can give causal inference. Requirements:\n\nExogeneity\nRelevance\nObservable (implicit)"
  },
  {
    "objectID": "Week10-Lecture2.html#week-8-endogeneity-and-instrument-variables-1",
    "href": "Week10-Lecture2.html#week-8-endogeneity-and-instrument-variables-1",
    "title": "Week 10 Module Wrap-up",
    "section": "4.5 Week 8: Endogeneity and Instrument Variables",
    "text": "4.5 Week 8: Endogeneity and Instrument Variables\n\nExplain \\(X\\) with \\(Z\\) (regress \\(X\\) on \\(Z\\)). The predicted \\(X\\), \\(\\hat{X}\\), is uncorrelated with the error term \\(\\epsilon\\) in the original regression.\n\n\\(\\hat{X}\\) is exogenous, because \\(Z\\) is exogenous\nAll endogenous parts are now absorbed/teased out into the error term in the first-stage regression \\(\\epsilon_{i}\\)\n\n\n\\[\nX_{i}=Z\\eta+\\epsilon_{i}\n\\]\n\nUse the explained part \\(\\hat{X}\\) to explain \\(y\\), now \\(\\hat{X}\\) is exogenous and can give us causal inference.\n\n\\[\ny_{i}=\\hat{X} \\beta+\\varepsilon_{i}, \\quad \\operatorname{cov}\\left(\\hat{X}_{i}, \\varepsilon_{i}\\right) = 0\n\\]\n\nCOVID-19 case study: The causal impact of COVID-19 on Uber Driver Decision"
  },
  {
    "objectID": "Week10-Lecture2.html#week-9-difference-in-differences",
    "href": "Week10-Lecture2.html#week-9-difference-in-differences",
    "title": "Week 10 Module Wrap-up",
    "section": "4.6 Week 9: Difference-in-Differences",
    "text": "4.6 Week 9: Difference-in-Differences\n\n\n\n\n\n\nA new policy/regulation (GDPR, lockdown, etc.)\nRCTs that are hard to randomize at the individual level (East London and West London)"
  },
  {
    "objectID": "Week10-Lecture2.html#week-9-regression-discontinuity-design",
    "href": "Week10-Lecture2.html#week-9-regression-discontinuity-design",
    "title": "Week 10 Module Wrap-up",
    "section": "4.7 Week 9: Regression Discontinuity Design",
    "text": "4.7 Week 9: Regression Discontinuity Design\n\n\n\n\n\n\n“Distinction” honor on students’ future salaries\nRegression discontinuity in time"
  },
  {
    "objectID": "Week10-Lecture2.html#one-causal-question-many-solutions",
    "href": "Week10-Lecture2.html#one-causal-question-many-solutions",
    "title": "Week 10 Module Wrap-up",
    "section": "4.8 One Causal Question, Many Solutions",
    "text": "4.8 One Causal Question, Many Solutions\n\nIf we have historical data on number of restaurants on UberEat in each month, and the total number of orders in each month, can we get the causal effect?\n\n\\[\nNumOrders_t = \\beta_0 + \\beta_1 NumRestaurants_t + \\epsilon_t\n\\]\n\nSolutions\n\nRCT\nInstrumental Variable\nDiD\nRDD"
  },
  {
    "objectID": "Week10-Lecture2.html#msin0094-contract-from-week-1",
    "href": "Week10-Lecture2.html#msin0094-contract-from-week-1",
    "title": "Week 10 Module Wrap-up",
    "section": "5.1 MSIN0094 Contract from Week 1",
    "text": "5.1 MSIN0094 Contract from Week 1"
  },
  {
    "objectID": "Week10-Lecture2.html#weeks-not-enough",
    "href": "Week10-Lecture2.html#weeks-not-enough",
    "title": "Week 10 Module Wrap-up",
    "section": "5.2 10 Weeks Not Enough?",
    "text": "5.2 10 Weeks Not Enough?\n\nI love new challenges so my door is always open even after the class is over\nMore learning materials\n\nOptional reading materials in each week\nI plan to update more R tutorials/data analytics tools on my Youtube channel. It’s never too late to subscribe!"
  },
  {
    "objectID": "Week10-Lecture2.html#lessons-for-me",
    "href": "Week10-Lecture2.html#lessons-for-me",
    "title": "Week 10 Module Wrap-up",
    "section": "5.3 Lessons for Me",
    "text": "5.3 Lessons for Me\n\nImpressed with your willingness to learn hard tools and to dig in\n\n=&gt; My colleagues predicted you would chase me out of the classroom for making you learn Marketing, R, and so many complicated models at the same time\n\nYou’ve made me very proud:\n\nIt gives me a huge sense of achievement see that you all have made huge progress in your R skills and marketing analytics models!\nIt feels weird but I’m flattered answer questions from other modules :))\nR is the best language!!!!\n\nI learned something from you too\n\nsuperb time management\nhard-working attitude"
  },
  {
    "objectID": "Week10-Lecture2.html#looking-into-the-future",
    "href": "Week10-Lecture2.html#looking-into-the-future",
    "title": "Week 10 Module Wrap-up",
    "section": "5.4 Looking into the Future",
    "text": "5.4 Looking into the Future\n\nThe only things you will probably remember in 10 years:\n\nThere used to be a module leader with a big bubble tea belly, who could have been bribed by T4\nbut he tries his best to be a good musician, magician, youtuber, standup comedian, and most importantly a (great) lecturer\nThere used to be a lame alumnus named Tom, who claims to be Jeff Bezos’ brother-in-law and made us do many assignments."
  },
  {
    "objectID": "Week10-Lecture2.html#thank-you-for-being-amazing",
    "href": "Week10-Lecture2.html#thank-you-for-being-amazing",
    "title": "Week 10 Module Wrap-up",
    "section": "5.5 Thank You for Being Amazing!!",
    "text": "5.5 Thank You for Being Amazing!!\nThank you very much for your hard work the whole term!\nHope a few years later on a random street in London, you can still know the correct answer to this question!\n\n\n\n\n\n\n\n\n\n\n\n\nHey Jude\nHey Tom\n\n\n\n\nHey [G] Jude don’t make it [D] bad\nTake a [D7] sad song and make it [G] better\nRe[C]member to let her into your [G] heart\nThen you can [D7] start to make it [G] better\nHey [G] Tom don’t make it [D] bad\nTake a [D7] T4 Bubble Tea and nail the final [G] assignment\nRe[C]member to let R into your [G] heart\nThen you can [D7] start to make Wei [G] prouder\n\n\nHey [G] Jude don’t be a-[D]fraid\nYou were [D7] made to go out and [G] get her\nThe [C] minute you let her under your [G] skin\nThen you be[D7]gin to make it [G] better [G]\nHey [G] Tom don’t skip my [D] class\nI saw you tap your card and leave [D7], making my heart [G] so broken\nThe [C] minute you practice Python [G] in my class\nThen I realize [D7] it’s time to [G] duel David!\n\n\n[G7]And any time you feel the [C] pain hey [Em] Jude re-[Am]frain\nDon’t [Am7] carry the [D] world u[D7]pon your [G] shoulder [G]\n[G7]For well you know that it’s a [C] fool who [Em] plays it [Am] cool\nBy [Am7] making his [D] world a [D7] little [G] colder\n[G] Na na na [G7]na na na na [D7] na na [G] [D7]\n[G7]And any time you have a question [C] hey [Em] Tom come to [Am] Wei\nDon’t [Am7] carry the [D] world u[D7]pon your [G] shoulder [G]\n[G7]For well you know that it’s a [C] stats question [Em] no problem [Am] just ask Wei\nAll [Am7] he needs in return [D] is a [D7] bubble [G] tea\n[G] La La La [G7] R is the best language [D7] Marketing is the best module! [G] [D7]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is the online supplement to the MSIN0094 Marketing Analytics Module for the MSc Business Analytics program at UCL School of Management.\nFor more information about me and my research, please refer to my personal website."
  },
  {
    "objectID": "Week9-Lecture1.html",
    "href": "Week9-Lecture1.html",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "",
    "text": "RCTs are the gold standard of causal inference.\n\nIn an RCT, the treatment is randomized and hence not correlated with any confounding factor.\n\nIn practice, however, it’s challenging to implement a perfect RCT.\n\nCrossover and spillover\nCostly in terms of time and money\nPotential moral issues1\n\n\n\n\n\n\nA natural experiment is an empirical study in which individuals are exposed to the experimental and control conditions that are determined by nature or by other factors outside the control of the investigators. The process governing the exposures arguably resembles random assignment.\nNatural experiments are observational studies using secondary data and are not controlled in the sense of an RCT.\n\n\n\n\n\nA new government regulation (say, a local COVID-19 lockdown or GDPR) is implemented in city A but not the neighboring city B\n\nResidents in city A are treated due to uncontrollable forces but residents in city B are untreated\n\nMSc students get Merit honor for getting 69.9 and Disctinction for getting 70 score\n\nStudents can definitely work harder to get higher scores, but the 0.1 score may not be controllable by students\n\n\n\n\n\n\n\nRCT\n\nAssignment of treatment is purely randomized\nTreatment is under control of a researcher\nPrimary data\nWe can proactively design an RCT for the research questions we have\n\n\nNatural Experiment\n\nAssignment of treatment is “as-if” randomized\nTreatment is not controlled by a researcher\nSecondary data\nWe can only look for natural experiments from current data that fit our questions"
  },
  {
    "objectID": "Week9-Lecture1.html#real-experiments-rcts",
    "href": "Week9-Lecture1.html#real-experiments-rcts",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "",
    "text": "RCTs are the gold standard of causal inference.\n\nIn an RCT, the treatment is randomized and hence not correlated with any confounding factor.\n\nIn practice, however, it’s challenging to implement a perfect RCT.\n\nCrossover and spillover\nCostly in terms of time and money\nPotential moral issues1"
  },
  {
    "objectID": "Week9-Lecture1.html#natural-experiments",
    "href": "Week9-Lecture1.html#natural-experiments",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "",
    "text": "A natural experiment is an empirical study in which individuals are exposed to the experimental and control conditions that are determined by nature or by other factors outside the control of the investigators. The process governing the exposures arguably resembles random assignment.\nNatural experiments are observational studies using secondary data and are not controlled in the sense of an RCT."
  },
  {
    "objectID": "Week9-Lecture1.html#natural-experiment-examples",
    "href": "Week9-Lecture1.html#natural-experiment-examples",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "",
    "text": "A new government regulation (say, a local COVID-19 lockdown or GDPR) is implemented in city A but not the neighboring city B\n\nResidents in city A are treated due to uncontrollable forces but residents in city B are untreated\n\nMSc students get Merit honor for getting 69.9 and Disctinction for getting 70 score\n\nStudents can definitely work harder to get higher scores, but the 0.1 score may not be controllable by students"
  },
  {
    "objectID": "Week9-Lecture1.html#comparison-rct-natural-experiment",
    "href": "Week9-Lecture1.html#comparison-rct-natural-experiment",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "",
    "text": "RCT\n\nAssignment of treatment is purely randomized\nTreatment is under control of a researcher\nPrimary data\nWe can proactively design an RCT for the research questions we have\n\n\nNatural Experiment\n\nAssignment of treatment is “as-if” randomized\nTreatment is not controlled by a researcher\nSecondary data\nWe can only look for natural experiments from current data that fit our questions"
  },
  {
    "objectID": "Week9-Lecture1.html#a-motivating-example",
    "href": "Week9-Lecture1.html#a-motivating-example",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.1 A Motivating Example",
    "text": "2.1 A Motivating Example\n\nIn the RCT lecture, we learned how to design an A/B testing in 5 steps to help Tom evaluate the causal impact of a loyalty program on customers’ retention rate.\nWe discussed that the unit of randomization should not be at the region level.\n\nEast London versus West London randomization would cause failed randomization checks.\n\nHowever, in reality, individual-level randomization may be too costly or infeasible, and we may need to do a region-level A/B testing.\n\n\n\nIn this case, how can we design the A/B testing to avoid failed randomization?"
  },
  {
    "objectID": "Week9-Lecture1.html#proposal-1-before-after-comparison-on-east-london",
    "href": "Week9-Lecture1.html#proposal-1-before-after-comparison-on-east-london",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.2 Proposal 1: Before-After Comparison on East London",
    "text": "2.2 Proposal 1: Before-After Comparison on East London\n\nKeep status quo (no loyalty program) in East London for 1 month; collect customer retention data for this month.\nThen Introduce the loyalty program in East London; collect customer retention data for another 1 month.\nCompare the before-after difference in retention rate by a t-test or running the following regression:\n\n\\[\nRetention_{i, t}=\\alpha+\\beta_{1} Post_{i}+\\mu_{i, t}\n\\]\nCan this difference tell us the causal effect of the loyalty program?2\n\nNo, because the variable \\(post\\) is confounded with seasonality. Even without the loyalty program, customers’ retention may already change with time due to seasonality. So this simple before-after difference cannot tease out the causal effect of the loyalty program."
  },
  {
    "objectID": "Week9-Lecture1.html#proposal-2-east-west-london-comparison",
    "href": "Week9-Lecture1.html#proposal-2-east-west-london-comparison",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.3 Proposal 2: East-West London Comparison",
    "text": "2.3 Proposal 2: East-West London Comparison\n\nRandomize which region (East London/West London) would receive the loyalty program, and then introduce the loyalty program in the treatment region.\nCollect customer retention data for both regions for a month.\nCompare the treatment-control difference in retention rate by a t-test or running the following regression:\n\n\\[\nRetention_{i, t}=\\alpha+\\beta_{1} Treated_{i}+\\mu_{i, t}\n\\]\nCan this difference tell us the causal effect of the loyalty program?3\n\nNo, because customers in East London and West London are intrinsically different. Even without the loyalty program, we may already see a difference in their retention rate. So the simple treatment-control comparison cannot tease out the causal effect of the loyalty program."
  },
  {
    "objectID": "Week9-Lecture1.html#proposal-3-difference-in-differences",
    "href": "Week9-Lecture1.html#proposal-3-difference-in-differences",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.4 Proposal 3: Difference-in-Differences",
    "text": "2.4 Proposal 3: Difference-in-Differences\n\nKeep status quo (no loyalty program) in both East and West London for 1 month; collect data on customer retention for this month in both regions.\n\nCompute the difference in retention rate \\(Diff_{pre} = Y_{treated,pre} - Y_{control,pre}\\), which measures the pre-existing difference across the treatment and control group even without the loyalty program.\n\nRandomize which region (East London/West London) would receive the loyalty program, and then introduce the loyalty program in the treatment region; collect customer retention data for both regions for another 1 month\n\nCompute the difference in retention rate across the treatment and control group in this second month: \\(Diff_{post} = Y_{treated,post} - Y_{control,post}\\), which measures the the total difference across the treatment and control group due to (1) pre-existing difference; and (2) treatment effect.\n\nTake the difference in the differences or by running the below regression. \\(DiD\\) can measure the causal effect of loyalty program on retention rate. \\[\nDiD = Diff_{post} - Diff_{pre} = [Y_{treated,post} - Y_{control,post}] - [Y_{treated,pre} - Y_{control,pre}]\n\\] \\[\nRetention_{i, t}=\\beta_0+ \\beta_{1} Post_{i}+\\beta_{2} Treated_{i}+\\beta_{3}  Treated_{i} \\times Post_{i}  + \\mu_{i, t}\n\\]"
  },
  {
    "objectID": "Week9-Lecture1.html#difference-in-differences",
    "href": "Week9-Lecture1.html#difference-in-differences",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.5 Difference-in-Differences",
    "text": "2.5 Difference-in-Differences\n\nDifference-in-differences (DiD or DD) is a statistical technique used in economics and business that attempts to mimic an experimental research design using observational data (secondary data), by studying the differential effect of a treatment on a ‘treatment group’ versus a ‘control group’ in a natural experiment.\nIn practice, DiD can also be combined with an RCT, if true randomization is costly or infeasible."
  },
  {
    "objectID": "Week9-Lecture1.html#did-estimator-graphical-illustration",
    "href": "Week9-Lecture1.html#did-estimator-graphical-illustration",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.6 DiD Estimator: Graphical Illustration",
    "text": "2.6 DiD Estimator: Graphical Illustration"
  },
  {
    "objectID": "Week9-Lecture1.html#did-estimator-alternative-illustration",
    "href": "Week9-Lecture1.html#did-estimator-alternative-illustration",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.7 DiD Estimator: Alternative Illustration",
    "text": "2.7 DiD Estimator: Alternative Illustration\n\nWe can also use linear regression to quantify the treatment effects from a Diff-in-Diff experiment:\n\n\\[\nOutcome_{i, t}=\\beta_0+ \\beta_{1} Post_{t}+\\beta_{2} Treated_{i}+\\beta_{3}  Treated_{i} \\times Post_{t}  + \\mu_{i, t}\n\\]\n\nThe idea of running a regression is to control the confounding factors: (1) \\(Post_{t}\\) can control for the seasonality for all customers (2) \\(Treated_{i}\\) can control for the pre-existing difference between the treatment group and control group.\nTherefore, \\(\\beta_3\\) is the actual treatment effect, after teasing out (1) seasonality and (2) pre-existing differences."
  },
  {
    "objectID": "Week9-Lecture1.html#parallel-pre-trend-assumption",
    "href": "Week9-Lecture1.html#parallel-pre-trend-assumption",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "2.8 Parallel Pre-trend Assumption",
    "text": "2.8 Parallel Pre-trend Assumption\n\nThe requirement for a valid DiD analysis is that there is no differential trend between the treatment and control group before the treatment happens, or we must need parallel pre-trend."
  },
  {
    "objectID": "Week9-Lecture1.html#sample-data",
    "href": "Week9-Lecture1.html#sample-data",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "3.1 Sample Data",
    "text": "3.1 Sample Data\n\nbase_did is a data frame with 1,040 observations and 6 variables named \\(y\\), \\(x1\\), \\(id\\), \\(period\\), \\(post\\) and \\(treat\\).\n\n\\(y\\): The outcome variable (e.g., retention) affected by the treatment (e.g., loyalty program).\n\\(x1\\): standardized customer income.\n\\(id\\): Identifier of the individual.\n\\(period\\): From 1 to 10\n\\(post\\): Indicator taking value 1 if the period is strictly greater than 5, 0 otherwise.\n\\(treat\\): Indicator taking value 1 if the individual is treated, 0 otherwise."
  },
  {
    "objectID": "Week9-Lecture1.html#dataset",
    "href": "Week9-Lecture1.html#dataset",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "3.2 Dataset",
    "text": "3.2 Dataset\n\npacman::p_load(fixest,modelsummary)\ndata(\"base_did\")\nhead(base_did,8)\n\n\n\n\n\n\n\ny\n\n\nx1\n\n\nid\n\n\nperiod\n\n\npost\n\n\ntreat\n\n\n\n\n\n\n2.8753063\n\n\n0.5365377\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n1.8606527\n\n\n-3.0431894\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n\n\n0.0941652\n\n\n5.5768439\n\n\n1\n\n\n3\n\n\n0\n\n\n1\n\n\n\n\n3.7814749\n\n\n-2.8300587\n\n\n1\n\n\n4\n\n\n0\n\n\n1\n\n\n\n\n-2.5581996\n\n\n-5.0443544\n\n\n1\n\n\n5\n\n\n0\n\n\n1\n\n\n\n\n1.7287324\n\n\n-0.6363849\n\n\n1\n\n\n6\n\n\n1\n\n\n1\n\n\n\n\n6.2842363\n\n\n-2.1298837\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n\n\n4.7668878\n\n\n3.4918558\n\n\n1\n\n\n8\n\n\n1\n\n\n1"
  },
  {
    "objectID": "Week9-Lecture1.html#estimation-of-did",
    "href": "Week9-Lecture1.html#estimation-of-did",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "3.3 Estimation of DiD",
    "text": "3.3 Estimation of DiD\n\n[in help(feols)] … You can interact a numeric variable with a “factor-like” variable by using i(factor_var, continuous_var, ref), where continuous_var will be interacted with each value of factor_var and the argument ref is a value of factor_var taken as a reference (optional).\n\n\n\n\nest_did = feols(\n  fml = y ~ x1 + i(period, treat, 1) | \n                  id + period, \n                data = base_did)"
  },
  {
    "objectID": "Week9-Lecture1.html#report-the-regression-results",
    "href": "Week9-Lecture1.html#report-the-regression-results",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "3.4 Report the Regression Results",
    "text": "3.4 Report the Regression Results\n\n\n\n\n\n\n (1)\n\n\n\n\nx1\n0.973***\n\n\n\n(0.046)\n\n\nperiod = 2 × treat\n0.156\n\n\n\n(1.121)\n\n\nperiod = 3 × treat\n1.130\n\n\n\n(1.071)\n\n\nperiod = 4 × treat\n−0.393\n\n\n\n(1.091)\n\n\nperiod = 5 × treat\n1.403\n\n\n\n(1.110)\n\n\nperiod = 6 × treat\n2.187*\n\n\n\n(1.025)\n\n\nperiod = 7 × treat\n5.002***\n\n\n\n(0.998)\n\n\nperiod = 8 × treat\n5.215***\n\n\n\n(1.155)\n\n\nperiod = 9 × treat\n6.134***\n\n\n\n(1.095)\n\n\nperiod = 10 × treat\n8.009***\n\n\n\n(1.072)\n\n\nNum.Obs.\n1080\n\n\nR2\n0.548\n\n\nR2 Adj.\n0.488\n\n\nR2 Within\n0.390\n\n\nR2 Within Adj.\n0.383\n\n\nAIC\n6223.2\n\n\nBIC\n6856.2\n\n\nRMSE\n3.84\n\n\nStd.Errors\nby: id\n\n\nFE: id\nX\n\n\nFE: period\nX\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Week9-Lecture1.html#plot-estimates-of-did",
    "href": "Week9-Lecture1.html#plot-estimates-of-did",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "3.5 Plot Estimates of DiD",
    "text": "3.5 Plot Estimates of DiD\n\niplot(est_did)"
  },
  {
    "objectID": "Week9-Lecture1.html#pre-trend-test",
    "href": "Week9-Lecture1.html#pre-trend-test",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "3.6 Pre-trend Test",
    "text": "3.6 Pre-trend Test\n\nThe treatment happens in period 5, we need to make sure that, the differences across the treatment group and control group are not statistically changing over time.\nIn terms of the regression coefficients, we need to make sure that the interactions between \\(treated\\) and \\(period\\) before the treatment are statistically insignificant."
  },
  {
    "objectID": "Week9-Lecture1.html#did-on-secondary-data-1",
    "href": "Week9-Lecture1.html#did-on-secondary-data-1",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "4.1 DiD on Secondary Data",
    "text": "4.1 DiD on Secondary Data\n\nAlthough DiD method can be combined with RCT to collect primary data, DiD is more commonly used on secondary data.\nFor your dissertation project, once you collect data from the companies, you can also work on a causal inference project using the DiD method."
  },
  {
    "objectID": "Week9-Lecture1.html#causal-effect-of-surge-pricing-on-drivers",
    "href": "Week9-Lecture1.html#causal-effect-of-surge-pricing-on-drivers",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "4.2 Causal Effect of Surge Pricing on Drivers",
    "text": "4.2 Causal Effect of Surge Pricing on Drivers\n\nMiao et al. (2022) study the causal effects of surge pricing on driver labor supply decisions. The ridesharing company introduced surge pricing in one city but not the other, such that we have a nice DiD setup:\n\nTreatment group: drivers in the city that implemented the surge pricing\nControl group: drivers in the city that did not implement the surge pricing"
  },
  {
    "objectID": "Week9-Lecture1.html#framework-of-analyses",
    "href": "Week9-Lecture1.html#framework-of-analyses",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "4.3 Framework of Analyses",
    "text": "4.3 Framework of Analyses\n\n\n\n\n\n\nWeekly income\n\nSurge pricing leads to higher weekly revenues.\n\nIntensive margin of labor supply\n\nSurge pricing leads to decreased daily revenues due to intensified competition.\n\nExtensive margin of labor supply\n\nDrivers have to work more days to make up of the decreased daily revenues."
  },
  {
    "objectID": "Week9-Lecture1.html#clustering-for-heterogeneity-analyses",
    "href": "Week9-Lecture1.html#clustering-for-heterogeneity-analyses",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "4.4 Clustering for Heterogeneity Analyses",
    "text": "4.4 Clustering for Heterogeneity Analyses\n\n\n\n\n\n\nWe use K-means clustering to segment out 2 clusters of drivers: full time and part time drivers.\n\nFull-time drivers have decreased weekly revenue due to capacity constraint.\nPart-time drivers flooded into the market and have increased weekly revenues by working more days.\nAlthough surge pricing enlarged the total pie for the company, the benefit was unevenly distributed among drivers."
  },
  {
    "objectID": "Week9-Lecture1.html#footnotes",
    "href": "Week9-Lecture1.html#footnotes",
    "title": "Class 17 Difference-in-Differences Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you collect any individual data in your dissertation, you need to seek research ethics approval from UCL.↩︎\nSee html version for answers.↩︎\nSee html version for answers.↩︎"
  },
  {
    "objectID": "Week9-Lecture2.html",
    "href": "Week9-Lecture2.html",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "",
    "text": "A regression discontinuity design (RDD) is a quasi-experimental design that aims to determine the causal effects of interventions by assigning a cutoff or threshold above or below which an intervention is assigned.\nIt was invented by educational psychology1 and generalized by economists to economics and business fields.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What is the causal effect of a Distinction honor on a student’s future salary?\n\n\n\n\n\n\n\n\n\nAn RDD arises when treatment is assigned based on whether an underlying continuous score variable crosses a cutoff.\n\nThe characteristic is often referred to as the running variable.\n\nAND the characteristic cannot be perfectly manipulated by individuals\n\nWe should only focus on individuals in the neighborhood of the cutoff point.\nWe can only estimate the local treatment effects from an RDD study.\n\n\n\n\n\n\nBecause the “running variable” cannot be perfectly controlled by the individuals around the cutoff point, it’s as if the treatment was randomly assigned in the neighborhood of cutoff.\nAt the same time, individuals on either side of the cut-off should be very similar to each other, such that there should be no systematic differences across the treatment and control group other than the treatment.\nWith the treatment being the only discontinuity at this threshold, a discontinuous jump in the outcome of interest at the threshold is the treatment effect."
  },
  {
    "objectID": "Week9-Lecture2.html#what-is-an-rdd",
    "href": "Week9-Lecture2.html#what-is-an-rdd",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "",
    "text": "A regression discontinuity design (RDD) is a quasi-experimental design that aims to determine the causal effects of interventions by assigning a cutoff or threshold above or below which an intervention is assigned.\nIt was invented by educational psychology1 and generalized by economists to economics and business fields."
  },
  {
    "objectID": "Week9-Lecture2.html#visual-illustration-of-rdd-an-example-of-distinction-on-salary",
    "href": "Week9-Lecture2.html#visual-illustration-of-rdd-an-example-of-distinction-on-salary",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "",
    "text": "Question: What is the causal effect of a Distinction honor on a student’s future salary?"
  },
  {
    "objectID": "Week9-Lecture2.html#when-to-use-an-rdd",
    "href": "Week9-Lecture2.html#when-to-use-an-rdd",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "",
    "text": "An RDD arises when treatment is assigned based on whether an underlying continuous score variable crosses a cutoff.\n\nThe characteristic is often referred to as the running variable.\n\nAND the characteristic cannot be perfectly manipulated by individuals\n\nWe should only focus on individuals in the neighborhood of the cutoff point.\nWe can only estimate the local treatment effects from an RDD study."
  },
  {
    "objectID": "Week9-Lecture2.html#why-rdd-gives-causal-effects",
    "href": "Week9-Lecture2.html#why-rdd-gives-causal-effects",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "",
    "text": "Because the “running variable” cannot be perfectly controlled by the individuals around the cutoff point, it’s as if the treatment was randomly assigned in the neighborhood of cutoff.\nAt the same time, individuals on either side of the cut-off should be very similar to each other, such that there should be no systematic differences across the treatment and control group other than the treatment.\nWith the treatment being the only discontinuity at this threshold, a discontinuous jump in the outcome of interest at the threshold is the treatment effect."
  },
  {
    "objectID": "Week9-Lecture2.html#step-1-select-sample-of-analysis",
    "href": "Week9-Lecture2.html#step-1-select-sample-of-analysis",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "2.1 Step 1: Select Sample of Analysis",
    "text": "2.1 Step 1: Select Sample of Analysis\n\nDetermine the cutoff-point and select the subset of individuals near the cut-off point\n\ne.g., filter out students with average scores between 69 and 70\n\n\n\nThere is no econometric requirement on the “near”; however, we face a trade-off between external validity and internal validity:\n\nExternal validity: If we have a narrower subset of individuals, we have a smaller subset of subjects which may not be representative of remaining individuals.\nInternal validity: If we have a broader subset of individuals, it is more likely the control group and treatment group are less likely to be “as-if randomized”.\n\nIn practice, we may need to run a set of different neighborhood bands as robustness checks."
  },
  {
    "objectID": "Week9-Lecture2.html#step-2-examine-continuity-of-observed-characteristics",
    "href": "Week9-Lecture2.html#step-2-examine-continuity-of-observed-characteristics",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "2.2 Step 2: Examine Continuity of Observed Characteristics",
    "text": "2.2 Step 2: Examine Continuity of Observed Characteristics\n\nExamine if the observed characteristics of the treatment group and control group are continuous at the cut-off point.\n\nThe idea is similar to “randomization check” in the Step 5 of an RCT."
  },
  {
    "objectID": "Week9-Lecture2.html#step-3-analysis",
    "href": "Week9-Lecture2.html#step-3-analysis",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "2.3 Step 3: Analysis",
    "text": "2.3 Step 3: Analysis\n\nRegress the outcome variable on the treatment indicator to obtain the statistical significance.\n\nIn R, there is also a package rddtools which can help us estimate an RDD model."
  },
  {
    "objectID": "Week9-Lecture2.html#causal-impact-of-distinction-on-salaries",
    "href": "Week9-Lecture2.html#causal-impact-of-distinction-on-salaries",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.1 Causal Impact of Distinction on Salaries",
    "text": "3.1 Causal Impact of Distinction on Salaries\n\nIt is important to understand the causal impact of degree honors on students’ future salaries and other outcomes.\nCan we get causal inference from simple linear regression?2\n\n\\[\nSalary_i = \\beta_0 + \\beta_1 Distinction_i + X\\beta + \\epsilon_i\n\\]\n\nNo, because there is severe omitted variable bias. Confounding factors may include a person’s ability, IQ, persistence, etc. These confounding factors are correlated with \\(Disinction_i\\) and also affects a person’s future salary. Not controlling these factors will cause omitted variable bias."
  },
  {
    "objectID": "Week9-Lecture2.html#rct-iv-or-did",
    "href": "Week9-Lecture2.html#rct-iv-or-did",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.2 RCT, IV or DID?",
    "text": "3.2 RCT, IV or DID?\n\nSince omitted variable bias prevents us from obtaining causal inference, we need to find another causal inference tool to overcome the challenge.\nHow about\n\nRCT\nInstrumental Variable\nDifference-in-Differences\n\nFortunately, we can use regression discontinuity design."
  },
  {
    "objectID": "Week9-Lecture2.html#dataset-for-rdd",
    "href": "Week9-Lecture2.html#dataset-for-rdd",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.3 Dataset for RDD",
    "text": "3.3 Dataset for RDD\n\nTo run RDD, we need to select students with very similar scores due to the tradeoff between internal validity and external validity.\n\nIn the selected dataset, scores range from 69.07 to 72.93\n\n\n\npacman::p_load(dplyr)\ndata_rdd &lt;- read.csv('https://www.dropbox.com/s/4f0zaqqkzo0at5o/data_rdd.csv?dl=1')\nhead(data_rdd,5)\n\n\n\n\n\nstudent_id\nsalary\nscore\nexperience\n\n\n\n\n1\n46.41270\n69.06849\n3.872425\n\n\n2\n47.55037\n69.15068\n3.236511\n\n\n3\n46.07215\n69.23288\n3.202071\n\n\n4\n44.21388\n69.31507\n3.280689\n\n\n5\n44.35247\n69.39726\n3.548198"
  },
  {
    "objectID": "Week9-Lecture2.html#data-wrangling",
    "href": "Week9-Lecture2.html#data-wrangling",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.4 Data Wrangling",
    "text": "3.4 Data Wrangling\n\nTo use RDD, we need to generate the treatment variable \\(treated\\), which equals 1 if a student receives the treatment and 0 otherwise.\n\nThe treatment in an RDD is in spirit similar to that of an RCT, only that the treatment is assigned by nature (hence the name “natural experiment”)\n\n\n\ndata_rdd &lt;- data_rdd %&gt;%\n  mutate(treated = ifelse(score&gt;=70,1,0))"
  },
  {
    "objectID": "Week9-Lecture2.html#rdd-analysis-using-r",
    "href": "Week9-Lecture2.html#rdd-analysis-using-r",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.5 RDD Analysis Using R",
    "text": "3.5 RDD Analysis Using R\n\npacman::p_load(modelsummary,fixest)\nrdd_result &lt;- feols(\n  fml = salary ~ treated,\n  data = data_rdd\n)\nmodelsummary(rdd_result,stars = TRUE)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n46.143***\n\n\n\n(0.479)\n\n\ntreated\n2.257***\n\n\n\n(0.553)\n\n\nNum.Obs.\n48\n\n\nR2\n0.266\n\n\nR2 Adj.\n0.250\n\n\nAIC\n186.8\n\n\nBIC\n190.5\n\n\nRMSE\n1.62\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Week9-Lecture2.html#variant-1-regression-discontinuity-in-time",
    "href": "Week9-Lecture2.html#variant-1-regression-discontinuity-in-time",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.6 Variant 1: Regression Discontinuity in Time",
    "text": "3.6 Variant 1: Regression Discontinuity in Time\n\nAn event or treatment occurred at a point in time. Meanwhile, the treatment affected all individuals.\n\nBecause all individuals were affected, there were no control group and we could not do DiD analyses.\n\nHowever, if we can justify, seasonality is not strong within certain time window before and after the event, then we can do a regression discontinuity in time design (RDiT), as follows:\n\n\\[\nOutcome_{i, t}=\\alpha+\\beta_{1} Post_{i}+X\\beta + \\mu_{i, t}\n\\]\n\nAs we learned in DiD lecture, \\(\\beta_1\\) includes both (1) the treatment effect, and (2) seasonality\n\nIf the time window is short, say a few weeks before and after, it is likely seasonality effect is null, and we can claim \\(\\beta_1\\) measures the causal effect of the event."
  },
  {
    "objectID": "Week9-Lecture2.html#variant-2-spatial-regression-discontinuity",
    "href": "Week9-Lecture2.html#variant-2-spatial-regression-discontinuity",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.7 Variant 2: Spatial Regression Discontinuity",
    "text": "3.7 Variant 2: Spatial Regression Discontinuity\n\nSome new policies/events may be region specific. For instance, In the US, each state has their independent laws and regulations, so a state’s new policy only affects that state but not other states.\nResidents near the same border should be similar in their characteristics, but only one side of the border receives the treatment.\n\nAs-if a randomized controlled trial"
  },
  {
    "objectID": "Week9-Lecture2.html#variant-2-spatial-regression-discontinuity-1",
    "href": "Week9-Lecture2.html#variant-2-spatial-regression-discontinuity-1",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.8 Variant 2: Spatial Regression Discontinuity",
    "text": "3.8 Variant 2: Spatial Regression Discontinuity\n\nThen we can compare the outcome of the treated residents and control residents near the border. Hence, spatial regression discontinuity is sometimes called border strategy.\n\nRefer to this paper for a comprehensive description of the topic."
  },
  {
    "objectID": "Week9-Lecture2.html#after-class-reading",
    "href": "Week9-Lecture2.html#after-class-reading",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "3.9 After-class Reading",
    "text": "3.9 After-class Reading\n\n(recommended) Quasi-experiment (Econometrics with R)"
  },
  {
    "objectID": "Week9-Lecture2.html#footnotes",
    "href": "Week9-Lecture2.html#footnotes",
    "title": "Class 18 Regression Discontinuity Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThistlethwaite, Donald L., and Donald T. Campbell. 1960. “Regression-Discontinuity Analysis: An Alternative to the Ex Post Facto Experiment.” Journal of Educational Psychology 51 (6): 309.↩︎\nRefer to html version for answers.↩︎"
  },
  {
    "objectID": "Week8-Lecture1.html",
    "href": "Week8-Lecture1.html",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "Task: Tesco would like to understand the causal impact of customer \\(Income\\) on customer \\(Spending\\)\n\n\nPlease run the two regressions on your laptop:\n\nRegression 1: \\(Spending\\) ~ \\(Income\\)\nRegression 2: \\(Spending\\) ~ \\(Income\\) + \\(Kidhome\\)\n\n\n\n\n\n\npacman::p_load(fixest,modelsummary)\n\nregression1 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income ) \n\nregression2 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income + Kidhome) \n\n\n\nmodelsummary(list(regression1,regression2),\n             stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n−556.823***\n−299.119***\n\n\n\n(21.654)\n(28.069)\n\n\nIncome\n0.022***\n0.019***\n\n\n\n(0.000)\n(0.000)\n\n\nKidhome\n\n−230.610***\n\n\n\n\n(16.945)\n\n\nNum.Obs.\n2000\n2000\n\n\nR2\n0.629\n0.661\n\n\nR2 Adj.\n0.629\n0.660\n\n\nAIC\n29306.1\n29130.7\n\n\nBIC\n29317.3\n29147.5\n\n\nRMSE\n367.45\n351.51\n\n\nStd.Errors\nIID\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\nQuestion: if we want to evaluate income’s causal effect on spending, which value (0.022, 0.019) should we use?\n\n\n\n\n\n\n\n\n\n\n\n\nDirect Effect (causal effect)\n\nKeeping other variables ﬁxed (ceteris paribus)\nDirect effect only\n\n\n\n\nTotal Effect\n\nIncluding side effects through other variables\nDirect + indirect effects\n\n\n\n\n\n\n\n\nTo obtain causal inference, we need to obtain the direct effects of an \\(X\\) variable on the outcome variable \\(Y\\).\nTotal effects include both direct effects and indirect effects (i.e., the impacts of other confounding variables).\nTherefore, it is important to include all confounding variables, which affect income and total spending at the same time, to control for the side effects from other variables.\n\n\n\n\n\nFor causal inference tasks, we need to use business senses to decide which confounding variables to control.\n\ngood controls and bad controls\n\nSometimes, control variables may be statistically insignificant, they should NOT be removed because they still serve the purpose of control variables.\nIf some variables are mechanically correlated, then we should not put all of them in the regression, to avoid perfect collinearity problems.\n\n\n\n\nQuestion: what is the best you can do with data_full to estimate the causal effect of income on spending?\n\n\n\n\nNow we have included Kidhome to tease out the effect of kids, what problems do we still have which hinder us from getting causal effect of income on total spending?\n\nDue to data availability, we are never able to include all confounding variables in the regression.\nStrictly speaking, we can never obtain causal effects from simple regression models based on non-experiment data.\nMathematically speaking, because we can never control all confounding factors, the error term is very likely to be correlated with income, violating \\(E[\\epsilon|X] = 0\\).\n\n\n\n\n\nWhy RCTs are the gold standard for causal inference?\n\nIf we are able to randomize people into different income groups, we can then collect the total_spending for each individual in each income group.\nWe can run a linear regression to examine the impact of income on total_spending.\n\n\n\\[\nSpending = \\beta_0 + \\beta_1Income + \\epsilon\n\\]\n\nIn the above regression\n\nAre there still any confounding effects?\nIs \\(Income\\) correlated with any of the confounding effects?"
  },
  {
    "objectID": "Week8-Lecture1.html#causal-effect-from-linear-regression-models",
    "href": "Week8-Lecture1.html#causal-effect-from-linear-regression-models",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "Task: Tesco would like to understand the causal impact of customer \\(Income\\) on customer \\(Spending\\)\n\n\nPlease run the two regressions on your laptop:\n\nRegression 1: \\(Spending\\) ~ \\(Income\\)\nRegression 2: \\(Spending\\) ~ \\(Income\\) + \\(Kidhome\\)"
  },
  {
    "objectID": "Week8-Lecture1.html#regression-results",
    "href": "Week8-Lecture1.html#regression-results",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "pacman::p_load(fixest,modelsummary)\n\nregression1 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income ) \n\nregression2 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income + Kidhome) \n\n\n\nmodelsummary(list(regression1,regression2),\n             stars = TRUE)\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n−556.823***\n−299.119***\n\n\n\n(21.654)\n(28.069)\n\n\nIncome\n0.022***\n0.019***\n\n\n\n(0.000)\n(0.000)\n\n\nKidhome\n\n−230.610***\n\n\n\n\n(16.945)\n\n\nNum.Obs.\n2000\n2000\n\n\nR2\n0.629\n0.661\n\n\nR2 Adj.\n0.629\n0.660\n\n\nAIC\n29306.1\n29130.7\n\n\nBIC\n29317.3\n29147.5\n\n\nRMSE\n367.45\n351.51\n\n\nStd.Errors\nIID\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\nQuestion: if we want to evaluate income’s causal effect on spending, which value (0.022, 0.019) should we use?"
  },
  {
    "objectID": "Week8-Lecture1.html#direct-and-indirect-effects",
    "href": "Week8-Lecture1.html#direct-and-indirect-effects",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "Direct Effect (causal effect)\n\nKeeping other variables ﬁxed (ceteris paribus)\nDirect effect only\n\n\n\n\nTotal Effect\n\nIncluding side effects through other variables\nDirect + indirect effects"
  },
  {
    "objectID": "Week8-Lecture1.html#causal-inference-from-regression-models",
    "href": "Week8-Lecture1.html#causal-inference-from-regression-models",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "To obtain causal inference, we need to obtain the direct effects of an \\(X\\) variable on the outcome variable \\(Y\\).\nTotal effects include both direct effects and indirect effects (i.e., the impacts of other confounding variables).\nTherefore, it is important to include all confounding variables, which affect income and total spending at the same time, to control for the side effects from other variables."
  },
  {
    "objectID": "Week8-Lecture1.html#practical-suggestions-for-running-regression-models",
    "href": "Week8-Lecture1.html#practical-suggestions-for-running-regression-models",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "For causal inference tasks, we need to use business senses to decide which confounding variables to control.\n\ngood controls and bad controls\n\nSometimes, control variables may be statistically insignificant, they should NOT be removed because they still serve the purpose of control variables.\nIf some variables are mechanically correlated, then we should not put all of them in the regression, to avoid perfect collinearity problems.\n\n\n\n\nQuestion: what is the best you can do with data_full to estimate the causal effect of income on spending?"
  },
  {
    "objectID": "Week8-Lecture1.html#causal-inference-from-regressions",
    "href": "Week8-Lecture1.html#causal-inference-from-regressions",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "Now we have included Kidhome to tease out the effect of kids, what problems do we still have which hinder us from getting causal effect of income on total spending?\n\nDue to data availability, we are never able to include all confounding variables in the regression.\nStrictly speaking, we can never obtain causal effects from simple regression models based on non-experiment data.\nMathematically speaking, because we can never control all confounding factors, the error term is very likely to be correlated with income, violating \\(E[\\epsilon|X] = 0\\)."
  },
  {
    "objectID": "Week8-Lecture1.html#rcts-and-causal-inference",
    "href": "Week8-Lecture1.html#rcts-and-causal-inference",
    "title": "Class 15 Endogeneity",
    "section": "",
    "text": "Why RCTs are the gold standard for causal inference?\n\nIf we are able to randomize people into different income groups, we can then collect the total_spending for each individual in each income group.\nWe can run a linear regression to examine the impact of income on total_spending.\n\n\n\\[\nSpending = \\beta_0 + \\beta_1Income + \\epsilon\n\\]\n\nIn the above regression\n\nAre there still any confounding effects?\nIs \\(Income\\) correlated with any of the confounding effects?"
  },
  {
    "objectID": "Week8-Lecture1.html#endogeneity",
    "href": "Week8-Lecture1.html#endogeneity",
    "title": "Class 15 Endogeneity",
    "section": "2.1 Endogeneity",
    "text": "2.1 Endogeneity\n\n2.1.1 Endogeneity\nEndogeneity refers to an econometric issue with OLS linear regression, in which an explanatory variable is correlated with the error term, such that the requirement for OLS linear regression \\(E[\\epsilon|X] = 0\\) is violated."
  },
  {
    "objectID": "Week8-Lecture1.html#cause-i-omitted-variable-bias",
    "href": "Week8-Lecture1.html#cause-i-omitted-variable-bias",
    "title": "Class 15 Endogeneity",
    "section": "2.2 Cause I: Omitted Variable Bias",
    "text": "2.2 Cause I: Omitted Variable Bias\n\n2.2.1 Omitted Variable Bias (OVB)\nAn omitted variable is a determinant of the outcome variable \\(y_i\\) that is correlated with the focal explanatory variable \\(x_i\\), but is not included in the regression, either due to data unavailability or ignorance of data scientists.\n\nTwo conditions for omitted variable bias\n\nThe variable affects the dependent variable.\nThe variable is correlated with the focal explanatory variable."
  },
  {
    "objectID": "Week8-Lecture1.html#examples-of-ovb",
    "href": "Week8-Lecture1.html#examples-of-ovb",
    "title": "Class 15 Endogeneity",
    "section": "2.3 Examples of OVB",
    "text": "2.3 Examples of OVB\n\nIf we would like to understand the causal effect of years in education on a person’s salary.\n\n\\[\nSalary_t = \\beta_0 + \\beta_1 Education_t + \\epsilon_t\n\\]\n\nCan we get causal effect from this regression? What would be the issue here?"
  },
  {
    "objectID": "Week8-Lecture1.html#examples-of-ovb-1",
    "href": "Week8-Lecture1.html#examples-of-ovb-1",
    "title": "Class 15 Endogeneity",
    "section": "2.4 Examples of OVB",
    "text": "2.4 Examples of OVB\n\nWhen building Marketing Mix Modeling, the common practice in the industry is to regress the sales in each period on the price in each period.\n\n\\[\nSales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n\\]\n\nHowever, is this regression correct?\n\nVery often, if we regress sales on price, we get a positive coefficient for price."
  },
  {
    "objectID": "Week8-Lecture1.html#cause-ii-reverse-causality-simultaneity",
    "href": "Week8-Lecture1.html#cause-ii-reverse-causality-simultaneity",
    "title": "Class 15 Endogeneity",
    "section": "2.5 Cause II: Reverse Causality (Simultaneity)",
    "text": "2.5 Cause II: Reverse Causality (Simultaneity)\n\n2.5.1 Reverse Causality\nReverse causality refers to the phenomenon that the independent variable \\(X_i\\) affects the dependent variable \\(y_i\\) and the dependent variable \\(y_i\\) also affects the independent variable \\(X_i\\) at the same time."
  },
  {
    "objectID": "Week8-Lecture1.html#examples-of-reverse-causality-simultaneity",
    "href": "Week8-Lecture1.html#examples-of-reverse-causality-simultaneity",
    "title": "Class 15 Endogeneity",
    "section": "2.6 Examples of Reverse Causality (Simultaneity)",
    "text": "2.6 Examples of Reverse Causality (Simultaneity)\n\nBesides potential omitted variable biases, there may also exist reverse causality problems with marketing mix modelling.\n\n\\[\nSales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n\\]\n\nPrice affects demand, and demand affects sellers’ price setting decisions.\n\nHigher price leads to lower sales. (X =&gt; Y)\nIf sellers expect higher demand, sellers may increase the price to increase profits. (Y =&gt; X)"
  },
  {
    "objectID": "Week8-Lecture1.html#examples-of-reverse-causality-simultaneity-1",
    "href": "Week8-Lecture1.html#examples-of-reverse-causality-simultaneity-1",
    "title": "Class 15 Endogeneity",
    "section": "2.7 Examples of Reverse Causality (Simultaneity)",
    "text": "2.7 Examples of Reverse Causality (Simultaneity)\n\nUberEat interview question: If we have historical data on number of restaurants on UberEat in each month, and the total number of orders in each month, can we run an OLS regression to get the causal effect?\n\n\\[\nNumOrders_t = \\beta_0 + \\beta_1 NumRestaurants_t + \\epsilon_t\n\\]\n\nIf not, how can we measure the causal effects for UberEat?\nThis question is not just limited to UberEat; it is in fact related to any platform business with network effect!\n\nAmazon; Airbnb; Uber Ridesharing; etc."
  },
  {
    "objectID": "Week8-Lecture1.html#main-takeaway",
    "href": "Week8-Lecture1.html#main-takeaway",
    "title": "Class 15 Endogeneity",
    "section": "2.8 Main Takeaway",
    "text": "2.8 Main Takeaway\n\nCommon threats to causal inference from secondary data include\n\nOmitted Variable Bias\nReverse Causality\n\nWe can overcome the endogeneity problem using instrumental variable method."
  },
  {
    "objectID": "Case-BreakEvenAnalysis.html",
    "href": "Case-BreakEvenAnalysis.html",
    "title": "Beak-Even Analysis for PineApple Inc",
    "section": "",
    "text": "4th October 2023: Tom Cooper, the suave Senior Marketing Manager of PineApple Inc, felt a rush of nostalgia as he stepped into the lecture halls of the UCL School of Management. Having graduated from UCL’s esteemed MSc Business Analytics programme, coming back felt like reuniting with an old friend. But, between us, the actual magnet pulling him back wasn’t just academic—it was the allure of the T4 Bubble Tea in Jubilee Place. A man’s got his priorities, right?\nThe Stage Is Set: Tom was looking to launch a series of marketing campaigns to further promote Pineapple’s new product, PinePhone 15, against its direct competitor, Apple Inc, who just launched its brand new iPhone 15 product line earlier this year.\nDiving Into The Figures: The marketing analytics team at PineApple Inc had applied predictive analytics models on historical sales data and predicted that the sales this year will reach 10 million units at the retail price of £600, without any additional marketing activities. The team had also collected the information on the Cost of Goods Sold of PinePhone 15, which is 60%. The Research and Development (R&D) costs for PinePhone 15 is 100 million pounds.\nIn the class Tom sat in, the module leader, Dr Wayne Meow, was introducing the concepts of break-even analysis and the methods to evaluate the feasibility of a marketing campaign, which was just handy for the task. Tom would like to use the concept of break-even analysis to help guide PineApple Inc’s marketing decisions.\nCalculating the break-even quantity is one way to determine the feasibility of a marketing campaign. The break-even quantity determines how many additional units the company must sell to cover the expense of the campaign. If the business sells fewer than the break-even quantity, it loses money since it does not sell enough to recover its investment. If the company sells more than the break-even quantity, the marketing campaign can be approved as it is profitable to the company.\nAfter a few months of researching and brainstorming, the marketing analytics team under Tom’s lead has come up with several proposals for Tom to decide. Tom, taking another sip of the delicious QQ Style Milk Tea (30% sugar, less ice) from T4, started to review the proposals."
  },
  {
    "objectID": "Case-BreakEvenAnalysis.html#marketing-decision-a-static-view",
    "href": "Case-BreakEvenAnalysis.html#marketing-decision-a-static-view",
    "title": "Beak-Even Analysis for PineApple Inc",
    "section": "1 Marketing Decision: A Static View",
    "text": "1 Marketing Decision: A Static View\nThe marketing analytics team has proposed a plan of an influencer marketing campaign. Influencer marketing is a type of social media marketing that entails endorsements and product placement by influencers, individuals and organizations with a reputed expert degree of knowledge or social influence in their industry. Influencers are individuals who have the ability to influence others’ purchasing habits other quantifiable activities by uploading original—often sponsored—content to social media platforms such as TikTok, Instagram, YouTube, Snapchat, or other social media platforms.\nThe team proposes to collaborate with the top tech influencers on Tiktok and Youtube to promote the new PinePhone 15. The one-off endorsement fee is estimated to be £50 million in total. And from historical data, the team estimates that such an influencer campaign can increase the total sales within the next financial year by 2.5%.\n\nQuestion 2:\n\nBased on the information at hand, should Tom approve the influencer marketing plan?"
  },
  {
    "objectID": "Case-BreakEvenAnalysis.html#marketing-decision-a-dynamic-view",
    "href": "Case-BreakEvenAnalysis.html#marketing-decision-a-dynamic-view",
    "title": "Beak-Even Analysis for PineApple Inc",
    "section": "2 Marketing Decision: A Dynamic View",
    "text": "2 Marketing Decision: A Dynamic View\nIn the afternoon, during a monthly board meeting, the CFO reported that the company was facing increased uncertainty regarding future cash flows due to the recent plunge of British Pounds and surge in interest rate. The current cost of financing, weighted average cost of capital (WACC), increased to 10% annually. Therefore, any marketing event is recommended to take time value of money into consideration.\nRight after the meeting, Tom asked his team for a decomposition of the predicted annual incremental sales, 2.5%, into a more granular monthly level analysis.\nThe team came back with the predicted monthly incremental sales: with influencer marketing, the first month sales will increase by 0.3% and 0.2% in the following 11 months.\n\nQuestion 3:\n\nBased on the information at hand, should Tom approve the influencer marketing plan based on Net Present Value method?"
  },
  {
    "objectID": "Week2-Lecture2.html",
    "href": "Week2-Lecture2.html",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "",
    "text": "Understand the major steps to conduct data analytics\nData collection: Learn how to collect first-hand data\nData cleaning: Learn how to use the dplyr package to collect, load, and clean data\nData analysis: Learn how to conduct descriptive analytics"
  },
  {
    "objectID": "Week2-Lecture2.html#class-objectives",
    "href": "Week2-Lecture2.html#class-objectives",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "",
    "text": "Understand the major steps to conduct data analytics\nData collection: Learn how to collect first-hand data\nData cleaning: Learn how to use the dplyr package to collect, load, and clean data\nData analysis: Learn how to conduct descriptive analytics"
  },
  {
    "objectID": "Week2-Lecture2.html#overview-1",
    "href": "Week2-Lecture2.html#overview-1",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "2.1 Overview",
    "text": "2.1 Overview"
  },
  {
    "objectID": "Week2-Lecture2.html#collect-data",
    "href": "Week2-Lecture2.html#collect-data",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "2.2 Collect Data",
    "text": "2.2 Collect Data\n\nPrimary Data: Data that are generated by the researcher himself/herself, surveys, interviews, experiments, specially designed for understanding and solving the research problem at hand.\nSecondary Data: Existing data generated by the company’s or consumer’s past activities, as part of organizational record keeping."
  },
  {
    "objectID": "Week2-Lecture2.html#collect-data-marketing-surveys",
    "href": "Week2-Lecture2.html#collect-data-marketing-surveys",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "2.3 Collect Data: Marketing Surveys",
    "text": "2.3 Collect Data: Marketing Surveys\n\nIn a marketing survey, we typically would like to solicit the following\n\npurchase intention\nwillingness to pay (WTP)\nshopping basket\nshare of wallet (SoW)\ndemographics\n\nLet’s see an example of how to design a simple marketing survey!\nUseful supplementary readings if you need to design marketing surveys\n\nThe quick start guide on how to conduct market research"
  },
  {
    "objectID": "Week2-Lecture2.html#data-frames",
    "href": "Week2-Lecture2.html#data-frames",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.1 Data Frames",
    "text": "3.1 Data Frames\n\nData Frame is the R object that we will deal with most of the time in the MSc program. You can think of data.frame as a spreadsheet in excel\nEach row stands for an observation\nEach column stands for a variable; each column should have a unique name.\n\nEach column must contain the same data type, but the different columns can store different data types.\n\ncompare with matrix?\n\n\nEach column must be of same length, because rows have the same length across variables."
  },
  {
    "objectID": "Week2-Lecture2.html#install-and-load-the-dplyr-package",
    "href": "Week2-Lecture2.html#install-and-load-the-dplyr-package",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.2 Install and Load the dplyr package",
    "text": "3.2 Install and Load the dplyr package\n\nIn R, we will be using the dplyr package for data cleaning and manipulation.\n\n\ninstall.packages(\"dplyr\")\n\n\nLoad the package\n\n\nlibrary(dplyr)\n\n\nLoad a built-in dataset called mtcars using data()\n\n\ndata(\"mtcars\")\n\n\nTo browse the whole dataset, we can simply click the dataset in the environment\n\nIt may takes time to view a huge dataset"
  },
  {
    "objectID": "Week2-Lecture2.html#subset-rows-based-on-conditions-filter",
    "href": "Week2-Lecture2.html#subset-rows-based-on-conditions-filter",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.3 Subset Rows Based on Conditions: filter",
    "text": "3.3 Subset Rows Based on Conditions: filter\n\nWe can use filter() to extract rows that meet logical criteria.\n\nWe can also add multiple criteria separated by comma\n\n\n\n# show all cars with more than 4 gears\nfilter(mtcars, gear == 4 )"
  },
  {
    "objectID": "Week2-Lecture2.html#the-pipe-operator",
    "href": "Week2-Lecture2.html#the-pipe-operator",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.4 The Pipe Operator %>%",
    "text": "3.4 The Pipe Operator %&gt;%\n\n3.4.1 Pipe Operator\n%&gt;%, or pipe operator, will forward a value, or the result of an expression, into the next function call/expression.\n\n\nmtcars %&gt;% filter(gear == 4) %&gt;% head()"
  },
  {
    "objectID": "Week2-Lecture2.html#sort-rows-arrange",
    "href": "Week2-Lecture2.html#sort-rows-arrange",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.5 Sort Rows: arrange",
    "text": "3.5 Sort Rows: arrange\narrange() orders the rows of a data frame by the values of selected columns.\n\nThe default is by ascending order; for descending order, put a minus sign before the variable.\n\n\n# reorder mtcar based on hp\nmtcars %&gt;% \n  arrange(hp) %&gt;%\n  head()"
  },
  {
    "objectID": "Week2-Lecture2.html#generate-new-variables-mutate",
    "href": "Week2-Lecture2.html#generate-new-variables-mutate",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.6 Generate New Variables: mutate",
    "text": "3.6 Generate New Variables: mutate\nmutate() adds new variables and preserves existing ones\n\nmtcars %&gt;%\n  mutate(sqrt_mpg = sqrt(mpg))%&gt;%\n  head()"
  },
  {
    "objectID": "Week2-Lecture2.html#important-tips",
    "href": "Week2-Lecture2.html#important-tips",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.7 Important Tips",
    "text": "3.7 Important Tips\n\nEach dplyr operation does not overwrite the original data frame, therefore, we must assign the object back if we wish to overwrite the previous data frame.\nExercises:\n\nfind car models with gear equal to 4 and mpg larger than 15\nreorder the above dataset by wt from large to small.\ngenerate a new column which computes the ratio of mpg to wt"
  },
  {
    "objectID": "Week2-Lecture2.html#after-class-exercise",
    "href": "Week2-Lecture2.html#after-class-exercise",
    "title": "Class 4 Data Wrangling with R Part I",
    "section": "3.8 After-Class Exercise",
    "text": "3.8 After-Class Exercise\n\nData camp dplyr exercise\nRead “Preliminary Customer Analyses” dataset, and try to solve the case questions using the techniques learned today"
  },
  {
    "objectID": "Case-Zalora.html",
    "href": "Case-Zalora.html",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "",
    "text": "Load the Tesco dataset and necessary packages\n\n\npacman::p_load(dplyr)\n# Load both datasets\ndata_purchase &lt;- read.csv(file = \"https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1\", \n                      header = T)\n\ndata_demo &lt;- read.csv(\"https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1\",\n                      header = T)\n\n# Left join demographic data into purchase data\ndata_full &lt;- data_purchase %&gt;%\n  left_join(data_demo, by = \"ID\")\n\n# Handle Missing Values of Income\ndata_full &lt;- data_full %&gt;%\n  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T))) %&gt;%\n  mutate(total_spending = MntFishProducts + MntFruits + MntGoldProds + MntMeatProducts + MntSweetProducts + MntWines)\n\n\nRun a linear regression by regressing total_spending on Income\n\n\npacman::p_load(modelsummary,fixest)\n\nOLS_result &lt;- feols( \n   fml = total_spending ~ Income, # Y ~ X\n   data = data_full, # dataset from Tesco\n   ) \n\n\nGenerate the regression result table using modelsummary() from the modelsummary package.\n\n\nmodelsummary(OLS_result,\n    stars = TRUE,  # export statistical significance\n  )\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n−556.823***\n\n\n\n(21.654)\n\n\nIncome\n0.022***\n\n\n\n(0.0004)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.629\n\n\nR2 Adj.\n0.629\n\n\nRMSE\n367.45\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun a linear regression by regressing total_spending on Income and Kidhome\n\n\nfeols(data = data_full,\n     fml = total_spending ~ Income + Kidhome) %&gt;%\n  modelsummary(stars = T)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n−299.119***\n\n\n\n(28.069)\n\n\nIncome\n0.019***\n\n\n\n(0.0004)\n\n\nKidhome\n−230.610***\n\n\n\n(16.945)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.661\n\n\nR2 Adj.\n0.660\n\n\nRMSE\n351.51\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-Zalora.html#univariate-regression",
    "href": "Case-Zalora.html#univariate-regression",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "",
    "text": "Load the Tesco dataset and necessary packages\n\n\npacman::p_load(dplyr)\n# Load both datasets\ndata_purchase &lt;- read.csv(file = \"https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1\", \n                      header = T)\n\ndata_demo &lt;- read.csv(\"https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1\",\n                      header = T)\n\n# Left join demographic data into purchase data\ndata_full &lt;- data_purchase %&gt;%\n  left_join(data_demo, by = \"ID\")\n\n# Handle Missing Values of Income\ndata_full &lt;- data_full %&gt;%\n  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T))) %&gt;%\n  mutate(total_spending = MntFishProducts + MntFruits + MntGoldProds + MntMeatProducts + MntSweetProducts + MntWines)\n\n\nRun a linear regression by regressing total_spending on Income\n\n\npacman::p_load(modelsummary,fixest)\n\nOLS_result &lt;- feols( \n   fml = total_spending ~ Income, # Y ~ X\n   data = data_full, # dataset from Tesco\n   ) \n\n\nGenerate the regression result table using modelsummary() from the modelsummary package.\n\n\nmodelsummary(OLS_result,\n    stars = TRUE,  # export statistical significance\n  )\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n−556.823***\n\n\n\n(21.654)\n\n\nIncome\n0.022***\n\n\n\n(0.0004)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.629\n\n\nR2 Adj.\n0.629\n\n\nRMSE\n367.45\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-Zalora.html#multivariate-regression",
    "href": "Case-Zalora.html#multivariate-regression",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "",
    "text": "Run a linear regression by regressing total_spending on Income and Kidhome\n\n\nfeols(data = data_full,\n     fml = total_spending ~ Income + Kidhome) %&gt;%\n  modelsummary(stars = T)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n−299.119***\n\n\n\n(28.069)\n\n\nIncome\n0.019***\n\n\n\n(0.0004)\n\n\nKidhome\n−230.610***\n\n\n\n(16.945)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.661\n\n\nR2 Adj.\n0.660\n\n\nRMSE\n351.51\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-Zalora.html#factor-variables",
    "href": "Case-Zalora.html#factor-variables",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "2.1 Factor variables",
    "text": "2.1 Factor variables\n\nGenerate a factor variable for Education\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor = factor(Education))\n\n# check levels of a factor\nlevels(data_full$Education_factor)\n\n[1] \"2n Cycle\"   \"Basic\"      \"Graduation\" \"Master\"     \"PhD\"       \n\n\n\nChange the baseline group to “Basic”\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor_2 = relevel(Education_factor, \n                                      ref = \"Basic\") )\n\nlevels(data_full$Education_factor_2)\n\n[1] \"Basic\"      \"2n Cycle\"   \"Graduation\" \"Master\"     \"PhD\"       \n\n\n\nRun a regression with factor variable Education_factor_2\n\n\nfeols_categorical &lt;- feols(data = data_full,\n  total_spending ~ Income + Kidhome + Education_factor_2)\n\n# report the result\nmodelsummary(feols_categorical,\n             stars = T)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n−180.297**\n\n\n\n(56.305)\n\n\nIncome\n0.020***\n\n\n\n(0.0004)\n\n\nKidhome\n−227.761***\n\n\n\n(16.961)\n\n\nEducation_factor_22n Cycle\n−164.044**\n\n\n\n(60.448)\n\n\nEducation_factor_2Graduation\n−119.695*\n\n\n\n(56.176)\n\n\nEducation_factor_2Master\n−143.015*\n\n\n\n(58.443)\n\n\nEducation_factor_2PhD\n−153.190**\n\n\n\n(57.751)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.662\n\n\nR2 Adj.\n0.661\n\n\nRMSE\n350.59\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nfeols_categorical_2 &lt;- feols(data = data_full,\n  total_spending ~ Income + Kidhome + Education_factor)\n\n# report the result\nmodelsummary(feols_categorical_2,\n             stars = T)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n−344.341***\n\n\n\n(36.730)\n\n\nIncome\n0.020***\n\n\n\n(0.0004)\n\n\nKidhome\n−227.761***\n\n\n\n(16.961)\n\n\nEducation_factorBasic\n164.044**\n\n\n\n(60.448)\n\n\nEducation_factorGraduation\n44.349\n\n\n\n(28.188)\n\n\nEducation_factorMaster\n21.029\n\n\n\n(32.381)\n\n\nEducation_factorPhD\n10.854\n\n\n\n(30.796)\n\n\nNum.Obs.\n2000\n\n\nR2\n0.662\n\n\nR2 Adj.\n0.661\n\n\nRMSE\n350.59\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-Zalora.html#non-linear-effects",
    "href": "Case-Zalora.html#non-linear-effects",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "2.2 Non-linear Effects",
    "text": "2.2 Non-linear Effects\n\nGenerate a quadratic term of Income\n\n\ndata_full &lt;- data_full %&gt;%\n  mutate(Income_quadartic = Income^2 )\n\n\nRun 2 models with and without the quadratic term\n\n\n# model 1: without quadratic term\nfeols_noquadratic &lt;- feols(data = data_full,\n  fml =  total_spending ~ Income )\n\n# model 2: with quadratic term\nfeols_quadratic &lt;- feols(data = data_full,\n  fml =  total_spending ~ Income  + Income_quadartic )\n\n\nReport the results from the two regressions\n\n\nmodelsummary(list(feols_noquadratic,feols_quadratic),\n             stars = T)\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n\n(Intercept)\n−556.823***\n−627.040***\n\n\n\n(21.654)\n(36.522)\n\n\nIncome\n0.022***\n0.025***\n\n\n\n(0.0004)\n(0.001)\n\n\nIncome_quadartic\n\n−3e−08*\n\n\n\n\n(1e−08)\n\n\nNum.Obs.\n2000\n2000\n\n\nR2\n0.629\n0.630\n\n\nR2 Adj.\n0.629\n0.630\n\n\nRMSE\n367.45\n366.92\n\n\nStd.Errors\nIID\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "Case-Zalora.html#data-wrangling",
    "href": "Case-Zalora.html#data-wrangling",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "3.1 Data Wrangling",
    "text": "3.1 Data Wrangling\n\n3.1.1 Load data\n\ndata_zalora &lt;- read.csv('https://www.dropbox.com/s/ok2b7pcudo9c5m5/519701-XLS-ENG.csv?dl=1')\n\n\n\n3.1.2 Examine the data types\n\nstr(data_zalora)\n\n'data.frame':   3847 obs. of  12 variables:\n $ SKU                : chr  \"00075ZZA9550E1GS\" \"00075ZZA9550E1GS\" \"00075ZZA9550E1GS\" \"00075ZZA9550E1GS\" ...\n $ Color              : chr  \"Black\" \"Black\" \"Black\" \"Black\" ...\n $ Week_of_Year       : int  36 38 39 43 49 52 33 35 38 39 ...\n $ Brand_Name         : chr  \"ZALORA\" \"ZALORA\" \"ZALORA\" \"ZALORA\" ...\n $ Units_Sold         : int  6 2 3 3 17 16 3 6 3 14 ...\n $ Unit_Price         : num  39.9 39.9 39.9 29.9 19.9 19.9 34.9 34.9 32.4 29.9 ...\n $ Web_SKU_Clicks     : int  1121 1215 1112 2336 190 635 177 359 3940 6144 ...\n $ Web_SKU_Impressions: int  37237 23405 19632 43152 4196 12690 4379 10553 100899 237176 ...\n $ Item_Cost          : num  15.5 15.5 15.5 15.5 15.5 ...\n $ Price_Band         : int  4 4 4 4 4 4 4 4 4 4 ...\n $ Weeks_Since_Release: int  1 3 4 8 14 17 1 3 6 7 ...\n $ Stock              : int  60 54 52 49 46 29 92 89 83 80 ...\n\n\n\n\n3.1.3 Summary Statistics\n\ndatasummary_skim(data_zalora)\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\n\nWeek_of_Year\n52\n0\n40.3\n9.8\n1.0\n43.0\n52.0\n\n\n\nUnits_Sold\n67\n0\n8.9\n9.5\n1.0\n6.0\n91.0\n\n\n\nUnit_Price\n548\n0\n32.5\n12.5\n9.9\n29.9\n129.9\n\n\n\nWeb_SKU_Clicks\n1924\n0\n1721.5\n2772.1\n4.0\n673.0\n26720.0\n\n\n\nWeb_SKU_Impressions\n3640\n0\n35892.2\n63882.4\n11.0\n11919.0\n825936.0\n\n\n\nItem_Cost\n419\n0\n14.1\n3.7\n4.7\n13.8\n45.2\n\n\n\nPrice_Band\n10\n0\n4.1\n1.1\n1.0\n4.0\n10.0\n\n\n\nWeeks_Since_Release\n43\n0\n8.0\n7.5\n1.0\n6.0\n46.0\n\n\n\nStock\n230\n0\n66.7\n45.3\n1.0\n56.0\n298.0\n\n\n\n\n\n\n\n\n\n\n3.1.4 Variable Operationalization\nWhen building marketing mix modeling, we need to transform categorical variables into factor variables in R. Think about the following variables:\n\nWeek_of_Year should be treated as a categorical variable, because seasonality should not be linear in time.\n\n\n#  Change Week_of_Year into a factor\ndata_zalora &lt;- data_zalora %&gt;%\n  mutate(Week_of_Year = factor(Week_of_Year))\n\n\nBrand_Name and Price_Band should also be converted into factor variables.\n\n\n# complete the code below\ndata_zalora &lt;- data_zalora %&gt;%\n  mutate(Brand_Name = factor(Brand_Name),\n         Price_Band = factor(Price_Band))"
  },
  {
    "objectID": "Case-Zalora.html#regression-analysis",
    "href": "Case-Zalora.html#regression-analysis",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "3.2 Regression Analysis",
    "text": "3.2 Regression Analysis\n\n\nRun the above multivariate regression in R, where the dependent variable is Units_Sold and the explanatory variables are Unit_Price and the others.\n\n\n# If we do not care about the estimates of fixed effects\n\nresult_ols2 &lt;- feols(\n  fml =  Units_Sold ~ Unit_Price + Weeks_Since_Release +\n    Price_Band + Brand_Name + Week_of_Year,\n  data = data_zalora\n)\n\nmodelsummary(result_ols2,\n             stars = TRUE)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n14.608*\n\n\n\n(7.189)\n\n\nUnit_Price\n−0.412***\n\n\n\n(0.029)\n\n\nWeeks_Since_Release\n−0.282***\n\n\n\n(0.029)\n\n\nPrice_Band2\n−1.807\n\n\n\n(3.783)\n\n\nPrice_Band3\n−0.481\n\n\n\n(3.589)\n\n\nPrice_Band4\n2.027\n\n\n\n(3.582)\n\n\nPrice_Band5\n4.676\n\n\n\n(3.606)\n\n\nPrice_Band6\n6.139\n\n\n\n(3.777)\n\n\nPrice_Band7\n11.266**\n\n\n\n(3.833)\n\n\nPrice_Band8\n15.093***\n\n\n\n(4.014)\n\n\nPrice_Band9\n19.105***\n\n\n\n(4.385)\n\n\nPrice_Band10\n28.518***\n\n\n\n(4.997)\n\n\nBrand_NameZALORA\n0.464\n\n\n\n(0.320)\n\n\nWeek_of_Year2\n−3.099\n\n\n\n(7.126)\n\n\nWeek_of_Year3\n1.003\n\n\n\n(7.297)\n\n\nWeek_of_Year4\n5.009\n\n\n\n(7.965)\n\n\nWeek_of_Year5\n5.138\n\n\n\n(6.994)\n\n\nWeek_of_Year6\n−1.787\n\n\n\n(7.963)\n\n\nWeek_of_Year7\n3.975\n\n\n\n(7.963)\n\n\nWeek_of_Year8\n0.613\n\n\n\n(7.554)\n\n\nWeek_of_Year9\n6.766\n\n\n\n(7.299)\n\n\nWeek_of_Year10\n3.478\n\n\n\n(7.301)\n\n\nWeek_of_Year11\n2.958\n\n\n\n(6.895)\n\n\nWeek_of_Year12\n2.996\n\n\n\n(6.758)\n\n\nWeek_of_Year13\n5.265\n\n\n\n(6.626)\n\n\nWeek_of_Year14\n4.064\n\n\n\n(6.706)\n\n\nWeek_of_Year15\n2.840\n\n\n\n(6.896)\n\n\nWeek_of_Year16\n2.894\n\n\n\n(6.544)\n\n\nWeek_of_Year17\n4.654\n\n\n\n(6.402)\n\n\nWeek_of_Year18\n4.274\n\n\n\n(6.567)\n\n\nWeek_of_Year19\n6.291\n\n\n\n(6.444)\n\n\nWeek_of_Year20\n5.714\n\n\n\n(6.402)\n\n\nWeek_of_Year21\n4.199\n\n\n\n(6.354)\n\n\nWeek_of_Year22\n3.272\n\n\n\n(6.393)\n\n\nWeek_of_Year23\n6.793\n\n\n\n(6.365)\n\n\nWeek_of_Year24\n8.726\n\n\n\n(6.302)\n\n\nWeek_of_Year25\n4.851\n\n\n\n(6.308)\n\n\nWeek_of_Year26\n5.636\n\n\n\n(6.309)\n\n\nWeek_of_Year27\n11.933+\n\n\n\n(6.301)\n\n\nWeek_of_Year28\n6.108\n\n\n\n(6.292)\n\n\nWeek_of_Year29\n8.166\n\n\n\n(6.327)\n\n\nWeek_of_Year30\n5.859\n\n\n\n(6.346)\n\n\nWeek_of_Year31\n10.385\n\n\n\n(6.315)\n\n\nWeek_of_Year32\n5.501\n\n\n\n(6.272)\n\n\nWeek_of_Year33\n5.980\n\n\n\n(6.239)\n\n\nWeek_of_Year34\n8.400\n\n\n\n(6.239)\n\n\nWeek_of_Year35\n8.738\n\n\n\n(6.223)\n\n\nWeek_of_Year36\n6.126\n\n\n\n(6.229)\n\n\nWeek_of_Year37\n8.147\n\n\n\n(6.229)\n\n\nWeek_of_Year38\n6.702\n\n\n\n(6.220)\n\n\nWeek_of_Year39\n6.285\n\n\n\n(6.216)\n\n\nWeek_of_Year40\n7.229\n\n\n\n(6.220)\n\n\nWeek_of_Year41\n9.075\n\n\n\n(6.208)\n\n\nWeek_of_Year42\n5.403\n\n\n\n(6.208)\n\n\nWeek_of_Year43\n5.637\n\n\n\n(6.201)\n\n\nWeek_of_Year44\n5.304\n\n\n\n(6.207)\n\n\nWeek_of_Year45\n6.763\n\n\n\n(6.201)\n\n\nWeek_of_Year46\n13.841*\n\n\n\n(6.206)\n\n\nWeek_of_Year47\n4.474\n\n\n\n(6.200)\n\n\nWeek_of_Year48\n13.203*\n\n\n\n(6.201)\n\n\nWeek_of_Year49\n6.688\n\n\n\n(6.201)\n\n\nWeek_of_Year50\n4.299\n\n\n\n(6.197)\n\n\nWeek_of_Year51\n12.704*\n\n\n\n(6.201)\n\n\nWeek_of_Year52\n3.666\n\n\n\n(6.202)\n\n\nNum.Obs.\n3847\n\n\nR2\n0.169\n\n\nR2 Adj.\n0.155\n\n\nRMSE\n8.65\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nExercise: How would you interpret the coefficients in this regression table?"
  },
  {
    "objectID": "Case-Zalora.html#pricing-optimization",
    "href": "Case-Zalora.html#pricing-optimization",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "3.3 Pricing Optimization",
    "text": "3.3 Pricing Optimization\n\n3.3.1 If we use the same price level\n\nLoad the data for the focal product, SKU #0D159AAFC1DF38GS.\n\n\ndata_SKU &lt;- read.csv('https://www.dropbox.com/s/hdl3gw6utjirvf4/data_sku.csv?dl=1')\n\n\nIf keep the same pricing from week 3, $34.9, for the remaining weeks, predict the weekly sales.\n\n\npacman::p_load(dplyr)\ndata_SKU_afterweek3 &lt;- data_SKU %&gt;%\n  filter(Weeks_Since_Release &gt; 3) %&gt;% # only keep data after week 3\n  mutate(Unit_Price = 34.9)  # fill in unit price as 34.9\n\n# predict the weekly sales from Marketing Mix Modeling\npredicted_weekly_sales &lt;- round(predict(result_ols2,data_SKU_afterweek3),0)\npredicted_weekly_sales\n\n [1]  0 -1 -1 -1 -2 -2 -2 -2 -3 -3 -3 -4 -4 -4\n\n# fill in the predicted sales\ndata_SKU_afterweek3 &lt;- data_SKU_afterweek3%&gt;% \n  mutate(predicted_sales = predicted_weekly_sales)%&gt;%\n  mutate(Stock = pmax(39 +predicted_weekly_sales[1] - (cumsum(predicted_sales)),0) )%&gt;%\n  mutate(Profit = pmin(predicted_sales,Stock) * (Unit_Price - Item_Cost) )\n\n\ndata_SKU_afterweek3 %&gt;%\n  select(Weeks_Since_Release, Unit_Price, predicted_sales, Stock, Profit)\n\n\n\n  \n\n\n\n\n# compute the total profits\nsum(data_SKU_afterweek3$Profit)\n\n[1] -624.32\n\n\n\nFrom the above analyses, it seems if Zalora keeps using $34.9, by the end of week 7, all products will be sold out. And the total profits will be 800 dollars.\n\n\n\n3.3.2 Profit Optimization1\n\nWrite a user-defined function to compute the revenue each week conditional on a new value of price\n\n\n# define a function, given the price input, return the revenue\ncompute_revenue &lt;- function(price, week_of_year) {\n  data_SKU_week &lt;- data_SKU%&gt;%\n    filter(Week_of_Year == as.character(week_of_year) ) %&gt;%\n    mutate(Unit_Price = price)\n  \n  # predict the sales\n  sales &lt;- round(predict(result_ols2,\n                   data_SKU_week),0)\n  \n  # compute revenue\n  revenue &lt;- sales * price\n  \n  # compute profits\n  profit &lt;- sales * (price - 15.39)\n  \n  return(list(revenue, # return all results as a R list\n              profit,\n              sales))\n}\n\n\nUse optimization solver to find the best price, that can maximize profit for each individual week\n\n\nfor (i_iter in c(1:nrow(data_SKU_afterweek3))){\n  data_SKU_afterweek3$Unit_Price[i_iter] &lt;- optimize(\n    f =  \\(x) {-1 * compute_revenue(x,data_SKU_afterweek3$Week_of_Year[i_iter])[[2]]},\n         c(0,100))$minimum\n  \n  print(data_SKU_afterweek3$Unit_Price[i_iter])\n}\n\n[1] 25.32307\n[1] 24.63913\n[1] 23.95517\n[1] 23.27124\n[1] 25.01213\n[1] 24.32814\n[1] 23.64422\n[1] 22.96025\n[1] 22.2763\n[1] 24.01721\n[1] 20.9084\n[1] 22.64926\n[1] 21.96535\n[1] 21.28136\n\n\n\nCompute the sales and profit from the prices that maximize profit for each individual week\n\n\n# predict the weekly sales from Marketing Mix Modeling\npredicted_weekly_sales &lt;- round(predict(result_ols2,data_SKU_afterweek3),0)\n\n# fill in the predicted sales\ndata_SKU_afterweek3 &lt;- data_SKU_afterweek3%&gt;% \n  mutate(predicted_sales = predicted_weekly_sales)%&gt;%\n  mutate(Stock = pmax(39 +predicted_weekly_sales[1] - (cumsum(predicted_sales)),0) ) %&gt;% # calculate the remaining stock from last week\n  \n  mutate(Profit = pmin(predicted_sales,Stock) * (Unit_Price - Item_Cost) )\n\n\n# print out the dataset\ndata_SKU_afterweek3 %&gt;%\n  select(Weeks_Since_Release, Unit_Price, predicted_sales, Stock, Profit)\n\n\n\n  \n\n\n\n\n# compute the total profits from data-driven pricing\nsum(data_SKU_afterweek3$Profit)\n\n[1] 333.6977\n\n\n\nFrom the above analyses, it seems if Zalora uses profit maximization pricing, by the end of week 7, all products will be sold out. And the total profits will be 911 dollars.\n\nWe observe a huge boost in profits after we use marketing mix models and data-driven pricing analytics!"
  },
  {
    "objectID": "Case-Zalora.html#footnotes",
    "href": "Case-Zalora.html#footnotes",
    "title": "Zalora: Data-Driven Pricing Recommendations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou will learn how to use optimization solver in term 2 Operations Analytics.↩︎"
  },
  {
    "objectID": "Week2-Lecture1.html",
    "href": "Week2-Lecture1.html",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Marketing activities need to create value for the company\n\n\n\n\n\nAny marketing activity (in fact, any business activity)\n\nincurs some marketing expense/investment costs\ngenerates benefits for the company (e.g., incremental sales; higher customer retention rate)\n\nThe core idea of a break-even analysis is to compare the benefit with the cost\n\nBEA is sometimes called cost-benefit analysis.\n\n\n\n\n\n\nFor a marketing campaign with fixed marketing expenditure with short-term impacts, we can compute BEQ to evaluate its feasibility\nBEQ calculates the number of incremental units the firm needs to sell to cover the cost of the marketing campaign.\n\nIncremental because we are comparing with status quo\n\n\n\n\n\n\nContribution Margin Per Unit = Price Per Unit - Variable Costs Per Unit\n\nMeasures how much money each additional sale “contributes” to the company’s total profits.\ncontribution margin rate1 = contribution margin per unit / price per unit\n\n\n\n\nBreak-Even Quantity = Marketing Expenditure / Contribution Margin Per Unit\nCompare BEQ with estimated incremental sales to finish break-even analyses\n\nMarketing costs are usually easy to obtain through budgeting\nIncremental sales will need to be estimated through causal inference tools (i.e., the causal impact of influencer marketing on sales)\n\nIf the estimated incremental sales can exceed BEQ, approve the marketing campaign\n\n\n\n\n\nAssign values to R objects based on case background information\n\n\nprice &lt;- 600 # retail price\nquantity &lt;- 10 # sales; it's 10 million, bear this unit in mind\nCOGS &lt;- 0.6 # cost of goods sold in percentage terms\nRD_costs &lt;- 100 # R&D Costs\nendorsement_fee &lt;- 50 # fixed marketing expenditure\n\n\nquantity is 10 million; we use 10 for brevity\n\nSales refers to quantity sales by industry practice\nRevenue or revenue sales refers to monetary sales\n\nCOGS is the variable costs per unit in the BEQ formula\n\nUsed in both percentage or value terms interchangeably.\n\nR&D costs are sunk costs\n\nShould sunk costs be considered in a BEA for a marketing campaign?\n\n\n\n\n\n\nCompute the contribution margin per unit\n\n\n# Following the definition\n# contribution margin per unit = price - variable cost\ncontribution_margin_per_unit &lt;- price - price * COGS\ncontribution_margin_per_unit\n\n[1] 240\n\n# equivalently, contribution margin rate = 1 - COGS\n# contribution margin per unit = price * contribution margin rate  \ncontribution_margin_per_unit &lt;- price * (1 - COGS) \ncontribution_margin_per_unit\n\n[1] 240\n\n\n\n\n\n\nCompute the break-even quantity\n\n\n# numerator is the marketing expense\n# denominator is the the contribution margin per unit\n\nBEQ &lt;- endorsement_fee / contribution_margin_per_unit\nBEQ\n\n[1] 0.2083333\n\n\n\nThe marketing costs, i.e., the endorsement fee, is 50 million pounds\nEach incremental sale makes profit by 240 pounds\n=&gt; This means, the influencer marketing campaign needs to increase sales by at least BEQ (0.2083333 million) units, in order for the company not to lose any money\n\n\n\n\n\nCompare BEQ with estimated incremental sales to finish break-even analyses\nIn the case study, “the team estimates that such an influencer campaign can increase the total sales within the next financial year by 2.5%.”\n\nThe comparison base is the original estimated sales without any marketing campaign, so the incremental units of sales would be quantity * 0.025\n\n\n\nquantity * 0.025 \n\n[1] 0.25\n\n\n\nWe need to sell 0.2083333 million units to break-even (not earn or lose money), but we can in fact sell 0.25 million, which is more than the BEQ.\nThe influencer marketing campaign is profitable and should be approved.\n\n\n\n\n\nIf the benefits of the marketing campaign come in longer periods, we need to consider the time value of money and use NPV to evaluate the profitability\n\n\\[\nN P V=-I_{0}+\\frac{CF_{1}}{(1+k)}+\\frac{C F_{2}}{(1+k)^{2}}+\\cdots+\\frac{C F_{n}}{(1+k)^{n}}\n\\]\n\n\n\n\n\\(k\\) is called discount rate, which reflects the time value of money\n\nThe same £1 today is more valuable than £1 tomorrow\ne.g., if interest rate is 10% annually, then £1 today is worth £1.1 a year later\n\n\\(\\frac{1}{1+k}\\) is called discount factor, which is a factor to discount the future CFs to today\n\nIn each period, we discount the future CF by multiplying it with the discount factor\nCF received 1 month later \\(CF_1\\) is worth \\(\\frac{1}{1+k} * CF_1\\) today\nCF received 2 months later \\(CF_2\\) is worth \\(\\frac{1}{(1+k)^2} * CF_2\\) today\n\nFor a company, \\(k\\) is often estimated by the finance department, which is usually the Weighted Average Cost of Capital, or WACC\n\n\n\n\n\nCompute the sequence of monthly cash flows\n\n\nFirst, we compute the incremental sales percentage for each month, relative to the 10 million.\nThis is a 12-element vector, each element representing the incremental sales percentage.\n\n\nincremental.sales.percentage_1stmonth &lt;- 0.003\nincremental.sales.percentage_next11months &lt;- rep(0.002,11)\n\n# incremental profit each month\nvector_incremental.sales.percentage_12months &lt;- \n  c(incremental.sales.percentage_1stmonth,\n    incremental.sales.percentage_next11months)\n\nvector_incremental.sales.percentage_12months\n\n [1] 0.003 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n\n\n\nInterpretation: 0.003 means that, the first month incremental sales units would be 0.3% of the baseline quantity.\n\n\n\n\n\nNext, we multiply the incremental sales percentage with quantity, to get the incremental sales in terms of units for each month.\n\n\nvector_incremental.sales.units_12months &lt;- \n  vector_incremental.sales.percentage_12months * \n  quantity\n\nvector_incremental.sales.units_12months\n\n [1] 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n\n\n\nInterpretation: 0.03 means that, the first month incremental sales units would be 0.03 million units.\n\n\n\n\n\nLastly, we multiply the incremental quantity sales with the contribution margin per unit, to get the total contribution margins (incremental profits) for each month, i.e., the CF\n\n\nvector_CF &lt;- vector_incremental.sales.units_12months * \n  contribution_margin_per_unit\n\nvector_CF\n\n [1] 7.2 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8\n\n\n\nInterpretation: 7.2 means that, the first month incremental net profits would be 7.2 million pounds.\n\n\n\n\n\nCompute the sequence of discount factors\n\n\n# divide annual wacc to get monthly wacc\nmonthly_WACC &lt;- 0.1/12 \n# monthly wacc is the k in the NPV formula\nk &lt;- monthly_WACC\nk\n\n[1] 0.008333333\n\n# discount factor is 1/(1+k)\ndiscount_factor &lt;- 1/ (1+k)\ndiscount_factor\n\n[1] 0.9917355\n\n# Generate a geometric sequence vector of discounted CFs for 12 months\nvector_discount_factor &lt;- discount_factor^c(1:12)\nvector_discount_factor\n\n [1] 0.9917355 0.9835394 0.9754110 0.9673497 0.9593551 0.9514265 0.9435635\n [8] 0.9357654 0.9280319 0.9203622 0.9127559 0.9052124\n\n\n\nInterpretation: 0.9917355 means that, £1 1 month later is worth £0.992 today; 0.9052124 means that, £1 12 month later is worth £0.905 today.\n\n\n\n\n\nCompute the NPV\n\n\nMultiply CF vector with discount factor vector, to get the discounted CF vector\n\n\n# this will do element-by-element multiplication\nvector_discounted.CF &lt;- vector_CF * vector_discount_factor \nvector_discounted.CF\n\n [1] 7.140496 4.720989 4.681973 4.643279 4.604904 4.566847 4.529105 4.491674\n [9] 4.454553 4.417738 4.381228 4.345020\n\n\n\n\n\n\nuse function sum() to get the sum of all elements in a vector. That is, the sum of discounted cash flows in all 12 months.\n\n\nsum(vector_CF * vector_discount_factor)\n\n[1] 56.97781\n\n\n\nWe need to subtract the endorsement fee, which is the marketing expense, to get the net present value\n\n\nNPV &lt;- sum(vector_CF * vector_discount_factor) - endorsement_fee\nNPV\n\n[1] 6.977806\n\n\n\n\n\n\nCLV is a break-even analysis from the perspective of a single customer, which considers a customer as an asset to the company that generates future cashflows\n\nincurs customer acquisition costs (CAC)\ncustomer generates profits for the company in each period\ncustomer churns at some point in time\n\n\n\n\n\n\n\n\n\n\n\nThe total marketing costs to acquire a new customer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\mathrm{CLV} = - CAC + \\sum_{t=1}^{N} \\frac{CF_t * r^{(t-1)}}{(1+k)^{t}}\n\\]\nwhere \\(CF_t = M_t - c_t\\)\n\n\\(r\\) is the average annual retention rate; \\(r^{(t-1)}\\) is the cumulative retention rate in year \\(t\\)\n\\(N\\) is the number of years over which the relationship is calculated\n\\(M_{t}\\) is the margin the customer generates in year \\(t\\)\n\\(c_{t}\\) is the expected cost of marketing communications or promotions targeted to the customer in year \\(t\\)\n\\(k\\) is the rate for discounting future cash flows"
  },
  {
    "objectID": "Week2-Lecture1.html#objective-of-marketing-process",
    "href": "Week2-Lecture1.html#objective-of-marketing-process",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Marketing activities need to create value for the company"
  },
  {
    "objectID": "Week2-Lecture1.html#break-even-analyses",
    "href": "Week2-Lecture1.html#break-even-analyses",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Any marketing activity (in fact, any business activity)\n\nincurs some marketing expense/investment costs\ngenerates benefits for the company (e.g., incremental sales; higher customer retention rate)\n\nThe core idea of a break-even analysis is to compare the benefit with the cost\n\nBEA is sometimes called cost-benefit analysis."
  },
  {
    "objectID": "Week2-Lecture1.html#break-even-quantity",
    "href": "Week2-Lecture1.html#break-even-quantity",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "For a marketing campaign with fixed marketing expenditure with short-term impacts, we can compute BEQ to evaluate its feasibility\nBEQ calculates the number of incremental units the firm needs to sell to cover the cost of the marketing campaign.\n\nIncremental because we are comparing with status quo"
  },
  {
    "objectID": "Week2-Lecture1.html#break-even-quantity-formula",
    "href": "Week2-Lecture1.html#break-even-quantity-formula",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Contribution Margin Per Unit = Price Per Unit - Variable Costs Per Unit\n\nMeasures how much money each additional sale “contributes” to the company’s total profits.\ncontribution margin rate1 = contribution margin per unit / price per unit\n\n\n\n\nBreak-Even Quantity = Marketing Expenditure / Contribution Margin Per Unit\nCompare BEQ with estimated incremental sales to finish break-even analyses\n\nMarketing costs are usually easy to obtain through budgeting\nIncremental sales will need to be estimated through causal inference tools (i.e., the causal impact of influencer marketing on sales)\n\nIf the estimated incremental sales can exceed BEQ, approve the marketing campaign"
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-beq",
    "href": "Week2-Lecture1.html#pineapple-beq",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Assign values to R objects based on case background information\n\n\nprice &lt;- 600 # retail price\nquantity &lt;- 10 # sales; it's 10 million, bear this unit in mind\nCOGS &lt;- 0.6 # cost of goods sold in percentage terms\nRD_costs &lt;- 100 # R&D Costs\nendorsement_fee &lt;- 50 # fixed marketing expenditure\n\n\nquantity is 10 million; we use 10 for brevity\n\nSales refers to quantity sales by industry practice\nRevenue or revenue sales refers to monetary sales\n\nCOGS is the variable costs per unit in the BEQ formula\n\nUsed in both percentage or value terms interchangeably.\n\nR&D costs are sunk costs\n\nShould sunk costs be considered in a BEA for a marketing campaign?"
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-beq-step-1",
    "href": "Week2-Lecture1.html#pineapple-beq-step-1",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Compute the contribution margin per unit\n\n\n# Following the definition\n# contribution margin per unit = price - variable cost\ncontribution_margin_per_unit &lt;- price - price * COGS\ncontribution_margin_per_unit\n\n[1] 240\n\n# equivalently, contribution margin rate = 1 - COGS\n# contribution margin per unit = price * contribution margin rate  \ncontribution_margin_per_unit &lt;- price * (1 - COGS) \ncontribution_margin_per_unit\n\n[1] 240"
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-beq-step-2",
    "href": "Week2-Lecture1.html#pineapple-beq-step-2",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Compute the break-even quantity\n\n\n# numerator is the marketing expense\n# denominator is the the contribution margin per unit\n\nBEQ &lt;- endorsement_fee / contribution_margin_per_unit\nBEQ\n\n[1] 0.2083333\n\n\n\nThe marketing costs, i.e., the endorsement fee, is 50 million pounds\nEach incremental sale makes profit by 240 pounds\n=&gt; This means, the influencer marketing campaign needs to increase sales by at least BEQ (0.2083333 million) units, in order for the company not to lose any money"
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-beq-step-3",
    "href": "Week2-Lecture1.html#pineapple-beq-step-3",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Compare BEQ with estimated incremental sales to finish break-even analyses\nIn the case study, “the team estimates that such an influencer campaign can increase the total sales within the next financial year by 2.5%.”\n\nThe comparison base is the original estimated sales without any marketing campaign, so the incremental units of sales would be quantity * 0.025\n\n\n\nquantity * 0.025 \n\n[1] 0.25\n\n\n\nWe need to sell 0.2083333 million units to break-even (not earn or lose money), but we can in fact sell 0.25 million, which is more than the BEQ.\nThe influencer marketing campaign is profitable and should be approved."
  },
  {
    "objectID": "Week2-Lecture1.html#npv",
    "href": "Week2-Lecture1.html#npv",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "If the benefits of the marketing campaign come in longer periods, we need to consider the time value of money and use NPV to evaluate the profitability\n\n\\[\nN P V=-I_{0}+\\frac{CF_{1}}{(1+k)}+\\frac{C F_{2}}{(1+k)^{2}}+\\cdots+\\frac{C F_{n}}{(1+k)^{n}}\n\\]"
  },
  {
    "objectID": "Week2-Lecture1.html#discount-rate-and-discount-factor",
    "href": "Week2-Lecture1.html#discount-rate-and-discount-factor",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "\\(k\\) is called discount rate, which reflects the time value of money\n\nThe same £1 today is more valuable than £1 tomorrow\ne.g., if interest rate is 10% annually, then £1 today is worth £1.1 a year later\n\n\\(\\frac{1}{1+k}\\) is called discount factor, which is a factor to discount the future CFs to today\n\nIn each period, we discount the future CF by multiplying it with the discount factor\nCF received 1 month later \\(CF_1\\) is worth \\(\\frac{1}{1+k} * CF_1\\) today\nCF received 2 months later \\(CF_2\\) is worth \\(\\frac{1}{(1+k)^2} * CF_2\\) today\n\nFor a company, \\(k\\) is often estimated by the finance department, which is usually the Weighted Average Cost of Capital, or WACC"
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-npv-step-1",
    "href": "Week2-Lecture1.html#pineapple-npv-step-1",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Compute the sequence of monthly cash flows\n\n\nFirst, we compute the incremental sales percentage for each month, relative to the 10 million.\nThis is a 12-element vector, each element representing the incremental sales percentage.\n\n\nincremental.sales.percentage_1stmonth &lt;- 0.003\nincremental.sales.percentage_next11months &lt;- rep(0.002,11)\n\n# incremental profit each month\nvector_incremental.sales.percentage_12months &lt;- \n  c(incremental.sales.percentage_1stmonth,\n    incremental.sales.percentage_next11months)\n\nvector_incremental.sales.percentage_12months\n\n [1] 0.003 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n\n\n\nInterpretation: 0.003 means that, the first month incremental sales units would be 0.3% of the baseline quantity."
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-npv-step-1-1",
    "href": "Week2-Lecture1.html#pineapple-npv-step-1-1",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Next, we multiply the incremental sales percentage with quantity, to get the incremental sales in terms of units for each month.\n\n\nvector_incremental.sales.units_12months &lt;- \n  vector_incremental.sales.percentage_12months * \n  quantity\n\nvector_incremental.sales.units_12months\n\n [1] 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n\n\n\nInterpretation: 0.03 means that, the first month incremental sales units would be 0.03 million units."
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-npv-step-1-2",
    "href": "Week2-Lecture1.html#pineapple-npv-step-1-2",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Lastly, we multiply the incremental quantity sales with the contribution margin per unit, to get the total contribution margins (incremental profits) for each month, i.e., the CF\n\n\nvector_CF &lt;- vector_incremental.sales.units_12months * \n  contribution_margin_per_unit\n\nvector_CF\n\n [1] 7.2 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8 4.8\n\n\n\nInterpretation: 7.2 means that, the first month incremental net profits would be 7.2 million pounds."
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-npv-step-2",
    "href": "Week2-Lecture1.html#pineapple-npv-step-2",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Compute the sequence of discount factors\n\n\n# divide annual wacc to get monthly wacc\nmonthly_WACC &lt;- 0.1/12 \n# monthly wacc is the k in the NPV formula\nk &lt;- monthly_WACC\nk\n\n[1] 0.008333333\n\n# discount factor is 1/(1+k)\ndiscount_factor &lt;- 1/ (1+k)\ndiscount_factor\n\n[1] 0.9917355\n\n# Generate a geometric sequence vector of discounted CFs for 12 months\nvector_discount_factor &lt;- discount_factor^c(1:12)\nvector_discount_factor\n\n [1] 0.9917355 0.9835394 0.9754110 0.9673497 0.9593551 0.9514265 0.9435635\n [8] 0.9357654 0.9280319 0.9203622 0.9127559 0.9052124\n\n\n\nInterpretation: 0.9917355 means that, £1 1 month later is worth £0.992 today; 0.9052124 means that, £1 12 month later is worth £0.905 today."
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-npv-step-3",
    "href": "Week2-Lecture1.html#pineapple-npv-step-3",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "Compute the NPV\n\n\nMultiply CF vector with discount factor vector, to get the discounted CF vector\n\n\n# this will do element-by-element multiplication\nvector_discounted.CF &lt;- vector_CF * vector_discount_factor \nvector_discounted.CF\n\n [1] 7.140496 4.720989 4.681973 4.643279 4.604904 4.566847 4.529105 4.491674\n [9] 4.454553 4.417738 4.381228 4.345020"
  },
  {
    "objectID": "Week2-Lecture1.html#pineapple-npv-step-3-1",
    "href": "Week2-Lecture1.html#pineapple-npv-step-3-1",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "use function sum() to get the sum of all elements in a vector. That is, the sum of discounted cash flows in all 12 months.\n\n\nsum(vector_CF * vector_discount_factor)\n\n[1] 56.97781\n\n\n\nWe need to subtract the endorsement fee, which is the marketing expense, to get the net present value\n\n\nNPV &lt;- sum(vector_CF * vector_discount_factor) - endorsement_fee\nNPV\n\n[1] 6.977806"
  },
  {
    "objectID": "Week2-Lecture1.html#customer-lifetime-value",
    "href": "Week2-Lecture1.html#customer-lifetime-value",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "CLV is a break-even analysis from the perspective of a single customer, which considers a customer as an asset to the company that generates future cashflows\n\nincurs customer acquisition costs (CAC)\ncustomer generates profits for the company in each period\ncustomer churns at some point in time"
  },
  {
    "objectID": "Week2-Lecture1.html#customer-acquisition-costs",
    "href": "Week2-Lecture1.html#customer-acquisition-costs",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "The total marketing costs to acquire a new customer"
  },
  {
    "objectID": "Week2-Lecture1.html#clv-formula",
    "href": "Week2-Lecture1.html#clv-formula",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "",
    "text": "\\[\n\\mathrm{CLV} = - CAC + \\sum_{t=1}^{N} \\frac{CF_t * r^{(t-1)}}{(1+k)^{t}}\n\\]\nwhere \\(CF_t = M_t - c_t\\)\n\n\\(r\\) is the average annual retention rate; \\(r^{(t-1)}\\) is the cumulative retention rate in year \\(t\\)\n\\(N\\) is the number of years over which the relationship is calculated\n\\(M_{t}\\) is the margin the customer generates in year \\(t\\)\n\\(c_{t}\\) is the expected cost of marketing communications or promotions targeted to the customer in year \\(t\\)\n\\(k\\) is the rate for discounting future cash flows"
  },
  {
    "objectID": "Week2-Lecture1.html#situation-analyses-i-basket",
    "href": "Week2-Lecture1.html#situation-analyses-i-basket",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.1 Situation Analyses: i-basket",
    "text": "2.1 Situation Analyses: i-basket\n\nCompany\nCustomer\nCollaborators\nCompetitors\nContext/Climate"
  },
  {
    "objectID": "Week2-Lecture1.html#step-1-determine-time-unit-of-analysis",
    "href": "Week2-Lecture1.html#step-1-determine-time-unit-of-analysis",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.2 Step 1: Determine time unit of analysis",
    "text": "2.2 Step 1: Determine time unit of analysis\n\nTime unit of analysis\n\n[…] (find info in the case study)\n\nWhen should we use monthly analysis?"
  },
  {
    "objectID": "Week2-Lecture1.html#step-2-determine-number-of-years",
    "href": "Week2-Lecture1.html#step-2-determine-number-of-years",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.3 Step 2: Determine number of years",
    "text": "2.3 Step 2: Determine number of years\n\n\\(N\\): the number of years over which the customer relationship is assessed\n\n[…] (find info in the case study)\n\n\n\nN &lt;-"
  },
  {
    "objectID": "Week2-Lecture1.html#step-3-compute-profit-margin-for-each-period",
    "href": "Week2-Lecture1.html#step-3-compute-profit-margin-for-each-period",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.4 Step 3: Compute profit margin for each period",
    "text": "2.4 Step 3: Compute profit margin for each period\n\\(CF = M - c\\): gross profit each year\n\nmost customers paid the $99 annual membership fee\n\n\nmembership &lt;- \n\n\n40 times each year; each time $100\n\n\nn_visit &lt;- \nrevenue_each_visit &lt;-"
  },
  {
    "objectID": "Week2-Lecture1.html#step-3-compute-profit-margin-for-each-period-1",
    "href": "Week2-Lecture1.html#step-3-compute-profit-margin-for-each-period-1",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.5 Step 3: Compute profit margin for each period",
    "text": "2.5 Step 3: Compute profit margin for each period\n\nprofit margin 7% (COGS 93%)\n\n\nprofit_margin &lt;- 0.07\n## think carefully about how M is calculated, it's tricky ~~~~\nM &lt;- \n\n\nvariable delivery costs each order\n\n\ndeliverycost_each_visit &lt;- 5 + 100 * 0.035\nc &lt;- deliverycost_each_visit * n_visit"
  },
  {
    "objectID": "Week2-Lecture1.html#step-3-compute-profit-margin-for-each-period-2",
    "href": "Week2-Lecture1.html#step-3-compute-profit-margin-for-each-period-2",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.6 Step 3: Compute profit margin for each period",
    "text": "2.6 Step 3: Compute profit margin for each period\n\nthe annual CF from customers CF\n\n\n# CF is the cash flow for one year\nCF &lt;- \n\n# create a sequence of CF for N years \nprofit_seq &lt;- rep(CF,N)"
  },
  {
    "objectID": "Week2-Lecture1.html#step-4-compute-sequence-of-retention-rate",
    "href": "Week2-Lecture1.html#step-4-compute-sequence-of-retention-rate",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.7 Step 4: Compute sequence of retention rate",
    "text": "2.7 Step 4: Compute sequence of retention rate\n\n\\(r\\): retention rate\n\n\n[…] (find info in the case study)\n\n\n# retention_rate is the probability of customer staying with us after 1 year\nretention_rate &lt;- \n  \n# create a geometric sequence of accumulative retention rate for N years  \nretention_seq &lt;-"
  },
  {
    "objectID": "Week2-Lecture1.html#step-5-compute-sequence-of-discount-factors",
    "href": "Week2-Lecture1.html#step-5-compute-sequence-of-discount-factors",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.8 Step 5: Compute sequence of discount factors",
    "text": "2.8 Step 5: Compute sequence of discount factors\n\n\\(k\\): the discount rate\n\n\n[…] A yearly discount rate of 10%\n\n\ndiscount_rate &lt;- 0.1\ndiscount_factor_seq &lt;- \n\n\n[…] The team decided to take a conservative approach whereby all profits are booked at the end of year.\n\nAll profits earned per customer in year 1 need to be discounted once, the profits earned in year 2 need to be discounted twice, and so on"
  },
  {
    "objectID": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs",
    "href": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.9 Step 6: Compute customer acquisition costs",
    "text": "2.9 Step 6: Compute customer acquisition costs\n\nCAC = total costs for customer ad clicks + total costs of $15 promo + total costs of free deliveries\n\n\nHow the Marketing Funnel Works From Top to Bottom"
  },
  {
    "objectID": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-1",
    "href": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-1",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.10 Step 6: Compute customer acquisition costs",
    "text": "2.10 Step 6: Compute customer acquisition costs\n6.1 Total costs for customer clicks\n\n[…] a fifth of those who clicked on an ad were willing to give the service a try\n[…] 20% of those that signed up for the free trial ended up becoming members\n\n\n# click_to_trier_rate is the % of trier customers from clickers\nclick_to_trier_rate &lt;- 0.2\n\n# trier_to_buyer_rate is the % of final customer from trier customers\ntrier_to_buyer_rate &lt;- 0.2\n\n\nHow many customers need to click the ad to get 1 new customer?\n\n\nn_clicks_1newcustomer &lt;- 1/click_to_trier_rate/trier_to_buyer_rate\n\n\nTotal costs for customer clicks\n\n\ntotal_cost_clicks &lt;- 0.4 * n_clicks_1newcustomer"
  },
  {
    "objectID": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-2",
    "href": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-2",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.11 Step 6: Compute customer acquisition costs",
    "text": "2.11 Step 6: Compute customer acquisition costs\n\nCAC = total costs for customer ad clicks + total costs of $15 promo + total costs of free deliveries\n\n6.2 total costs of $15 promo for first order each trier customer\n\nHow many customers need to try the service to get 1 new customer?\n\n\nn_triers &lt;- 1/conversion_rate\n\n\nWhat is the total promo cost for these “trier” customers’ first order?\n\n\npromo_first_order_each_trier &lt;- 15\n\ntotal_cost_promo &lt;- promo_first_order_each_trier * (1 - profit_margin) * n_triers\ntotal_cost_promo"
  },
  {
    "objectID": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-3",
    "href": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-3",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.12 Step 6: Compute customer acquisition costs",
    "text": "2.12 Step 6: Compute customer acquisition costs\n\nCAC = total costs for customer ad clicks + total costs of $15 promo + total costs of free deliveries\n\n6.3 total costs from free deliveries\n\nAssume two visits, the delivery costs for each visit\n\n\ndeliverycost_1st &lt;- 5 + 115 * 0.035\ndeliverycost_2nd &lt;- 5 + 100 * 0.035\ndeliverycost_each_trier &lt;- deliverycost_1st + deliverycost_2nd\n\n\nWe also make a profit from each trier\n\n\nprofit_each_trier &lt;-  revenue_each_visit * profit_margin * 2\n\n\nNet delivery costs for each trier\n\n\nNetDeliveryCost_each_trier&lt;- deliverycost_each_trier - profit_each_trier\ntotal_cost_delivery &lt;- NetDeliveryCost_each_trier * n_triers"
  },
  {
    "objectID": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-4",
    "href": "Week2-Lecture1.html#step-6-compute-customer-acquisition-costs-4",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.13 Step 6: Compute customer acquisition costs",
    "text": "2.13 Step 6: Compute customer acquisition costs\n\nCAC = total costs for customer ad clicks + total costs of $15 promo + total costs of free deliveries\n\n\nCAC &lt;- total_cost_clicks + total_cost_promo + total_cost_delivery\nCAC"
  },
  {
    "objectID": "Week2-Lecture1.html#step-7-compute-clv",
    "href": "Week2-Lecture1.html#step-7-compute-clv",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "2.14 Step 7: Compute CLV",
    "text": "2.14 Step 7: Compute CLV\n\nCompute the CLV based on the CLV formula (Table A)\n\n\n\n7.1 Revenues, variables costs, and profit for the next 5 years\n\n\nprofit_seq\n\n\n7.2 Apply retention rate\n\n\nprofit_seq_after_churn &lt;- profit_seq * retention_seq\n\n\n7.3 Apply discount factor\n\n\nprofit_seq_after_churn_discount&lt;- profit_seq_after_churn * discount_factor_seq\n\n\n7.4 Compute CLV by summing up future expected profits\n\n\nCLV &lt;- sum(profit_seq_after_churn_discount) - CAC"
  },
  {
    "objectID": "Week2-Lecture1.html#clv-as-a-key-management-tool",
    "href": "Week2-Lecture1.html#clv-as-a-key-management-tool",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "3.1 CLV as a Key Management Tool",
    "text": "3.1 CLV as a Key Management Tool\n\n\n\n\n\n\n\n\n\nWe can use CLV as the key managerial tool for evaluating different marketing initiatives!"
  },
  {
    "objectID": "Week2-Lecture1.html#discussion",
    "href": "Week2-Lecture1.html#discussion",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "3.2 Discussion",
    "text": "3.2 Discussion\n\nHow important is it for i-basket to measure CLV? Can you think of other companies or industries where CLV is particularly relevant?\nConduct sensitivity analyses\n\nwhat assumptions have we made here? Are these assumptions sensitive to different values?\n\nFrom our analyses, what suggestions would you offer to i-basket in order to improve its customer profitability? How are you going to evaluate the feasibility of your proposal?\n\nacquisition/development/retention"
  },
  {
    "objectID": "Week2-Lecture1.html#exercise",
    "href": "Week2-Lecture1.html#exercise",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "3.3 Exercise",
    "text": "3.3 Exercise\n\nHow much annual membership fee should the company charge to break even?\nThe company is looking to develop a personalized recommendation system that can increase the average shopping basket to $150. Compute the upper bound for the company’s investment in developing the algorithm in order to break even? Assume the company has 10,000 customers at this moment."
  },
  {
    "objectID": "Week2-Lecture1.html#footnotes",
    "href": "Week2-Lecture1.html#footnotes",
    "title": "Class 3 Workshop: Break-Even Analyses and Customer Lifetime Value",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s important to infer percentage/absolute terms from the context.↩︎"
  },
  {
    "objectID": "Week4-Lecture2.html",
    "href": "Week4-Lecture2.html",
    "title": "Class 8: Workshop: Improving Marketing Efficiency Using Predictive Analytics",
    "section": "",
    "text": "Tesco is looking to promote its new private-label products to existing customers. The marketing analytics team decides to use the conventional mailing marketing strategy so that customer would receive color-printed leaflets via Royal Mails to their doorsteps.\n\n\n\n\nEach mail costs £1.5 to produce and another £0.5 to mail to the customers. If customer responds to the offer, the management expects customers to spend £20 on trying the new products, where the COGS is 60%.\nThe cost is the marketing offer we send, cost_per_offer\n\n\n# cost of sending an offer\ncost_per_offer &lt;- 1.5 + 0.5\ncost_per_offer\n\n[1] 2\n\n\n\nThe benefit is the profit margin if a customer responds, profit_per_customer\n\n\n# profit from a responding customer\nCOGS &lt;- 0.6\nprofit_per_customer &lt;- 20 * (1 - COGS)\nprofit_per_customer\n\n[1] 8\n\n\n\n\n\n\nIn order to break-even, we can calculate the break-even response rate from customers:\n\n\nbreak_even_response &lt;- cost_per_offer/profit_per_customer\nbreak_even_response\n\n[1] 0.25\n\n\n\nOnly if a customer responds to us with at least 25% response rate can we recover the costs of making an marketing offer.\nIf we send offers to customers whose expected response rate is lower than 25%, we make a loss by expectation.\n\n\n\n\n\nPrepare data for ML model (data wrangling)\n\na training set and a test set\n\nTrain predictive models on the training set (decision tree and random forest)\nPredict customer response rate on the test set\nTarget customers based on predicted response rate\nCompute ROI for each scenario\n\nBlanket marketing\nDecision tree\nRandom forest\n\n\nLet’s work on the remaining case study questions together!"
  },
  {
    "objectID": "Week4-Lecture2.html#background",
    "href": "Week4-Lecture2.html#background",
    "title": "Class 8: Workshop: Improving Marketing Efficiency Using Predictive Analytics",
    "section": "",
    "text": "Tesco is looking to promote its new private-label products to existing customers. The marketing analytics team decides to use the conventional mailing marketing strategy so that customer would receive color-printed leaflets via Royal Mails to their doorsteps."
  },
  {
    "objectID": "Week4-Lecture2.html#cost-benefit-analyses",
    "href": "Week4-Lecture2.html#cost-benefit-analyses",
    "title": "Class 8: Workshop: Improving Marketing Efficiency Using Predictive Analytics",
    "section": "",
    "text": "Each mail costs £1.5 to produce and another £0.5 to mail to the customers. If customer responds to the offer, the management expects customers to spend £20 on trying the new products, where the COGS is 60%.\nThe cost is the marketing offer we send, cost_per_offer\n\n\n# cost of sending an offer\ncost_per_offer &lt;- 1.5 + 0.5\ncost_per_offer\n\n[1] 2\n\n\n\nThe benefit is the profit margin if a customer responds, profit_per_customer\n\n\n# profit from a responding customer\nCOGS &lt;- 0.6\nprofit_per_customer &lt;- 20 * (1 - COGS)\nprofit_per_customer\n\n[1] 8"
  },
  {
    "objectID": "Week4-Lecture2.html#break-even-response-rate",
    "href": "Week4-Lecture2.html#break-even-response-rate",
    "title": "Class 8: Workshop: Improving Marketing Efficiency Using Predictive Analytics",
    "section": "",
    "text": "In order to break-even, we can calculate the break-even response rate from customers:\n\n\nbreak_even_response &lt;- cost_per_offer/profit_per_customer\nbreak_even_response\n\n[1] 0.25\n\n\n\nOnly if a customer responds to us with at least 25% response rate can we recover the costs of making an marketing offer.\nIf we send offers to customers whose expected response rate is lower than 25%, we make a loss by expectation."
  },
  {
    "objectID": "Week4-Lecture2.html#improving-marketing-efficiency-using-predictive-analytics",
    "href": "Week4-Lecture2.html#improving-marketing-efficiency-using-predictive-analytics",
    "title": "Class 8: Workshop: Improving Marketing Efficiency Using Predictive Analytics",
    "section": "",
    "text": "Prepare data for ML model (data wrangling)\n\na training set and a test set\n\nTrain predictive models on the training set (decision tree and random forest)\nPredict customer response rate on the test set\nTarget customers based on predicted response rate\nCompute ROI for each scenario\n\nBlanket marketing\nDecision tree\nRandom forest\n\n\nLet’s work on the remaining case study questions together!"
  },
  {
    "objectID": "Week4-Lecture2.html#predictive-analytics-and-customer-life-cycle",
    "href": "Week4-Lecture2.html#predictive-analytics-and-customer-life-cycle",
    "title": "Class 8: Workshop: Improving Marketing Efficiency Using Predictive Analytics",
    "section": "2.1 Predictive Analytics and Customer Life Cycle",
    "text": "2.1 Predictive Analytics and Customer Life Cycle\n\nAcquisition (Tesco Case Study)\n\nUse predictive analytics to target responsive customers to reduce marketing costs\n\nDevelopment\n\nUse predictive analytics to recommend products to customers (personalized recommendation system)\n\nRetention (Week 5 Case Study)\n\nUse predictive analytics to find risky customers and conduct churn management"
  },
  {
    "objectID": "Module-introduction.html",
    "href": "Module-introduction.html",
    "title": "Module Introduction",
    "section": "",
    "text": "We will not rely on any specific textbook in this module.\n\n“Handbook of Marketing Analytics”. This book is free for download at UCL’s E-library.\n“Introduction to Econometrics with R”, which is a good textbook for learning econometrics with R.\n\nAll classes will be based on the lecture notes and supplementary readings I have prepared for you."
  },
  {
    "objectID": "Module-introduction.html#textbook",
    "href": "Module-introduction.html#textbook",
    "title": "Module Introduction",
    "section": "",
    "text": "We will not rely on any specific textbook in this module.\n\n“Handbook of Marketing Analytics”. This book is free for download at UCL’s E-library.\n“Introduction to Econometrics with R”, which is a good textbook for learning econometrics with R.\n\nAll classes will be based on the lecture notes and supplementary readings I have prepared for you."
  },
  {
    "objectID": "Module-introduction.html#lecture-notes-and-how-to-prepare",
    "href": "Module-introduction.html#lecture-notes-and-how-to-prepare",
    "title": "Module Introduction",
    "section": "2 Lecture Notes and How to Prepare",
    "text": "2 Lecture Notes and How to Prepare\n\nI’m updating my Lecture notes every year to keep up with the latest development in marketing analytics. So lecture notes will be released each week before the lecture.\n\nPDF version includes the slides you see now\nhtml version additionally includes the solution R codes for in-class questions\n\nFor each week’s class, there may be pre-class preparation needed\n\nMaterials will be posted under each week’s Moodle section with instructions\nThese are mandatory, usually involves reading case studies necessary for class discussion\n\nThere are also supplementary after-class reading/exercise to further enhance your understanding"
  },
  {
    "objectID": "Module-introduction.html#module-communications-and-any-questions",
    "href": "Module-introduction.html#module-communications-and-any-questions",
    "title": "Module Introduction",
    "section": "3 Module Communications and Any Questions",
    "text": "3 Module Communications and Any Questions\n\nWe will use Microsoft Teams for all communications of this module. Please use the code to join the MS Team now.\nMake sure you hit “Change Notifications =&gt; All Activities” for all channels, especially the General Channel where important module announcements will be made.\nFor any questions you have, please post them in the corresponding Teams channel first, so that everyone can see the question.\n\nWe have separate channels for assignments, lectures, and R programming questions.\n\nThe teaching team will monitor the channel and answer them."
  },
  {
    "objectID": "Module-introduction.html#still-have-questions-use-office-hours",
    "href": "Module-introduction.html#still-have-questions-use-office-hours",
    "title": "Module Introduction",
    "section": "4 Still Have Questions? Use Office Hours",
    "text": "4 Still Have Questions? Use Office Hours\n\nOffice hour sessions will be hosted on MS Teams each Tuesday by TAs and Friday by me.\nThe links to book office hours are below (also under Moodle’s “Module Overview” Section for easier access)\n\nFor questions related to lecture contents and assignments, book with Wei\nFor questions regarding R programming (such as R codes used in class, general R troubleshooting), book with our TAs"
  },
  {
    "objectID": "Module-introduction.html#about-r-programming",
    "href": "Module-introduction.html#about-r-programming",
    "title": "Module Introduction",
    "section": "5 About R Programming",
    "text": "5 About R Programming\n\nPlease bring your laptops in all classes. There will be R exercises in class every week.\nIt’s totally normal to feel overwhelmed in the first few weeks from learning R.\nIf you run into any problems with R\n\n“Common R Programming Errors Faced by Beginners”; I will constantly update this page based on your questions, so this should be the first place to look for an answer\nGoogle and ChatGPT are always your best places to seek answers for most debugging issues\nOffice hours with TAs"
  },
  {
    "objectID": "Module-introduction.html#classroom-etiquette",
    "href": "Module-introduction.html#classroom-etiquette",
    "title": "Module Introduction",
    "section": "6 Classroom Etiquette",
    "text": "6 Classroom Etiquette\n\nClass participation\n\nPlease remember to bring you name tags. Please let me know how to pronounce your name correctly.\n\nAttendance\n\nSchool will track your attendance per university policy\nPlease email me if you couldn’t attend a class"
  },
  {
    "objectID": "Module-introduction.html#week-contract-between-us",
    "href": "Module-introduction.html#week-contract-between-us",
    "title": "Module Introduction",
    "section": "7 10-week Contract between Us",
    "text": "7 10-week Contract between Us\n\nI promise\n\nI will teach you the “science” part of marketing including the state-of-the-art marketing analytics tools\nYou will be in a position to intelligently manage and benefit from marketing analytics people (without turning you into a statistician)\nGive you a way of thinking about how to use data analytics tools to solve business problems in the future, not just marketing, not just today\nAccessible when you need help\n\nYou promise\n\nDuly prepared for each class\nAttend classes regularly"
  },
  {
    "objectID": "Week3-Lecture1.html",
    "href": "Week3-Lecture1.html",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "pacman::p_load(dplyr,ggplot2)\n\n\nPlease install pacman on your RStudio\npacman’s functionality\n\nLoad all packages stated in the parantheses, seperated by commas\nIf the package is not downloaded yet, download it, and then load it\n\nR tip: if you want to use a function without loading the whole package, you can use two colons to call the function: package::function\n\n\n\n\n\nBest practice is to not save any objects once you close your RStudio session\n\n\n\n\n\n\n\nrm(list = ls()) is the command to remove everything in the current environment\n\nls() is a function that returns the list of all objects in the current environment\nrm(list = ) removes any objects passed to list argument\n\n\n\n\n\n\nfilter(dataset, criteria): pick observations by their values\n\n\n\n\n\n\n\narrange(dataset,variable): reorder the rows\nmutate(dataset, newvariable = ): create new variables with functions of existing variables\n\n\n\n\n\n\n\n\n\n\nImagine a factory with different machines placed along a belt. Each machine is a dplyr function that performs a data cleaning step, like filtering or arranging data.\n\n\n\nThe pipe therefore works like a conveyor belt, passing the output of one machine to another for further processing.\n\n\n\n\n\n\n\n\n\n\nThe pipe has a huge advantage over any other method of processing data in R or Python: It makes data wrangling processes easy to read. If we read %&gt;% as “then”, the code will be very easy to interpret as a set of instructions in plain English:\n\n\n\n\nmtcars %&gt;%\n  filter(cyl&gt;=5) %&gt;%\n  mutate(sqrt_wt=sqrt(wt)) %&gt;%\n  filter(sqrt_wt&gt;1.5) %&gt;%\n  arrange(hp)\n## can go on and on\n\n\n\ntake the mtcars data, THEN\nfind cars cyl &gt;= 5, THEN\nmutate a new variable sqrt_wt, THEN\nfind cars sqrt_wt &gt; 1.5 THEN\nreorder all cars based on hp\nchain more operations …\n\n\n\n\n\n\n\nAs a comparison, without using pipe operators, the previous data cleaning steps need to be done as follows. Overwriting our output dataframe new_data in every line is problematic.\n\nFirst, doing this for a procedure with lots of steps isn’t efficient and creates unnecessary repetition in the code.\nSecond, this repetition also makes it harder to identify exactly what is changing on each line in some cases.\n\n\n\nnew_data &lt;- filter(mtcars, cyl &gt;=5)\nnew_data &lt;- mutate(new_data, sqrt_wt=sqrt(wt))\nnew_data &lt;- filter(new_data,sqrt_wt&gt;1.5)\nnew_data &lt;- arrange(new_data,hp)\n\n\n\n\n\nselect() can select variables into a smaller dataset.\n\n\n# Select two columns: hp and cyl\nmtcars%&gt;%\n  select(hp, cyl) %&gt;%\n  head()\n\n\n\n\n\n\nhp\ncyl\n\n\n\n\nMazda RX4\n110\n6\n\n\nMazda RX4 Wag\n110\n6\n\n\nDatsun 710\n93\n4\n\n\nHornet 4 Drive\n110\n6\n\n\nHornet Sportabout\n175\n8\n\n\nValiant\n105\n6\n\n\n\n\n\n\n\n\n\n\ngroup_by() allows us to aggregate data by group and compute statistics for each group\n\n\n# group by cyl\nmtcars %&gt;%\n  group_by(cyl) \n\n\nAlthough nothing seemingly happens to the dataset, internally, the dataset is already grouped based on the specified variable(s).\n\n\n\n\n\n\n\n\n\n\nsummarise() creates a new data frame after aggregating data. The final dataset\n\nhas one row for each pair of grouping variables (for each cyl value)\ncontains one column for each grouping variable (cyl)\ncontains one column for each new summarised variable (avg_mp)\n\n\n\n# compute the average mpg for each cyl group\nmtcars %&gt;%\n  group_by(cyl) %&gt;% # group by cyl\n  summarise(avg_mp = mean(mpg)) %&gt;% # compute the average mpg\n  ungroup()\n\n\n\n\n\ncyl\navg_mp\n\n\n\n\n4\n26.66364\n\n\n6\n19.74286\n\n\n8\n15.10000\n\n\n\n\n\n\n\n\n\n\nWe can have multiple group variables for group_by\n\n\n# compute the average mpg for each cyl,vs group\nmtcars %&gt;%\n  group_by(cyl,vs) %&gt;% # group by cyl\n  summarise(avg_mp = mean(mpg)) %&gt;% # compute the average mpg\n  ungroup()\n\n\n\n\n\nTry the following code by replacing summarise() with mutate(), what do you get now?\n\n\n# compute the average mpg for each cyl,vs group\nmtcars %&gt;%\n  group_by(cyl,vs) %&gt;% # group by cyl\n  mutate(avg_mp = mean(mpg)) %&gt;% # compute the average mpg\n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\navg_mp\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n20.56667\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n20.56667\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n26.73000\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n19.12500\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n15.10000\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n19.12500\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n15.10000\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n26.73000\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n26.73000\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n19.12500\n\n\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n19.12500\n\n\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n15.10000\n\n\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n15.10000\n\n\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n15.10000\n\n\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n15.10000\n\n\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n15.10000\n\n\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n15.10000\n\n\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n26.73000\n\n\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n26.73000\n\n\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n26.73000\n\n\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n26.73000\n\n\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n15.10000\n\n\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n15.10000\n\n\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n15.10000\n\n\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n15.10000\n\n\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n26.73000\n\n\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n26.00000\n\n\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n26.73000\n\n\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n15.10000\n\n\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n20.56667\n\n\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n15.10000\n\n\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n26.73000\n\n\n\n\n\n\n\nA new column is added to the original dataset, the value of which is from the group-by aggregation.\n\n\n\n\n\nWhen consolidating multiple data frames, we usually have 4 types of joining methods\n\n\n\n\n\n\n\n\n\n\nleft_join keeps everything from the left data frame and matches as much as it can from the right data frame.\n\nAll IDs in the left data frame will be retained\nIf a match can be found, value from the right data frame will be filled in\nIf a match cannot be found, a missing value will be filled in\n\n\n\n# Method 1 without pipe operator\nleft_join(df_left, df_right, by = 'ID')\n# Method 2 with pipe operator\ndf_left %&gt;%\n  left_join(df_right, by = 'ID')\n\n\n\n\n\n\n\n\n\n\ninner_join only keeps the observations that appear in both data frames\n\nOnly common IDs in both data frames will be retained\nIf a match can be found, values will be filled in from both data frames\n\n\n\n# Method 1 without pipe operator\ninner_join(df_left, df_right, by = 'ID')\n# Method 2 with pipe operator\ndf_left %&gt;%\n  inner_join(df_right, by = 'ID')\n# Method 3: order of data frames should not matter. Why?\ndf_right %&gt;%\n  inner_join(df_left, by = 'ID')\n\n\n\n\n\n\n\n\n\n\nfull_join keeps all observations from both data frames\n\nAll IDs in either data frames will be retained\nIf a match can be found, values will be filled in from both data frames\n\n\n\n# Method 1 without pipe operator\nfull_join(df_left, df_right, by = 'ID')\n# Method 2 with pipe operator\ndf_left %&gt;%\n  full_join(df_right, by = 'ID')\n# Method 3: order of data frames should not matter. Why?\ndf_right %&gt;%\n  full_join(df_left, by = 'ID')"
  },
  {
    "objectID": "Week3-Lecture1.html#r-tips-more-convenient-package-management-using-pacman",
    "href": "Week3-Lecture1.html#r-tips-more-convenient-package-management-using-pacman",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "pacman::p_load(dplyr,ggplot2)\n\n\nPlease install pacman on your RStudio\npacman’s functionality\n\nLoad all packages stated in the parantheses, seperated by commas\nIf the package is not downloaded yet, download it, and then load it\n\nR tip: if you want to use a function without loading the whole package, you can use two colons to call the function: package::function"
  },
  {
    "objectID": "Week3-Lecture1.html#r-tips-managing-objects-in-the-rstudio-environment",
    "href": "Week3-Lecture1.html#r-tips-managing-objects-in-the-rstudio-environment",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "Best practice is to not save any objects once you close your RStudio session\n\n\n\n\n\n\n\nrm(list = ls()) is the command to remove everything in the current environment\n\nls() is a function that returns the list of all objects in the current environment\nrm(list = ) removes any objects passed to list argument"
  },
  {
    "objectID": "Week3-Lecture1.html#recap-filter-arrange-and-mutate",
    "href": "Week3-Lecture1.html#recap-filter-arrange-and-mutate",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "filter(dataset, criteria): pick observations by their values\n\n\n\n\n\n\n\narrange(dataset,variable): reorder the rows\nmutate(dataset, newvariable = ): create new variables with functions of existing variables"
  },
  {
    "objectID": "Week3-Lecture1.html#pipe-operator",
    "href": "Week3-Lecture1.html#pipe-operator",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "Imagine a factory with different machines placed along a belt. Each machine is a dplyr function that performs a data cleaning step, like filtering or arranging data.\n\n\n\nThe pipe therefore works like a conveyor belt, passing the output of one machine to another for further processing."
  },
  {
    "objectID": "Week3-Lecture1.html#pipe-operator-1",
    "href": "Week3-Lecture1.html#pipe-operator-1",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "The pipe has a huge advantage over any other method of processing data in R or Python: It makes data wrangling processes easy to read. If we read %&gt;% as “then”, the code will be very easy to interpret as a set of instructions in plain English:\n\n\n\n\nmtcars %&gt;%\n  filter(cyl&gt;=5) %&gt;%\n  mutate(sqrt_wt=sqrt(wt)) %&gt;%\n  filter(sqrt_wt&gt;1.5) %&gt;%\n  arrange(hp)\n## can go on and on\n\n\n\ntake the mtcars data, THEN\nfind cars cyl &gt;= 5, THEN\nmutate a new variable sqrt_wt, THEN\nfind cars sqrt_wt &gt; 1.5 THEN\nreorder all cars based on hp\nchain more operations …"
  },
  {
    "objectID": "Week3-Lecture1.html#without-pipe-operator",
    "href": "Week3-Lecture1.html#without-pipe-operator",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "As a comparison, without using pipe operators, the previous data cleaning steps need to be done as follows. Overwriting our output dataframe new_data in every line is problematic.\n\nFirst, doing this for a procedure with lots of steps isn’t efficient and creates unnecessary repetition in the code.\nSecond, this repetition also makes it harder to identify exactly what is changing on each line in some cases.\n\n\n\nnew_data &lt;- filter(mtcars, cyl &gt;=5)\nnew_data &lt;- mutate(new_data, sqrt_wt=sqrt(wt))\nnew_data &lt;- filter(new_data,sqrt_wt&gt;1.5)\nnew_data &lt;- arrange(new_data,hp)"
  },
  {
    "objectID": "Week3-Lecture1.html#select-variables-select",
    "href": "Week3-Lecture1.html#select-variables-select",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "select() can select variables into a smaller dataset.\n\n\n# Select two columns: hp and cyl\nmtcars%&gt;%\n  select(hp, cyl) %&gt;%\n  head()\n\n\n\n\n\n\nhp\ncyl\n\n\n\n\nMazda RX4\n110\n6\n\n\nMazda RX4 Wag\n110\n6\n\n\nDatsun 710\n93\n4\n\n\nHornet 4 Drive\n110\n6\n\n\nHornet Sportabout\n175\n8\n\n\nValiant\n105\n6"
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "group_by() allows us to aggregate data by group and compute statistics for each group\n\n\n# group by cyl\nmtcars %&gt;%\n  group_by(cyl) \n\n\nAlthough nothing seemingly happens to the dataset, internally, the dataset is already grouped based on the specified variable(s)."
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by-summarise",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by-summarise",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "summarise() creates a new data frame after aggregating data. The final dataset\n\nhas one row for each pair of grouping variables (for each cyl value)\ncontains one column for each grouping variable (cyl)\ncontains one column for each new summarised variable (avg_mp)\n\n\n\n# compute the average mpg for each cyl group\nmtcars %&gt;%\n  group_by(cyl) %&gt;% # group by cyl\n  summarise(avg_mp = mean(mpg)) %&gt;% # compute the average mpg\n  ungroup()\n\n\n\n\n\ncyl\navg_mp\n\n\n\n\n4\n26.66364\n\n\n6\n19.74286\n\n\n8\n15.10000"
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by-summarise-1",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by-summarise-1",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "We can have multiple group variables for group_by\n\n\n# compute the average mpg for each cyl,vs group\nmtcars %&gt;%\n  group_by(cyl,vs) %&gt;% # group by cyl\n  summarise(avg_mp = mean(mpg)) %&gt;% # compute the average mpg\n  ungroup()"
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by-mutate",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by-mutate",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "Try the following code by replacing summarise() with mutate(), what do you get now?\n\n\n# compute the average mpg for each cyl,vs group\nmtcars %&gt;%\n  group_by(cyl,vs) %&gt;% # group by cyl\n  mutate(avg_mp = mean(mpg)) %&gt;% # compute the average mpg\n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\navg_mp\n\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n20.56667\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n20.56667\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n26.73000\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n19.12500\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n15.10000\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n19.12500\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n15.10000\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n26.73000\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n26.73000\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n19.12500\n\n\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n19.12500\n\n\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n15.10000\n\n\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n15.10000\n\n\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n15.10000\n\n\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n15.10000\n\n\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n15.10000\n\n\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n15.10000\n\n\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n26.73000\n\n\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n26.73000\n\n\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n26.73000\n\n\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n26.73000\n\n\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n15.10000\n\n\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n15.10000\n\n\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n15.10000\n\n\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n15.10000\n\n\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n26.73000\n\n\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n26.00000\n\n\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n26.73000\n\n\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n15.10000\n\n\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n20.56667\n\n\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n15.10000\n\n\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n26.73000\n\n\n\n\n\n\n\nA new column is added to the original dataset, the value of which is from the group-by aggregation."
  },
  {
    "objectID": "Week3-Lecture1.html#consolidate-multiple-data-frames",
    "href": "Week3-Lecture1.html#consolidate-multiple-data-frames",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "When consolidating multiple data frames, we usually have 4 types of joining methods"
  },
  {
    "objectID": "Week3-Lecture1.html#left_join",
    "href": "Week3-Lecture1.html#left_join",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "left_join keeps everything from the left data frame and matches as much as it can from the right data frame.\n\nAll IDs in the left data frame will be retained\nIf a match can be found, value from the right data frame will be filled in\nIf a match cannot be found, a missing value will be filled in\n\n\n\n# Method 1 without pipe operator\nleft_join(df_left, df_right, by = 'ID')\n# Method 2 with pipe operator\ndf_left %&gt;%\n  left_join(df_right, by = 'ID')"
  },
  {
    "objectID": "Week3-Lecture1.html#inner_join",
    "href": "Week3-Lecture1.html#inner_join",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "inner_join only keeps the observations that appear in both data frames\n\nOnly common IDs in both data frames will be retained\nIf a match can be found, values will be filled in from both data frames\n\n\n\n# Method 1 without pipe operator\ninner_join(df_left, df_right, by = 'ID')\n# Method 2 with pipe operator\ndf_left %&gt;%\n  inner_join(df_right, by = 'ID')\n# Method 3: order of data frames should not matter. Why?\ndf_right %&gt;%\n  inner_join(df_left, by = 'ID')"
  },
  {
    "objectID": "Week3-Lecture1.html#full_join",
    "href": "Week3-Lecture1.html#full_join",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "",
    "text": "full_join keeps all observations from both data frames\n\nAll IDs in either data frames will be retained\nIf a match can be found, values will be filled in from both data frames\n\n\n\n# Method 1 without pipe operator\nfull_join(df_left, df_right, by = 'ID')\n# Method 2 with pipe operator\ndf_left %&gt;%\n  full_join(df_right, by = 'ID')\n# Method 3: order of data frames should not matter. Why?\ndf_right %&gt;%\n  full_join(df_left, by = 'ID')"
  },
  {
    "objectID": "Week3-Lecture1.html#variable-types",
    "href": "Week3-Lecture1.html#variable-types",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "2.1 Variable Types",
    "text": "2.1 Variable Types\n\nNon-metric\n\nCategorical (gender, region, brand, religion)\nOrdinal (Business Week rankings, NCAA rankings)\n\nMetric\n\nContinuous (age, height, sales, rainfall)\n\nDifferent types of variables are handled in different ways in statistics\n\nCan talk about an average age, but not an average color\nSome statistical techniques only work with one type of variable\n\nWe need to make sure the variables are of the correct data types. Or we may need to convert them to the correct types.\n\ne.g., from character to date time using lubridate package"
  },
  {
    "objectID": "Week3-Lecture1.html#missing-values",
    "href": "Week3-Lecture1.html#missing-values",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "2.2 Missing Values",
    "text": "2.2 Missing Values\n\nIn R, missing values are represented by the symbol NA (i.e., not available).\nMost statistical models cannot handle missing values, so we need to deal with them in R.\n\nFew missing values: remove them from analysis.\nMany missing values: need to replace them with appropriate values: mean/median/imputation"
  },
  {
    "objectID": "Week3-Lecture1.html#outliers",
    "href": "Week3-Lecture1.html#outliers",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "2.3 Outliers",
    "text": "2.3 Outliers\n\nIn statistics, an outlier is a data point that differs significantly from other observations.\n\nFew outliers: remove them from analysis\nMany outliers: winsorize data\nIf the distribution of a variable is not normal distribution, we often log transform variables to mitigate outlier issues"
  },
  {
    "objectID": "Week3-Lecture1.html#two-major-tasks-of-descriptive-analytics",
    "href": "Week3-Lecture1.html#two-major-tasks-of-descriptive-analytics",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "3.1 Two Major Tasks of Descriptive Analytics",
    "text": "3.1 Two Major Tasks of Descriptive Analytics\n\nDescribe data depending on your business purposes\n\n“How much do our customers spend each month on average?”\n“What percentage of our customers are unprofitable?”\n“What is the difference between the retention rates of men and women?”\n\nMake statistical inferences from data\n\n“Based on our sample, does the difference between the spendings of men and women indicate that men and women respond differently in the customer base at large?”\n“Based on our sample, can we conclude that customers who sign up for online banking are more profitable than customers who do not?”\n“Based on our test mailing, can we conclude that ad-copy A works better than ad-copy B?”"
  },
  {
    "objectID": "Week3-Lecture1.html#descriptive-analytics-1",
    "href": "Week3-Lecture1.html#descriptive-analytics-1",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "3.2 Descriptive Analytics",
    "text": "3.2 Descriptive Analytics\n\nYou can think of descriptive analytics as creating a dashboard to display the key information you would like to know for your business."
  },
  {
    "objectID": "Week3-Lecture1.html#summary-statistics",
    "href": "Week3-Lecture1.html#summary-statistics",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "3.3 Summary Statistics",
    "text": "3.3 Summary Statistics\n\nIn descriptive analytics, summary statistics are used to summarize a set of observations, in order to communicate the largest amount of information as simply as possible.\nThere are two main types of summary statistics used in evaluation: measures of central tendency and measures of dispersion.\n\nMeasures of central tendency provide different versions of the average, including the mean, the median, 25 percentile, 75 percentile, the mode, etc.\nMeasures of dispersion provide information about how much variation there is in the data, including the range and the standard deviation.\n\nIt’s good to include summary statistics table in your dissertation before any statistical analysis!\n\nCommonly reported summary statistics include mean, standard deviation, number of observations, min, 25 percentile, median, 75 percentile, and max.\nThen describe the distribution of the variable, dispersion of the variable, etc."
  },
  {
    "objectID": "Week3-Lecture1.html#summary-statistics-with-r",
    "href": "Week3-Lecture1.html#summary-statistics-with-r",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "3.4 Summary Statistics with R",
    "text": "3.4 Summary Statistics with R\n\nIn R, a nice package to report summary statistics is modelsummary. datasummary_skim() is a shortcut to conduct basic summary statistics\nFor more features, refer to the package tutorial here, especially datasummary() function.\n\ndatasummary_skim() is a special case of more general datasummary(), which outputs a pre-determined set of summary statistics"
  },
  {
    "objectID": "Week3-Lecture1.html#correlation-matrix",
    "href": "Week3-Lecture1.html#correlation-matrix",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "3.5 Correlation Matrix",
    "text": "3.5 Correlation Matrix\n\nCorrelation matrix helps us understand the co-movement of any two variables in the data\ndatasummary_correlation() reports the pairwise correlation coefficient\nIn general, in a statistical model, variables of high correlation should not be included together, which leads to instability"
  },
  {
    "objectID": "Week3-Lecture1.html#after-class-exercise",
    "href": "Week3-Lecture1.html#after-class-exercise",
    "title": "Class 5 Data Wrangling with R (Part II)",
    "section": "4.1 After-Class Exercise",
    "text": "4.1 After-Class Exercise\n\nWhat percent of customers are single? Try alternative ways to do the calculation.\nIs the average total spending by responders and non-responders statistically different? Answer this question using a t-test.\nIs income and total spending correlated?\nAre PhDs more likely to respond to marketing offers than Graduation? Use a statistical test to answer the question. Is the result what you expected?\nWhat would be the other useful descriptive analytics you would like to know for Tesco?"
  }
]