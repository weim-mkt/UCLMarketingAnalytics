[
  {
    "objectID": "R-RStudioSetup.html",
    "href": "R-RStudioSetup.html",
    "title": "RStudio Setup",
    "section": "",
    "text": "In this guide, I will walk you through the recommended setup for RStudio. Please follow the steps below to configure your RStudio settings. For the settings that are not mentioned here, you can keep the default settings or adjust them according to your preference.",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "R-RStudioSetup.html#basic",
    "href": "R-RStudioSetup.html#basic",
    "title": "RStudio Setup",
    "section": "1.1 Basic",
    "text": "1.1 Basic\n\nUntick Restore most recently opened project at startup: This will prevent RStudio from opening the last project you worked on when you open RStudio.\nUntick Restore previously open source documents at startup: This will prevent RStudio from opening the last script you worked on when you open RStudio.\nUntick Restore .RData into workspace at startup: This will prevent RStudio from loading the last workspace when you open RStudio.\nTick Always save history (even when not saving .RData): This will save the history of your R commands even if you do not save the workspace.\n\n\n\n\nGeneral-Basic",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "R-RStudioSetup.html#editing",
    "href": "R-RStudioSetup.html#editing",
    "title": "RStudio Setup",
    "section": "2.1 Editing",
    "text": "2.1 Editing\n\nTick and untick the options here based on the screenshot below.\n\n\n\n\nCode",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "R-RStudioSetup.html#display",
    "href": "R-RStudioSetup.html#display",
    "title": "RStudio Setup",
    "section": "2.2 Display",
    "text": "2.2 Display\n\nTick and untick the options here based on the screenshot below. This section controls how your code is displayed in RStudio.\n\n\n\n\nCode",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "R-RStudioSetup.html#completion",
    "href": "R-RStudioSetup.html#completion",
    "title": "RStudio Setup",
    "section": "2.3 Completion",
    "text": "2.3 Completion\n\nTick and untick the options here based on the screenshot below. This section controls the code completion settings in RStudio.\n\n\n\n\nCode",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "R-RStudioSetup.html#basic-1",
    "href": "R-RStudioSetup.html#basic-1",
    "title": "RStudio Setup",
    "section": "4.1 Basic",
    "text": "4.1 Basic\n\nTick and untick the options here based on the screenshot below. This section controls the R Markdown settings in RStudio.\n\n\n\n\nCode",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "R-RStudioSetup.html#visual",
    "href": "R-RStudioSetup.html#visual",
    "title": "RStudio Setup",
    "section": "4.2 Visual",
    "text": "4.2 Visual\n\nTick and untick the options here based on the screenshot below. This section controls the visual settings in RStudio.\n\n\n\n\nCode",
    "crumbs": [
      "R Tutorials",
      "RStudio Setup"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html",
    "href": "Week4-Lecture1.html",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "",
    "text": "The core of any business decision is profitability analysis (BEQ, NPV, CLV). To increase firm profitability,\n\nIncrease revenue\nReduce costs (CAC or variable marketing costs)\nReduce customer churn\n\nIn Weeks 4 and 5, we will learn how to utilize predictive analytics to improve profitability. Correspondingly,\n\nDevelop customers through ML recommender systems\nReduce costs by targeting more responsive customers\nPredict customer churn and take preventive actions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning\n\nOnly observe X =&gt; Want to uncover unknown subgroups\n\nSupervised Learning\n\nObserve both X and Y =&gt; Want to predict Y for new data\n\n\nIn Term 2, you will learn predictive analytics models systematically. By then, think about how those techniques can be applied back to these case studies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstand the concept of unsupervised learning\nUnderstand how to apply K-means clustering and find the optimal number of clusters\nHow to apply clustering analyses for customer segmentation for M&S",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#roadmap-for-predictive-analytics",
    "href": "Week4-Lecture1.html#roadmap-for-predictive-analytics",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "",
    "text": "The core of any business decision is profitability analysis (BEQ, NPV, CLV). To increase firm profitability,\n\nIncrease revenue\nReduce costs (CAC or variable marketing costs)\nReduce customer churn\n\nIn Weeks 4 and 5, we will learn how to utilize predictive analytics to improve profitability. Correspondingly,\n\nDevelop customers through ML recommender systems\nReduce costs by targeting more responsive customers\nPredict customer churn and take preventive actions",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#types-of-predictive-analytics",
    "href": "Week4-Lecture1.html#types-of-predictive-analytics",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "",
    "text": "Unsupervised Learning\n\nOnly observe X =&gt; Want to uncover unknown subgroups\n\nSupervised Learning\n\nObserve both X and Y =&gt; Want to predict Y for new data\n\n\nIn Term 2, you will learn predictive analytics models systematically. By then, think about how those techniques can be applied back to these case studies.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#learning-objectives-for-today",
    "href": "Week4-Lecture1.html#learning-objectives-for-today",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "",
    "text": "Understand the concept of unsupervised learning\nUnderstand how to apply K-means clustering and find the optimal number of clusters\nHow to apply clustering analyses for customer segmentation for M&S",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-1",
    "href": "Week4-Lecture1.html#k-means-clustering-1",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.1 K-Means Clustering",
    "text": "2.1 K-Means Clustering\n\nK-means clustering is one of the most commonly used unsupervised machine learning algorithms for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst.\nFor data scientists: It can classify customers into multiple segments (i.e., clusters), such that customers within the same cluster are as similar as possible, whereas customers from different clusters are as dissimilar as possible. \nInput: (1) customer characteristics; (2) the number of clusters\nOutput: cluster membership of each customer",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#similarity-and-dissimilarity",
    "href": "Week4-Lecture1.html#similarity-and-dissimilarity",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.2 Similarity and Dissimilarity",
    "text": "2.2 Similarity and Dissimilarity\n\nThe clustering of observations into groups requires computing the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix.\nThe choice of similarity measures is a critical step in clustering.\nThe most common distance measures are the Euclidean distance (the default for K-means) and the Manhattan distance.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#euclidean-distance",
    "href": "Week4-Lecture1.html#euclidean-distance",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.3 Euclidean Distance",
    "text": "2.3 Euclidean Distance\n\nThe most common distance measure is the Euclidean distance. \\[\nd_{\\text{euc}}(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n\\]\nExample of Income and Spending for 3 customers\n\n\\(Income = (5, 10, 20)\\)\n\\(Spending = (3, 4, 12)\\)\n\nEuclidean distance\n\n\\(d_{\\text{euc}}(2, 1) = \\sqrt{(10-5)^2 + (4-3)^2} = \\sqrt{25 + 1} = \\sqrt{26}\\)\n\\(d_{\\text{euc}}(2, 3) = \\sqrt{(10-20)^2 + (4-12)^2} = \\sqrt{100 + 64} = \\sqrt{164}\\)\n\n\n\n\n\n\nCode\nIncome &lt;- c(5, 10, 20)\n\nSpending &lt;- c(3, 4, 12)\n\ndata &lt;- data.frame(Income, Spending, ID = c(\"Customer 1\", \"Customer 2\", \"Customer 3\"))\n\nggplot(data, aes(x = Income, y = Spending)) +\n    geom_point(aes(shape = ID, color = ID), size = 2.5) +\n    geom_text(aes(label = rownames(data)), vjust = -0.5) +\n    theme_stata()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nIncome &lt;- c(5, 10, 20)\n\nSpending &lt;- c(3, 4, 12)\n\ndata &lt;- data.frame(Income, Spending, ID = c(\"Customer 1\", \"Customer 2\", \"Customer 3\"))\n\nggplot(data, aes(x = Income, y = Spending)) +\n    geom_point(aes(shape = ID, color = ID), size = 2.5) +\n    geom_text(aes(label = rownames(data)), vjust = -0.5) +\n    theme_stata() +\n    # show the Eucleadian distance between Customer 1 and Customer 2; also show the vertical and horizontal lines\n    geom_segment(aes(x = 5, y = 3, xend = 10, yend = 4), linetype = \"dashed\") +\n    geom_segment(aes(x = 5, y = 3, xend = 5, yend = 4), linetype = \"dashed\") +\n    geom_segment(aes(x = 5, y = 4, xend = 10, yend = 4), linetype = \"dashed\") +\n\n    # show the Eucleadian distance between Customer 2 and Customer 3; also show the vertical and horizontal lines\n    geom_segment(aes(x = 10, y = 4, xend = 20, yend = 12), linetype = \"dashed\") +\n    geom_segment(aes(x = 10, y = 4, xend = 10, yend = 12), linetype = \"dashed\") +\n    geom_segment(aes(x = 10, y = 12, xend = 20, yend = 12), linetype = \"dashed\")",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#manhattan-distance",
    "href": "Week4-Lecture1.html#manhattan-distance",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.4 Manhattan Distance",
    "text": "2.4 Manhattan Distance\n\nAnother common distance measure is the Manhattan distance, which is less commonly used because the absolute value function is not differentiable.\n\n\\[\nd_{\\text{man}}(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\n\\]\n\nExample of Income and Spending for 3 customers\n\n\\(Income = c(5, 10, 20)\\)\n\\(Spending = c(3, 4, 12)\\)\n\nDistance\n\n\\(d_{\\text{man}}(2, 1) = |10-5| + |4-3| = 5 + 1 = 6\\)\n\\(d_{\\text{man}}(2, 3) = |10-20| + |4-12| = 10 + 8 = 18\\)",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-step-1",
    "href": "Week4-Lecture1.html#k-means-clustering-step-1",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.5 K-Means Clustering: Step 1",
    "text": "2.5 K-Means Clustering: Step 1\n\n\n\n\n\n\n\n\n\nRaw data points; each dot is a customer\nX and Y axis are customer characteristics, say, income and spending\nObviously there should be 2 segments\nLet’s see how K-means uses a data-driven way to classify customers into 2 segments",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-step-2",
    "href": "Week4-Lecture1.html#k-means-clustering-step-2",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.6 K-Means Clustering: Step 2",
    "text": "2.6 K-Means Clustering: Step 2\n\n\n\n\n\n\n\n\n\nWe specify 2 segments\nK-means initializes the process by randomly selecting 2 centroids\n\n\n\nDue to this randomness, different starting points may yield varying results. We need to reinitialize the process repeatedly to ensure robustness of results.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-step-3",
    "href": "Week4-Lecture1.html#k-means-clustering-step-3",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.7 K-Means Clustering: Step 3",
    "text": "2.7 K-Means Clustering: Step 3\n\n\n\n\n\n\n\n\n\nK-means computes the distance of each customer to the red and blue centroids\nK-means assigns each customer to red segment or blue segment based on which centroid is closer",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-step-4",
    "href": "Week4-Lecture1.html#k-means-clustering-step-4",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.8 K-Means Clustering: Step 4",
    "text": "2.8 K-Means Clustering: Step 4\n\n\n\n\n\n\n\n\n\nK-means updates the new centroids of each segment\nThe red cross and blue cross in the picture are the new centroids\nWe still see some “outliers”, so the algorithm continues",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-step-5",
    "href": "Week4-Lecture1.html#k-means-clustering-step-5",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.9 K-Means Clustering: Step 5",
    "text": "2.9 K-Means Clustering: Step 5\n\n\n\n\n\n\n\n\n\nK-means computes the distance of each customer to the red and blue centroids\nK-means updates each customer to red segment or blue segment based on which centroid is closer\nNow the outliers are correctly assigned each segment",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#k-means-clustering-step-6",
    "href": "Week4-Lecture1.html#k-means-clustering-step-6",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.10 K-Means Clustering: Step 6",
    "text": "2.10 K-Means Clustering: Step 6\n\n\n\n\n\n\n\n\n\nK-means updates the new centroid from the previous clustering\nK-means computes the distance of each customer to the new centroids\nK-means finds that all customers are correctly assigned to their nearest centroids, so the algorithm does not need to continue\nWe say, the algorithm converges, and the algorithm stops",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture1.html#after-class-readings",
    "href": "Week4-Lecture1.html#after-class-readings",
    "title": "Class 7 Unsupervised Learning and K-Means Clustering",
    "section": "2.11 After-Class Readings",
    "text": "2.11 After-Class Readings\n\nMore technical details: K-means Cluster Analysis",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 1: Unsupervised Learning and K-Means Clustering"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html",
    "href": "Week4-Lecture2.html",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "",
    "text": "Segmentation is a key step in the marketing strategy (STP) process, where customers are divided into meaningful groups based on characteristics relevant to designing and executing your marketing strategy.\n\n\n\n\n\n\n\n\n\nIt assumes that different customer groups provide varying levels of value to the company and/or require distinct marketing programs to succeed (e.g., based on differing goals and needs).\n\n\n\n\nCustomer value segmentation is for targeting decisions based on customers’ potential long-term financial and strategic value to your company.\nDemographic segmentation uses variables such as age, gender, income, family life cycle, educational qualification, socio-economic status, religion, company size and income, etc. These serve as proxies for goals, preferences or psychographics, as well as to characterize segments for marketing mix decisions.\nPsychographic segmentation is for positioning and marketing mix design based on the psychology of the customer and consumer, including attitudes, identity, lifestyle, personality, etc.\n\nConventional segmentation methods often require subjective judgments. A more objective approach is to ‘let the data speak’ by utilizing data analytics tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\nx: data with selected variables to apply K-means\ncenters: number of clusters\niter.max: the maximum number of iterations allowed\nnstart: how many random sets should be chosen\nalgorithm: which algorithm to choose; default often works\ntrace: do you want to trace intermediate steps?",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#customer-segmentation",
    "href": "Week4-Lecture2.html#customer-segmentation",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "",
    "text": "Segmentation is a key step in the marketing strategy (STP) process, where customers are divided into meaningful groups based on characteristics relevant to designing and executing your marketing strategy.\n\n\n\n\n\n\n\n\n\nIt assumes that different customer groups provide varying levels of value to the company and/or require distinct marketing programs to succeed (e.g., based on differing goals and needs).",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#conventional-segmentation",
    "href": "Week4-Lecture2.html#conventional-segmentation",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "",
    "text": "Customer value segmentation is for targeting decisions based on customers’ potential long-term financial and strategic value to your company.\nDemographic segmentation uses variables such as age, gender, income, family life cycle, educational qualification, socio-economic status, religion, company size and income, etc. These serve as proxies for goals, preferences or psychographics, as well as to characterize segments for marketing mix decisions.\nPsychographic segmentation is for positioning and marketing mix design based on the psychology of the customer and consumer, including attitudes, identity, lifestyle, personality, etc.\n\nConventional segmentation methods often require subjective judgments. A more objective approach is to ‘let the data speak’ by utilizing data analytics tools.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#syntax-of-kmeans",
    "href": "Week4-Lecture2.html#syntax-of-kmeans",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "",
    "text": "x: data with selected variables to apply K-means\ncenters: number of clusters\niter.max: the maximum number of iterations allowed\nnstart: how many random sets should be chosen\nalgorithm: which algorithm to choose; default often works\ntrace: do you want to trace intermediate steps?",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#data-loading",
    "href": "Week4-Lecture2.html#data-loading",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "2.1 Data Loading",
    "text": "2.1 Data Loading\n\nLet’s first try customer segmentation based on total spending and Income.\nExercise: load data_full, create total_spending, and select total_spending and Income as the clustering variables into a new data frame data_kmeans.\n\n\n\nCode\npacman::p_load(dplyr, ggplot2, ggthemes, broom)\ndata_full &lt;- read.csv(\"images/data_full.csv\") %&gt;%\n    mutate(total_spending = MntWines + MntFruits + MntMeatProducts + MntFishProducts + MntSweetProducts + MntGoldProds)\n\ndata_kmeans &lt;- data_full %&gt;%\n    select(total_spending, Income)",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#data-pre-processing-1",
    "href": "Week4-Lecture2.html#data-pre-processing-1",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "2.2 Data Pre-processing",
    "text": "2.2 Data Pre-processing\n\nTo perform a cluster analysis in R, generally, the data should be prepared as follows:\n\nRows are observations (individuals) and columns are variables of interest for clustering.\nAny missing value in the data must be removed or imputed.\nThe data must be standardized (i.e., scaled) to make variables comparable. Standardization consists of transforming the variables such that they have mean zero and standard deviation one.1",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#data-pre-processing-missing-values",
    "href": "Week4-Lecture2.html#data-pre-processing-missing-values",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "2.3 Data Pre-processing: Missing Values",
    "text": "2.3 Data Pre-processing: Missing Values\n\nCheck if there are any missing values in the data.\nUse mean imputation to fill in missing values.\n\n\n\nCode\ndata_kmeans &lt;- data_kmeans %&gt;%\n    mutate(Income = ifelse(is.na(Income), mean(Income, na.rm = TRUE), Income))",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#data-pre-processing-standardization",
    "href": "Week4-Lecture2.html#data-pre-processing-standardization",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "2.4 Data Pre-processing: Standardization",
    "text": "2.4 Data Pre-processing: Standardization\n\nNeed to re-scale the clustering variables using scale(), because the variables can be of very different scales.\n\nExercise: Scale the variables and create a new data frame data_kmeans_scaled.\nThis is extremely important!\n\n\n\n\nCode\n# method 1\ndata_kmeans_scaled &lt;- data_kmeans %&gt;%\n    select(total_spending, Income) %&gt;%\n    mutate(\n        total_spending = scale(total_spending),\n        Income = scale(Income)\n    )\n\n# method 2: using across when there are many variables with the same transformation\ndata_kmeans_scaled &lt;- data_kmeans %&gt;%\n    select(total_spending, Income) %&gt;%\n    mutate(across(everything(), scale))",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#visualization-of-the-data",
    "href": "Week4-Lecture2.html#visualization-of-the-data",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "2.5 Visualization of the Data",
    "text": "2.5 Visualization of the Data\n\nLet’s visualize the data to see if there are any natural clusters.\nExercise: Create a scatter plot of total_spending and Income using ggplot2.\nRefer to the ggplot2 cheat sheet for more information on data visualization in R.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#apply-k-means-clustering-with-2-clusters",
    "href": "Week4-Lecture2.html#apply-k-means-clustering-with-2-clusters",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "3.1 Apply K-Means Clustering with 2 Clusters",
    "text": "3.1 Apply K-Means Clustering with 2 Clusters\n\nset.seed() is to allow replication of results.\nkmeans() is the function to perform K-means clustering.\ncenters is the number of clusters to form.\nnstart is the number of sets to be chosen.\n\n\n\nCode\nset.seed(888)\nresult_kmeans &lt;- kmeans(data_kmeans_scaled,\n    centers = 2,\n    nstart = 10\n)",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#examine-the-returned-object-result_kmeans",
    "href": "Week4-Lecture2.html#examine-the-returned-object-result_kmeans",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "3.2 Examine the returned object, result_kmeans",
    "text": "3.2 Examine the returned object, result_kmeans\n\n\nCode\ntidy(result_kmeans)\n\n\n\n  \n\n\n\n\nsize: The number of points in each cluster.\ncluster: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.\nwithinss: Vector of within-cluster sum of squares, one component per cluster.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#visualize-the-clusters",
    "href": "Week4-Lecture2.html#visualize-the-clusters",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "3.3 Visualize the clusters",
    "text": "3.3 Visualize the clusters\n\nWe need 2 packages cluster and factoextra\nUse function fviz_cluster() to generate visualizations\n\n\n\nCode\npacman::p_load(cluster, factoextra)\n\nfviz_cluster(result_kmeans,\n    data = data_kmeans_scaled\n)",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#determine-the-optimal-number-of-clusters-gap-method",
    "href": "Week4-Lecture2.html#determine-the-optimal-number-of-clusters-gap-method",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "4.1 Determine the optimal number of clusters: GAP Method",
    "text": "4.1 Determine the optimal number of clusters: GAP Method\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be the value that maximizes the gap statistic.\n\n\n\nCode\nset.seed(888)\ndata_kmeans_scaled %&gt;%\n    clusGap(FUN = kmeans, K.max = 10, B = 50) %&gt;%\n    fviz_gap_stat()",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#determine-the-optimal-number-of-clusters-silhouette-method",
    "href": "Week4-Lecture2.html#determine-the-optimal-number-of-clusters-silhouette-method",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "4.2 Determine the optimal number of clusters: Silhouette Method",
    "text": "4.2 Determine the optimal number of clusters: Silhouette Method\n\nThe silhouette value measures how similar an object is to its cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n\n\n\nCode\nset.seed(888)\ndata_kmeans_scaled %&gt;%\n    fviz_nbclust(kmeans, method = \"silhouette\")",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#next-steps-after-segmentation",
    "href": "Week4-Lecture2.html#next-steps-after-segmentation",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "4.3 Next Steps After Segmentation",
    "text": "4.3 Next Steps After Segmentation\n\nCompare the CLV in different segments, and decide which segments to serve.\nDevelop marketing strategies for each segment. For example, for the high-value segment, you may want to increase the frequency of purchase by offering discounts or promotions.\nDevelop a customer journey map for each segment.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#term-3-project-scopes",
    "href": "Week4-Lecture2.html#term-3-project-scopes",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "4.4 Term 3 Project Scopes",
    "text": "4.4 Term 3 Project Scopes\n\nSmartphones contain sensors, from which we can apply machine learning models to understand the context of the user, whether it be relaxing on the sofa, jogging in a park, or working indoors in an office. The task is to consume this real life data and produce visualisations, and to produce an anomaly detection engine. The project may be extended to clustering users according to their behavioural patterns in an unsupervised fashion.\nThe project will explore fraud detection approaches using unsupervised ML including models such as isolation forests. The candidate will develop an understanding of the business problem and our data, formulating hypotheses and testing them. They will build, evaluate, and interpret their ML models.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#after-class-readings",
    "href": "Week4-Lecture2.html#after-class-readings",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "4.5 After-Class Readings",
    "text": "4.5 After-Class Readings\n\nAfter-class Exercise: Try total spending, Frequency, and Recency as clustering covariates. Why these three variables? Then, find the optimal number of clusters. Visualize the clusters.\n\nBecause they are the most important variables for customer segmentation, i.e., RFM (Recency, Frequency, Monetary) analysis.\n\nUseful source: K-means Cluster Analysis",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week4-Lecture2.html#footnotes",
    "href": "Week4-Lecture2.html#footnotes",
    "title": "Class 8: Customer Segmentation Using Unsupervised Learning for M&S",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnother common method is to normalize the data, which consists of transforming the variables such that they have a minimum of zero and a maximum of one.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Lecture 2: Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "R-Basics.html",
    "href": "R-Basics.html",
    "title": "R Basics",
    "section": "",
    "text": "Primary language is Python\n\nProgramming (MSIN00143), Business Strategy (MSIN0093), Machine Learning electives\n\nSecondary language is R\n\nMarketing Analytics (MSIN0094), Statistical Foundations (MSIN0096)\n\n\n\n\n\n\nR project was initiated by Robert Gentleman and Ross Ihaka (Univ of Auckland) in 1991; both are statisticians, who later made the language open-source.\nSince 1997, R has been developed by the R Core Team on CRAN.\nAs of January 2022, it has almost 20k contributed packages.\nAs of 2024, R is ranked 18th in the TIOBE index1.\n\n\n\n\n\nHighly powerful data analytics and visualizations, including2\n\nData wrangling (dplyr) and data visualization (ggplot)\nStatistics and Econometrics (major advantage of R over Python)\nPredictive analytics such as machine learning\n\nWrite beautiful reports, dissertations, presentations using Quarto\n\nWrite your MSc dissertation\nEffortlessly build websites. I built and maintain my personal website and the marketing course website all in R.\n\n\n\n\n\n\nAs you will be learning Python in the programming course, it’s good to know the differences between R and Python. In addition to the general comparison below, I have also prepared a detailed side-by-side comparison of R and Python here.\nIt’s highly recommended that when you learn both languages at the same time, you should be able to compare them side-by-side often.\n\n\nR versus Python\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nLanguage purpose\nR is a statistical language specialized in the data analytics and visualization.\nBest for data science, may not be robust for production environment.\nPython is a general-purpose language that is used for the deployment and development of various projects.\nBest for production environment.\n\n\nData analytics\nR is better at statistical models and econometrics.\nPython is better at machine learning due to support from PyTorch and TensorFlow.\n\n\nIDEs (Integrated Development Environment)\nRStudio\nMany options such as Jupyter Notebook, Spyder, Pycharm, etc.\n\n\nTargeted users\nPrimary users of R include data scientists and researchers in academia, who heavily rely on data analyses and visualization.\nPrimary users of python include developers and programmers.\n\n\n\n\n\n\nR is the programming language, and we need a “place” to write codes. This place is called an Integrated development environment (IDE).\nRStudio is so far the best R IDE. And it’s interface consists of the following major panels (clockwise from top left):\n\nscript: (top left) where you do the coding\nenvironment: (top right) a list of named objects that we have generated\nhistory: (top right) the list of past commands that we have used\nhelp: (bottom right) user manuals of functions available in R\npackage: (bottom right) a collection of ready-to-use packages written by others\nconsole: (bottom left) where you can run commands interactively with R and see code outputs\n\n\n\n\n\n\nYou can write codes interactively in the R console. See an example: Type the following code into your console and see what happens.\n\n\nCode\nprint(\"Hello World\")\n\n\n[1] \"Hello World\"\n\n\nUsed for simple exploratory, unstructured tasks, where you don’t need to keep a record of codes.\n\ne.g., summary statistics; check variable values, etc.\n\n\n\n\n\n\nQuarto3 files have a .qmd suffix. You can think of Quarto as Microsoft Word that can run R codes.\nIf you have experience with Jupyter Notebook, Quarto is the R equivalent of Jupyter Notebook, but just much more powerful.\nQuarto can create dynamic contents with R, conveniently combining data analytics work with beautiful reporting.\nNow, let’s create a new quarto file together! Name it “MyFavoriteShow.qmd” and save it to your download folder.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#bilingual-arrangements-at-msc-ba",
    "href": "R-Basics.html#bilingual-arrangements-at-msc-ba",
    "title": "R Basics",
    "section": "",
    "text": "Primary language is Python\n\nProgramming (MSIN00143), Business Strategy (MSIN0093), Machine Learning electives\n\nSecondary language is R\n\nMarketing Analytics (MSIN0094), Statistical Foundations (MSIN0096)",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#a-brief-history-of-r",
    "href": "R-Basics.html#a-brief-history-of-r",
    "title": "R Basics",
    "section": "",
    "text": "R project was initiated by Robert Gentleman and Ross Ihaka (Univ of Auckland) in 1991; both are statisticians, who later made the language open-source.\nSince 1997, R has been developed by the R Core Team on CRAN.\nAs of January 2022, it has almost 20k contributed packages.\nAs of 2024, R is ranked 18th in the TIOBE index1.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#why-learn-r",
    "href": "R-Basics.html#why-learn-r",
    "title": "R Basics",
    "section": "",
    "text": "Highly powerful data analytics and visualizations, including2\n\nData wrangling (dplyr) and data visualization (ggplot)\nStatistics and Econometrics (major advantage of R over Python)\nPredictive analytics such as machine learning\n\nWrite beautiful reports, dissertations, presentations using Quarto\n\nWrite your MSc dissertation\nEffortlessly build websites. I built and maintain my personal website and the marketing course website all in R.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#one-one-comparison-with-python",
    "href": "R-Basics.html#one-one-comparison-with-python",
    "title": "R Basics",
    "section": "",
    "text": "As you will be learning Python in the programming course, it’s good to know the differences between R and Python. In addition to the general comparison below, I have also prepared a detailed side-by-side comparison of R and Python here.\nIt’s highly recommended that when you learn both languages at the same time, you should be able to compare them side-by-side often.\n\n\nR versus Python\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nLanguage purpose\nR is a statistical language specialized in the data analytics and visualization.\nBest for data science, may not be robust for production environment.\nPython is a general-purpose language that is used for the deployment and development of various projects.\nBest for production environment.\n\n\nData analytics\nR is better at statistical models and econometrics.\nPython is better at machine learning due to support from PyTorch and TensorFlow.\n\n\nIDEs (Integrated Development Environment)\nRStudio\nMany options such as Jupyter Notebook, Spyder, Pycharm, etc.\n\n\nTargeted users\nPrimary users of R include data scientists and researchers in academia, who heavily rely on data analyses and visualization.\nPrimary users of python include developers and programmers.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#a-first-look-at-the-rstudio-interface",
    "href": "R-Basics.html#a-first-look-at-the-rstudio-interface",
    "title": "R Basics",
    "section": "",
    "text": "R is the programming language, and we need a “place” to write codes. This place is called an Integrated development environment (IDE).\nRStudio is so far the best R IDE. And it’s interface consists of the following major panels (clockwise from top left):\n\nscript: (top left) where you do the coding\nenvironment: (top right) a list of named objects that we have generated\nhistory: (top right) the list of past commands that we have used\nhelp: (bottom right) user manuals of functions available in R\npackage: (bottom right) a collection of ready-to-use packages written by others\nconsole: (bottom left) where you can run commands interactively with R and see code outputs",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#where-to-write-r-codes-i-console",
    "href": "R-Basics.html#where-to-write-r-codes-i-console",
    "title": "R Basics",
    "section": "",
    "text": "You can write codes interactively in the R console. See an example: Type the following code into your console and see what happens.\n\n\nCode\nprint(\"Hello World\")\n\n\n[1] \"Hello World\"\n\n\nUsed for simple exploratory, unstructured tasks, where you don’t need to keep a record of codes.\n\ne.g., summary statistics; check variable values, etc.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#where-to-write-r-codes-ii-.qmd-script",
    "href": "R-Basics.html#where-to-write-r-codes-ii-.qmd-script",
    "title": "R Basics",
    "section": "",
    "text": "Quarto3 files have a .qmd suffix. You can think of Quarto as Microsoft Word that can run R codes.\nIf you have experience with Jupyter Notebook, Quarto is the R equivalent of Jupyter Notebook, but just much more powerful.\nQuarto can create dynamic contents with R, conveniently combining data analytics work with beautiful reporting.\nNow, let’s create a new quarto file together! Name it “MyFavoriteShow.qmd” and save it to your download folder.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#yaml-header",
    "href": "R-Basics.html#yaml-header",
    "title": "R Basics",
    "section": "2.1 YAML header",
    "text": "2.1 YAML header\n\nYou can think of YAML header as a MS Word template, which determines how your final report looks like (font, font size, color, margins, etc.).\nThe YAML header is always at the beginning of a document, separated from the main text by three dashes (---). YAML does not appear in the final report.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#authoring-with-normal-texts",
    "href": "R-Basics.html#authoring-with-normal-texts",
    "title": "R Basics",
    "section": "2.2 Authoring with normal texts",
    "text": "2.2 Authoring with normal texts\nRStudio provides two ways to edit a quarto file (1) visual mode and (2) source mode.\n\nRStudio’s visual editor offers a Microsoft-Word like experience for you to write R codes.\n\nExplore the rich formatting tools available for report authoring\n\nIf you are familiar with markdown syntax, you can use the source mode to write the report (optional; for advanced users only).\n\n\n\n\n\n\n\n\nVisual Mode versus Source Mode\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a new quarto file from RStudio with the following level-1 and level-2 headers\nLevel 1: Slowhorse Season 4\nLevel 2: Episode 1: Identity Theft\nBody: A London bombing puts Taverner under pressure. When River grows concerned for his grandfather, Louisa encourages him to go for a visit.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#coding-with-code-blocks",
    "href": "R-Basics.html#coding-with-code-blocks",
    "title": "R Basics",
    "section": "2.3 Coding with code blocks",
    "text": "2.3 Coding with code blocks\n\nIn qmd files, we write R codes in so-called code chunks (sometimes referred to as code cells or code blocks) identified with {r}.\nTo insert a code chunk, click Insert -&gt;Code Chunk -&gt; R. You can also use the shortcut Ctrl + Alt + I or Cmd + Option + I.\n\n\n\n\n\n\n\nCaveat\n\n\n\nEnsure the first line remains {r} only and do not include any comments or code on this line.\n\n\n\nYou can run each code chunk interactively by clicking the green solid triangle (run current code chunk). RStudio executes the codes in the code chunk and displays the results.\nSee an example and try on your computer!\n\n\n\nCode\nprint(\"R is the Best Language! Way better than Python! The battle is on!\")\n\n\n[1] \"R is the Best Language! Way better than Python! The battle is on!\"\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInsert the above R code block in your quarto file under any section.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#rendering-a-report",
    "href": "R-Basics.html#rendering-a-report",
    "title": "R Basics",
    "section": "2.4 Rendering a report",
    "text": "2.4 Rendering a report\nAt the end, when the Quarto document (including codes and main texts) are ready, use the Render button in the RStudio IDE to render the file.\nThe rendered report will be in the same folder with your qmd file.\n\n\n\n\n\n\n\nExercise\n\n\n\nRender your quarto file into a document and see how it looks like.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#more-learning-resources-for-quarto-optional",
    "href": "R-Basics.html#more-learning-resources-for-quarto-optional",
    "title": "R Basics",
    "section": "2.5 More learning resources for Quarto (Optional)",
    "text": "2.5 More learning resources for Quarto (Optional)\n\nThe available YAML fields vary based on document format\n\nHere for YAML fields for PDF documents\nHere for MS Word\nHere for HTML documents\n\nMarkdown syntax\n\nMarkdown basics\nMarkdown practice\n\nQuarto (recommended to be reviewed after-class)\n\nGet started",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#named-objects",
    "href": "R-Basics.html#named-objects",
    "title": "R Basics",
    "section": "3.1 Named objects",
    "text": "3.1 Named objects\n\nR is an object-oriented language, so we will be working on named objects.\nWe use the left arrow &lt;- to create a named object, the keyboard shortcut for &lt;- for windows users is Alt + -, or for mac users, Option + -.\nThe &lt;- is an assignment operator, which assigns the R objects on the RHS to the name on the LHS.4\nThe below code creates a new object called ‘x’ in R; x is a numeric object; its value is 3.\n\n\n\nCode\n# create an object x with value 3\nx &lt;- 3\n\n\n\nAfter an object is created, we can refer to the object by its name\n\n\n\nCode\n# print out x\nx\n\n\n[1] 3\n\n\n\nWe can also perform operations on the object\n\n\n\nCode\n# Question: hmmm, why does Wei choose these two numbers?\nx^2\n\n\n[1] 9\n\n\nCode\nx^3\n\n\n[1] 27\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInsert a code block in your quarto file, which does the following:\n\nCreate an object with name ‘x’ with the formula of 2 + 2",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#rules-for-naming-object",
    "href": "R-Basics.html#rules-for-naming-object",
    "title": "R Basics",
    "section": "3.2 Rules for naming object",
    "text": "3.2 Rules for naming object\nFor a variable to be valid, it should follow these rules\n\nIt should contain letters, numbers, and only dot or underscore characters.\nIt cannot start with a number (eg: 2iota), or a dot, or an underscore.\n\n\n\nCode\n# 2iota &lt;- 2\n# .iota &lt;- 2\n# _iota &lt;- 2\n\n\n\nIt should not be a reserved word in R (eg: mean, sum, etc.).\n\n\n\nCode\n# mean &lt;- 2\n\n\n\n\n\n\n\n\nTips\n\n\n\nIn the future, it’s good practice to use memorable names to name an object\n\nFor instance, use prefix “df_” or “data_” to name datasets.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#functions",
    "href": "R-Basics.html#functions",
    "title": "R Basics",
    "section": "3.3 Functions",
    "text": "3.3 Functions\n\nA function usually takes (one or several) objects as input, run specific operations on the object(s) defined by the function, and then return an output.\nFor instance, an R’s built-in function sqrt() takes a number as input, and returns the square root of the number. Let’s use it on object x.\n\n\n\nCode\nsqrt(x)\n\n\n[1] 1.732051\n\n\n\nWe will heavily rely on functions to conduct data analyses. For how to use a new function, search the function in RStudio’s help panel.\n\n\n\n\n\n\n\nExercise\n\n\n\n\nSearch and learn the usage of function “log()”.\nInsert a code block in your quarto file to compute the logarithm of x.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#collection-of-functions-packages",
    "href": "R-Basics.html#collection-of-functions-packages",
    "title": "R Basics",
    "section": "3.4 Collection of functions: Packages",
    "text": "3.4 Collection of functions: Packages\nThe base R already comes with many useful built-in functions to perform basic tasks, but as data scientists, we need more.\nTo perform certain tasks (such as a machine learning model), we can definitely write our own code from scratch, but it takes lots of (unnecessary) effort. Fortunately, many packages have been written by others for us to directly use.\n\nTo download a package, hit Tools -&gt; Install Packages in RStudio, and type the package name in the pop-up window. Now, download the package praise.\nTo load the packages, we need to type library().\n\n\n\nCode\nlibrary(praise)\n\n\n\nNow that the package is loaded, you can use the functions in it. praise() is a function in the praise package.\n\n\n\nCode\npraise()\n\n\n[1] \"You are kryptonian!\"\n\n\n\n\n\n\n\n\nTips\n\n\n\n\nPackages need to be downloaded only once, but need to be loaded every time you restart the RStudio.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#comment-codes",
    "href": "R-Basics.html#comment-codes",
    "title": "R Basics",
    "section": "3.5 Comment codes",
    "text": "3.5 Comment codes\nYou can put a # before any code, to indicate that any codes after the # on the same line are your comments, and will not be run by R.\nIt’s a good practice to often comment your codes, so that you can help the future you to remember what you were trying to achieve.\n\n\nCode\n# print(\"Let's fund Wei for an iPhone 16 Pro Max as a birthday gift!\")\n\n\n\n\nCode\n# Is x 1 or 2 below?\nx &lt;- 1 # +1",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#data-types",
    "href": "R-Basics.html#data-types",
    "title": "R Basics",
    "section": "4.1 Data types",
    "text": "4.1 Data types\n\n4.1.1 Numeric\n\nWe can use R as a calculator for numeric objects\n\n\n\nCode\n# Numeric Vector\nnum2 &lt;- 2.5\nlog(num2)\n\n\n[1] 0.9162907\n\n\nCode\nnum2^2\n\n\n[1] 6.25\n\n\nCode\nexp(num2)\n\n\n[1] 12.18249\n\n\n\n\n4.1.2 Logical (TRUE, FALSE):\n\nLogical objects are used to store logical values, such as TRUE and FALSE.\n\n\n\nCode\nnum2 &lt;- 2.5\n\n# larger than 2?\nnum2 &gt; 2\n\n\n[1] TRUE\n\n\nCode\n# smaller than 2?\nnum2 &lt; 2\n\n\n[1] FALSE\n\n\nCode\n# equal to 2?\nnum2 == 2\n\n\n[1] FALSE\n\n\nCode\n# not equal to 2?\nnum2 != 2\n\n\n[1] TRUE\n\n\n\nSometimes, we may need to operation on multiple relational operations. We can use logical operators to combine multiple relational operations.\n\n\n\nCode\nT & F # and\n\n\n[1] FALSE\n\n\nCode\nT | F # or\n\n\n[1] TRUE\n\n\nCode\n!T # not\n\n\n[1] FALSE\n\n\n\nFor instance, we may want to know if a number is between 3 and 8.\n\n\n\nCode\nnum2 &gt;= 3 & num2 &lt;= 8\n\n\n[1] FALSE\n\n\n\n\n4.1.3 Character:\n\nCharacters are enclosed within a pair of quotation marks.\nSingle or double quotation marks can both work.\nEven if a character may contain numbers, it will be treated as a character, and R will not perform any mathematical operations on it.\n\n\n\nCode\nstr1 &lt;- \"1 + 1 = 2\"",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#check-data-types-using-class",
    "href": "R-Basics.html#check-data-types-using-class",
    "title": "R Basics",
    "section": "4.2 Check data types using class()",
    "text": "4.2 Check data types using class()\nWe can use class() to check the type of an object in R.\n\n\nCode\na &lt;- \"1+1\"\nclass(a)\n\n\n[1] \"character\"\n\n\n\n\nCode\nb &lt;- 1 + 1\nclass(b)\n\n\n[1] \"numeric\"\n\n\nThis is very useful when we first load data from external databases, we need to make sure variables are of the correct data types.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#data-type-conversion",
    "href": "R-Basics.html#data-type-conversion",
    "title": "R Basics",
    "section": "4.3 Data type: conversion",
    "text": "4.3 Data type: conversion\nSometimes, data types of variables from raw data may not be what we want; we need to change the data type of a variable to the appropriate one.\nSee the following example:\n\na is a string, and we cannot use mathematical operations on it, or R will report errors.\n\n\n\nCode\na &lt;- \"1\"\nclass(a)\n\n\n[1] \"character\"\n\n\nCode\na + 1\n\n\nError in a + 1: non-numeric argument to binary operator\n\n\n\nWe can convert a to a numeric value. To convert from character to numeric, we use as.numeric()\n\n\n\nCode\nb &lt;- as.numeric(a)\nclass(b)\n\n\n[1] \"numeric\"",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#creating-vectors",
    "href": "R-Basics.html#creating-vectors",
    "title": "R Basics",
    "section": "5.1 Creating vectors",
    "text": "5.1 Creating vectors\n\n5.1.1 Creating vectors: c()\n\nIn R, a vector is a collection of elements of the same data type, which is often used to store a variable of a dataset. For instance, a vector can store the income of a group of people, the final grades of students, etc.\nVector can be created using the function c() by listing all the values in the parenthesis, separated by comma ‘,’.\nc() stands for “combine”.\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, 10)\nIncome\n\n\n[1]  1  3  5 10\n\n\n\nVectors must contain elements of the same data type. If they do not, R will automatically convert all elements into the same type, typically characters.\n\n\n\nCode\nx &lt;- c(1, \"intro\", TRUE)\nclass(x)\n\n\n[1] \"character\"\n\n\n\n\n5.1.2 Checking the number of elements in a vector: length()\nYou can measure the length of a vector using the command length()\n\n\nCode\nx &lt;- c(\"R\", \" is\", \" the\", \" best\", \" language\")\nlength(x)\n\n\n[1] 5\n\n\n\n\n5.1.3 Creating numeric sequences: seq()\nIt is also possible to easily create sequences with patterns\n\nuse seq() to create sequence with fixed steps\n\n\n\nCode\n# use seq()\nseq(from = 1, to = 2, by = 0.1)\n\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0\n\n\n\nIf the step is 1, there’s a convenient way using :\n\n\n\nCode\n1:5\n\n\n[1] 1 2 3 4 5\n\n\n\n\n5.1.4 Concatenate multiple vectors into one: c()\n\nSometimes, we may want to concatenate multiple vectors into one. For instance, we may have collected income data from two different sources, and we want to concatenate them into one vector.\nWe can use c() to concatenate different vectors; this is very commonly used to concatenate vectors.\n\n\n\nCode\nIncome1 &lt;- 1:3\nIncome2 &lt;- c(10, 15)\n\n\n\n\nCode\nc(Income1, Income2)\n\n\n[1]  1  2  3 10 15\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a sequence of {1,1,2,2,3,3,3}.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#indexing-and-subsetting",
    "href": "R-Basics.html#indexing-and-subsetting",
    "title": "R Basics",
    "section": "5.2 Indexing and subsetting",
    "text": "5.2 Indexing and subsetting\nWe put the index of elements we would like to extract in a square bracket [ ].\n\n\nCode\n# create a vector of income for 4 lecturers at UCL\n\nincome &lt;- c(5000, 5500, 6000, 9000)\n\n\n\nExtract a single element: use the index of the element\n\n\n\nCode\n# what is the income of the 3rd lecturer?\nincome[3]\n\n\n[1] 6000\n\n\n\nExtract multiple elements: use a vector of indices\n\n\n\nCode\n# what are the incomes of the 1st, 3rd, and 4th lecturers?\nincome[c(1, 3, 4)]\n\n\n[1] 5000 6000 9000",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#element-wise-arithmetic-operations",
    "href": "R-Basics.html#element-wise-arithmetic-operations",
    "title": "R Basics",
    "section": "5.3 Element-wise arithmetic operations",
    "text": "5.3 Element-wise arithmetic operations\nR is a vectorized language, which broadcasts operations to all elements in a vector. This behavior is also called element-wise operations, or broadcasting.\n\nIf you operate on a vector with a single number, the operation will be applied to all elements in the vector\n\n\n\nCode\nx &lt;- c(1, 3, 8, 7)\n\n\n\n\nCode\nx + 2\n\n\n[1]  3  5 10  9\n\n\n\n\nCode\nx * 2\n\n\n[1]  2  6 16 14\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a geometric sequence {2,4,8,16,32} using seq().",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#elementwise-relational-operations",
    "href": "R-Basics.html#elementwise-relational-operations",
    "title": "R Basics",
    "section": "5.4 Elementwise relational operations",
    "text": "5.4 Elementwise relational operations\n\nBesides arithmetic operations, we can also perform relational operations on vectors.\n\n\n\nCode\nx &lt;- c(1, 3, 8, 7)\nx &gt; 2\n\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n\n\nWe can also compare a vector with a vector, because R is vectorized\n\n\n\nCode\nincomeUCL &lt;- c(6000, 4600, 7000, 9100, 10000)\nincomeImperial &lt;- c(5000, 4500, 6000, 9000, 10000)\nincomeUCL &gt; incomeImperial\n\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#special-relational-operation-in",
    "href": "R-Basics.html#special-relational-operation-in",
    "title": "R Basics",
    "section": "5.5 Special relational operation: %in%",
    "text": "5.5 Special relational operation: %in%\n\nA special relational operation is %in% in R, which tests whether an element exists in the object.\n\n\n\nCode\nx &lt;- c(1, 3, 8, 7)\n\n3 %in% x\n\n\n[1] TRUE\n\n\nCode\n2 %in% x\n\n\n[1] FALSE",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#after-class-exercise",
    "href": "R-Basics.html#after-class-exercise",
    "title": "R Basics",
    "section": "5.6 After-class exercise",
    "text": "5.6 After-class exercise\n\nCreate a vector of 10 numbers from 1 to 10, and extract the 2nd, 4th, and 6th elements.\nCreate a vector of 5 numbers from 1 to 5, and check if 3 is in the vector.\nNow the interest rate is 0.1, and you have 1000 pounds in your bank account. Calculate the amount in your bank account after 1 year, 2 years, and 3 years, respectively.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#matrices-creating-matrices",
    "href": "R-Basics.html#matrices-creating-matrices",
    "title": "R Basics",
    "section": "6.1 Matrices: creating matrices",
    "text": "6.1 Matrices: creating matrices\n\n6.1.1 Creating matrices: matrix()\n\nA matrix can be created using the command matrix()\n\nthe first argument is the vector to be converted into matrix\nthe second argument is the number of rows\nthe last argument is the number of cols\n\n\n\n\nCode\nmatrix(1:9, nrow = 3, ncol = 3)\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR by default inserts elements vertically by columns.\n\n\n\nR will fill in the matrix by column and discard the remaining extra elements once fully filled, with a warning message\n\n\n\nCode\nmatrix(1:9, nrow = 3, ncol = 2)\n\n\nWarning in matrix(1:9, nrow = 3, ncol = 2): data length [9] is not a\nsub-multiple or multiple of the number of columns [2]\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\n\n6.1.2 Creating matrices: inserting by row\nHowever, we can ask R to insert by rows by setting the byrow argument.\n\n\nCode\nmatrix(1:9, nrow = 3, ncol = 3, byrow = TRUE)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\n6.1.3 Creating matrices: concatenate matrices cbind() and rbind()\nWe can use cbind() and rbind() to concatenate vectors and matrices into new matrices.\n\ncbind() does the column binding\n\n\n\nCode\na &lt;- matrix(1:6, nrow = 2, ncol = 3)\n\na\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nCode\ncbind(a, a) # column bind\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    3    5    1    3    5\n[2,]    2    4    6    2    4    6\n\n\n\nrbind() does the row binding\n\n\n\nCode\nrbind(a, a) # row bind\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n[3,]    1    3    5\n[4,]    2    4    6",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#matrices-indexing-and-subsetting",
    "href": "R-Basics.html#matrices-indexing-and-subsetting",
    "title": "R Basics",
    "section": "6.2 Matrices: indexing and subsetting",
    "text": "6.2 Matrices: indexing and subsetting\nMatrices have two dimensions: rows and columns. Therefore, to extract elements from a matrix, we just need to specify which row(s) and which column(s) we want.\n\n\nCode\nx &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\nExtract the element in the 2nd row, 3rd column.\n\nuse square bracket with a coma inside [ , ] to indicate subsetting; the argument before coma is the row index, and the argument after the coma is the column index.\n\n2 is specified for row index, so we will extract elements from the first row\n3 is specified for column index, so we will extract elements from the the second column\nAltogether, we extract a single element in row 2, column 3.\n\n\n\n\n\nCode\nx[2, 3] # the element in the 2nd row, 3rd column\n\n\n[1] 8\n\n\n\nIf we leave blank for a dimension, we extract all elements along that dimension.\n\nif we want to take out the entire first row\n\n1 is specified for the row index\ncolumn index is blank\n\n\n\n\n\nCode\nx[1, ] # all elements in the first row\n\n\n[1] 1 4 7\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExtract all elements in the second column\nExtract all elements in the first and third rows",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#matrices-operations",
    "href": "R-Basics.html#matrices-operations",
    "title": "R Basics",
    "section": "6.3 Matrices: operations",
    "text": "6.3 Matrices: operations\n\n6.3.1 Apply a math function to a matrix\nLet’s use 3 matrices x, y, and z:\n\n\nCode\nx &lt;- matrix(1:6, nrow = 3)\ny &lt;- matrix(1:6, byrow = T, nrow = 2)\nx\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nCode\ny\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\n\nFunctions will be vectorized over all elements in a matrix\n\n\n\nCode\nz &lt;- x^2\nz\n\n\n     [,1] [,2]\n[1,]    1   16\n[2,]    4   25\n[3,]    9   36\n\n\n\n\n6.3.2 Matrices’ operations: matrix addition and multiplication\n\nIf the two matrices are of the same dimensions, they can do element-wise operations, including element-wise addition and element-wise multiplication\n\n\n\nCode\nx + z # elementwise addition\n\n\n     [,1] [,2]\n[1,]    2   20\n[2,]    6   30\n[3,]   12   42\n\n\n\n\nCode\nx * x\n\n\n     [,1] [,2]\n[1,]    1   16\n[2,]    4   25\n[3,]    9   36\n\n\n\nIf we want to perform the matrix multiplication as in linear algebra, we need to use %*%\n\nx and y must have conforming dimensions\n\n\n\n\nCode\nx\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nCode\ny\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\nCode\nx %*% y # matrix multiplication\n\n\n     [,1] [,2] [,3]\n[1,]   17   22   27\n[2,]   22   29   36\n[3,]   27   36   45\n\n\n\n\n6.3.3 Matrices’ operations: inverse and transpose\n\nWe use t() to do matrix transpose\n\n\n\nCode\nx\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nCode\nt(x) # transpose\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\n\nWe use solve() to get the inverse of an matrix\n\n\n\nCode\nx\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nCode\nsolve(t(x) %*% x) # inverse; must be on a square matrix\n\n\n           [,1]       [,2]\n[1,]  1.4259259 -0.5925926\n[2,] -0.5925926  0.2592593",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#data-frames-creating-data.frame",
    "href": "R-Basics.html#data-frames-creating-data.frame",
    "title": "R Basics",
    "section": "7.1 Data Frames: creating data.frame",
    "text": "7.1 Data Frames: creating data.frame\n\n7.1.1 Data Frames: create dataframe using data.frame()\n\nYou can think of data.frame as a spreadsheet in excel.\n\n\n\nCode\ndf &lt;- data.frame(\n    id = 1:4,\n    name = c(\"David\", \"Karima\", \"Anil\", \"Wei\"),\n    wage = rnorm(n = 4, mean = 10^5, sd = 10^3),\n    male = c(T, T, T, T)\n)\ndf\n\n\n\n  \n\n\n\n\nData frames can also be created from external sources, e.g., from a csv file or database.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#data-frames-basics",
    "href": "R-Basics.html#data-frames-basics",
    "title": "R Basics",
    "section": "7.2 Data Frames: Basics",
    "text": "7.2 Data Frames: Basics\n\nEach row stands for an observation; each column stands for a variable.\nEach variable should have a unique name.\nEach column must contain the same data type, but the different columns can store different data types.\n\ncompare with matrix?\n\nEach column must be of same length, because rows have the same length across variables.",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#data-frames-check-dimensions-and-variable-types",
    "href": "R-Basics.html#data-frames-check-dimensions-and-variable-types",
    "title": "R Basics",
    "section": "7.3 Data Frames: check dimensions and variable types",
    "text": "7.3 Data Frames: check dimensions and variable types\n\nYou can verify the size of the data.frame using the command dim(); or nrow() and ncol()\n\n\n\nCode\ndim(df)\n\n\n[1] 4 4\n\n\nCode\nnrow(df)\n\n\n[1] 4\n\n\nCode\nncol(df)\n\n\n[1] 4\n\n\n\nYou can get the data type info using the command str()\n\n\n\nCode\nstr(df)\n\n\n'data.frame':   4 obs. of  4 variables:\n $ id  : int  1 2 3 4\n $ name: chr  \"David\" \"Karima\" \"Anil\" \"Wei\"\n $ wage: num  100023 98682 100462 100024\n $ male: logi  TRUE TRUE TRUE TRUE\n\n\n\nGet the variables names of the data frame\n\n\n\nCode\nnames(df)\n\n\n[1] \"id\"   \"name\" \"wage\" \"male\"",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#arrays",
    "href": "R-Basics.html#arrays",
    "title": "R Basics",
    "section": "8.1 Arrays",
    "text": "8.1 Arrays\n\nWe can use array() to generate a high-dimensional array\nJust like vectors and matrices, arrays can include only data types of the same kind.\nA 3D array is basically a combination of matrices each laid on top of other\n\n\n\nCode\nx &lt;- 1:4\nx &lt;- array(data = x, dim = c(2, 3, 2))\nx\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    1\n[2,]    2    4    2\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    3    1    3\n[2,]    4    2    4",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#lists",
    "href": "R-Basics.html#lists",
    "title": "R Basics",
    "section": "8.2 Lists",
    "text": "8.2 Lists\nA list is an R object that can contain anything. List is pretty useful when you need to store objects for latter use.\n\n\nCode\nx &lt;- 1:2\ny &lt;- c(\"a\", \"b\")\nL &lt;- list(numbers = x, letters = y)",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#lists-indexing-and-subsetting",
    "href": "R-Basics.html#lists-indexing-and-subsetting",
    "title": "R Basics",
    "section": "8.3 Lists: indexing and subsetting",
    "text": "8.3 Lists: indexing and subsetting\nThere are many ways to extract a certain element from a list.\n\nby index\nby the name of the element\nby dollar sign $\n\n\n\nCode\nL[[1]] # extract the first element\n\n\n[1] 1 2\n\n\nCode\nL[[\"numbers\"]] # based on element name\n\n\n[1] 1 2\n\n\nCode\nL$numbers # extract the element called numbers\n\n\n[1] 1 2\n\n\nAfter extracting the element, we can work on the element further:\n\n\nCode\nL$numbers[1:3] &gt; 2\n\n\n[1] FALSE FALSE    NA",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#ifelse",
    "href": "R-Basics.html#ifelse",
    "title": "R Basics",
    "section": "9.1 if/else",
    "text": "9.1 if/else\nSometimes, you want to run your code based on different conditions. For instance, if the observation is a missing value, then use the population average to impute the missing value. This is where if/else kicks in.\nif (condition == TRUE) {\n  action 1\n} else if (condition == TRUE ){\n  action 2\n} else {\n  action 3\n}\nExample 1:\n\n\nCode\na &lt;- 15\n\nif (a &gt; 10) {\n    larger_than_10 &lt;- TRUE\n} else {\n    larger_than_10 &lt;- FALSE\n}\n\nlarger_than_10\n\n\n[1] TRUE\n\n\nExample 2:\n\n\nCode\nx &lt;- -5\nif (x &gt; 0) {\n    print(\"x is a non-negative number\")\n} else {\n    print(\"x is a negative number\")\n}\n\n\n[1] \"x is a negative number\"",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#loops",
    "href": "R-Basics.html#loops",
    "title": "R Basics",
    "section": "9.2 Loops",
    "text": "9.2 Loops\nAs the name suggests, in a loop the program repeats a set of instructions many times, until the stopping criteria is met.\nLoop is very useful for repetitive jobs.\n\n\nCode\nfor (i in 1:10) { # i is the iterator\n    # loop body: gets executed each time\n    # the value of i changes with each iteration\n}",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#nested-loops",
    "href": "R-Basics.html#nested-loops",
    "title": "R Basics",
    "section": "9.3 Nested loops",
    "text": "9.3 Nested loops\nWe can also nest loops inside other loops.\n\n\nCode\nx &lt;- cbind(1:3, 4:6) # column bind\nx\n\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nCode\ny &lt;- cbind(7:9, 10:12) # row bind\ny\n\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n\nCode\nz &lt;- x\n\nfor (i in 1:nrow(x)) {\n    for (j in 1:ncol(x)) {\n        z[i, j] &lt;- x[i, j] + y[i, j]\n    }\n}\n\nz\n\n\n     [,1] [,2]\n[1,]    8   14\n[2,]   10   16\n[3,]   12   18",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#user-defined-functions",
    "href": "R-Basics.html#user-defined-functions",
    "title": "R Basics",
    "section": "9.4 User-Defined Functions",
    "text": "9.4 User-Defined Functions\nA function takes the argument as input, run some specified actions, and then return the result to us.\nFunctions are very useful. When we would like to test different ideas, we can combine functions with loops: We can write a function which takes different parameters as input, and we can use a loop to go through all the possible combinations of parameters.\n\n9.4.1 User-defined function syntax\nHere is how to define a function in general:\n\n\nCode\nfunction_name &lt;- function(arg1, arg2 = default_value) {\n    # write the actions to be done with arg1 and arg2\n    # you can have any number of arguments, with or without defaults\n    return() # the last line is to return some value\n}\n\n\nExample:\n\n\nCode\nmagic &lt;- function(x, y) {\n    results &lt;- x^2 + y\n\n    return(results)\n}\n\nmagic(2, 3)\n\n\n[1] 7\n\n\n\n\n9.4.2 Arguments\n\nDefault values: UDF can have default values for arguments by using arg = default_value. If the user does not provide a value for the argument when calling the UDF, the default value will be used.\n\n\n\nCode\nmagic &lt;- function(x, y = 1) {\n    results &lt;- x^2 + y\n\n    return(results)\n}\n\nmagic(2)\n\n\n[1] 5\n\n\n\nMissing values: If the user does not provide a value for an argument without a default value, R will throw an error.\n\n\n\nCode\nmagic &lt;- function(x, y) {\n    results &lt;- x^2 + y\n\n    return(results)\n}\n\nmagic(2)\n\n\nError in magic(2): argument \"y\" is missing, with no default\n\n\n\nOrder of arguments: The order of arguments matters when calling the UDF. If you want to provide a value for the second argument, you must provide a value for the first argument as well.\n\n\n\nCode\nmagic &lt;- function(y, x) {\n    results &lt;- x^2 + y\n\n    return(results)\n}\n\nmagic(3, 2)\n\n\n[1] 7\n\n\n\n\n9.4.3 Returned Value\n\nWe can return a value from a function using the return() function. The value returned can be of any data type.\n\n\n\nCode\nmagic &lt;- function(x, y) {\n    result &lt;- x^2 + y\n\n    return(result)\n}\n\nmagic(2, 3)\n\n\n[1] 7\n\n\n\nIf the function does not have a return() statement, it will return the last value calculated in the function.\n\n\n\nCode\nmagic &lt;- function(x, y) {\n    x + y\n}\n\nmagic(2, 3)\n\n\n[1] 5",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#variable-scope",
    "href": "R-Basics.html#variable-scope",
    "title": "R Basics",
    "section": "9.5 Variable Scope",
    "text": "9.5 Variable Scope\n\nVariables created inside a function are local to the function, and cannot be accessed outside the function.\n\n\n\nCode\nmagic &lt;- function(x, y) {\n    result &lt;- x^2 + y\n\n    return(result)\n}\n\nresult\n\n\nError in eval(expr, envir, enclos): object 'result' not found",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#a-comprehensive-example",
    "href": "R-Basics.html#a-comprehensive-example",
    "title": "R Basics",
    "section": "9.6 A comprehensive example",
    "text": "9.6 A comprehensive example\nTask: write a function, which takes a vector as input, and returns the max value of the vector\n\n\nCode\nget_max &lt;- function(input) {\n    max_value &lt;- input[1]\n    for (i in 2:length(input)) {\n        if (input[i] &gt; max_value) {\n            max_value &lt;- input[i]\n        }\n    }\n\n    return(max_value)\n}\n\nget_max(c(-1, 3, 2))\n\n\n[1] 3\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWrite your own version of which.max() function",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Basics.html#footnotes",
    "href": "R-Basics.html#footnotes",
    "title": "R Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTIOBE Programming Community index is a measure of programming language popularity.↩︎\nThere are many R-exclusive packages, such as the state-of-the-art causal machine learning library grf , which we will learn in the final week.↩︎\nWhy the name Quarto? “We wanted to use a name that had meaning in the history of publishing and landed on Quarto, which is the format of a book or pamphlet produced from full sheets printed with eight pages of text, four to a side, then folded twice to produce four leaves. The earliest known European printed book is a Quarto, the Sibyllenbuch, believed to have been printed by Johannes Gutenberg in 1452–53.”↩︎\nYou can also use equal sign =, but it’s recommended to stick with R’s tradition.↩︎",
    "crumbs": [
      "R Tutorials",
      "R Basics (Induction Week)"
    ]
  },
  {
    "objectID": "R-Exercise.html",
    "href": "R-Exercise.html",
    "title": "Weekly R Exercise",
    "section": "",
    "text": "1 Induction Week\n\nQuestion 1Answer\n\n\nCreate a sequence of {1,1,2,2,3,3,3}.\n\n\n\n\nCode\n# solution 1\n\nc(1, 1, 2, 2, 3, 3, 3)\n\n\n[1] 1 1 2 2 3 3 3\n\n\nCode\n# solution 2\n\nc(rep(1, 2), rep(2, 2), rep(3, 3))\n\n\n[1] 1 1 2 2 3 3 3\n\n\n\n\n\n\nQuestion 2Answer\n\n\nCreate a geometric sequence {2,4,8,16,32} using seq().\n\n\nWe can see that the sequence is a geometric sequence with a common ratio of 2.\n\nThe seq() function generates a sequence from 1 to 5 with a step of 1.\n\n\n\nCode\n# solution\n\nseq(1, 5, 1) # this generates a sequence from 1 to 5 with a step of 1\n\n\n[1] 1 2 3 4 5\n\n\n\nThe ^ operator calculates the power of 2 raised to the power of the sequence generated by seq(). Remember that R is vectorized, so the ^ operator will apply to each element in the sequence.\n\n\n\nCode\n# solution\n\n2^seq(1, 5, 1) # this generates a geometric sequence {2^1, 2^2, 2^3, 2^4, 2^5}\n\n\n[1]  2  4  8 16 32\n\n\n\n\n\n\nQuestion 3Answer\n\n\nCreate a vector of 10 numbers from 1 to 10, and extract the 2nd, 4th, and 6th elements.\n\n\n\n\nCode\n# solution\n\nx &lt;- 1:10 # create a vector of 10 numbers from 1 to 10\n\nx[c(2, 4, 6)] # use [] to extract the 2nd, 4th, and 6th elements\n\n\n[1] 2 4 6\n\n\n\n\n\n\nQuestion 4Answer\n\n\nCreate a vector of 5 numbers from 1 to 5, and check if 3 is in the vector.\n\n\n\n\nCode\n# solution\n\nx &lt;- 1:5 # create a vector of 5 numbers from 1 to 5\n\n3 %in% x # check if 3 is in the vector\n\n\n[1] TRUE\n\n\n\n\n\n\nQuestion 5Answer\n\n\nNow the interest rate is 0.1, and you have 1000 pounds in your bank account. Calculate the amount in your bank account after 1 year, 2 years, and 3 years, respectively.\n\n\nFirst, set the interest rate to 0.1 and the initial amount to 1000.\nThen, calculate the amount in your bank account after 1 year, 2 years, and 3 years, respectively.\nSince the interest is compounded annually, the formula is:\n[ A = P(1 + r)^n ]\nwhere:\n\n(A) is the amount in your bank account after (n) years\n(P) is the initial amount\n(r) is the interest rate\n(n) is the number of years\n\n\n\nCode\n# solution\n\ninterest_rate &lt;- 0.1 # set the interest rate to 0.1\n\ninitial_amount &lt;- 1000 # set the initial amount to 1000\n\n# calculate the amount in your bank account after 1 year, 2 years, and 3 years, respectively\n\n# generate the geometric sequence from 1 to 3 years\n\ninitial_amount * (1 + interest_rate)^(1:3) # use the formula A = P(1 + r)^n\n\n\n[1] 1100 1210 1331\n\n\n\n\n\n\n\n2 Week 1\n\nThe after-class exercise solutions are in the Week 1 case study",
    "crumbs": [
      "Lectures",
      "Weekly R Exercise"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html",
    "href": "Week2-Lecture2.html",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "",
    "text": "Apply CLV calculation in a real business scenario, where M&S is considering launching an online grocery delivery service\nUnderstand how CLV can be used by marketers to guide marketing decisions\n\n\n\n\n\nCompany\n\n\nM&S is a British multinational groceries and general merchandise retailer. It currently focuses on operating offline stores. The company is considering launching an online grocery delivery service to compete with other direct competitors such as Tesco, Sainsbury’s, and Asda.\n\n\nCustomer\n\n\nM&S serves a diverse customer base, including students, families, young professionals, and retirees.\n\n\nCompetitor\n\n\nM&S’s direct competitors include Sainsbury’s, Asda, and Morrisons. M&S also faces competition from discounters such as Aldi and Lidl.\n\n\nCollaborator\n\n\nSuppliers, delivery partners\n\n\nContext\n\n\nPESTLE example: Brexit, COVID-19, etc.\n\n\n\n\nAre you using any grocery store’s loyalty programs? Did you need to pay for the membership?\nWhy do you think the grocery chains offer these loyalty programs?",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#class-objectives",
    "href": "Week2-Lecture2.html#class-objectives",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "",
    "text": "Apply CLV calculation in a real business scenario, where M&S is considering launching an online grocery delivery service\nUnderstand how CLV can be used by marketers to guide marketing decisions",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#situation-analyses-for-ms",
    "href": "Week2-Lecture2.html#situation-analyses-for-ms",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "",
    "text": "Company\n\n\nM&S is a British multinational groceries and general merchandise retailer. It currently focuses on operating offline stores. The company is considering launching an online grocery delivery service to compete with other direct competitors such as Tesco, Sainsbury’s, and Asda.\n\n\nCustomer\n\n\nM&S serves a diverse customer base, including students, families, young professionals, and retirees.\n\n\nCompetitor\n\n\nM&S’s direct competitors include Sainsbury’s, Asda, and Morrisons. M&S also faces competition from discounters such as Aldi and Lidl.\n\n\nCollaborator\n\n\nSuppliers, delivery partners\n\n\nContext\n\n\nPESTLE example: Brexit, COVID-19, etc.",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#the-sparks-loyalty-program",
    "href": "Week2-Lecture2.html#the-sparks-loyalty-program",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "",
    "text": "Are you using any grocery store’s loyalty programs? Did you need to pay for the membership?\nWhy do you think the grocery chains offer these loyalty programs?",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#cac-roadmap",
    "href": "Week2-Lecture2.html#cac-roadmap",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "2.1 CAC: Roadmap",
    "text": "2.1 CAC: Roadmap\n\nWhat does the CAC include in the case study? What steps M&S needs to take to acquire one new customer?\n\n[…] (find info in the case study)\n-   total costs for customer ad clicks\n\n-   total costs of £10 promotion of free goods\n\n-   total costs of free deliveries",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#paid-search-advertising",
    "href": "Week2-Lecture2.html#paid-search-advertising",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "2.2 Paid Search Advertising",
    "text": "2.2 Paid Search Advertising\nSearch Engine Marketing (SEM) is a form of online marketing where businesses pay to display their ads on search engine results pages. This type of advertising is highly targeted and aims to capture users who are actively searching for products related to the advertiser’s offerings.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrands/companies (The universities)\nbid on keywords (MSc Business Analytics)\nAdvertisers (Google)\npaid search (bids-based with ads label)\norganic (quality-based)",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#marketing-funnel-for-paid-search-ads",
    "href": "Week2-Lecture2.html#marketing-funnel-for-paid-search-ads",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "2.3 Marketing Funnel for Paid Search Ads",
    "text": "2.3 Marketing Funnel for Paid Search Ads\nA marketing funnel is a model that represents the customer’s journey from the initial awareness of a product or service to the ultimate purchase. This journey is depicted as a funnel to illustrate the decrease in the number of potential customers at each stage.\n\n\n\n\n\n\n\n\n\nCAC Part I: Costs of paid search ads to get 1 new member.\n\n[…] about 10% of customers who click on an ad on search engine or social media will sign up for a free trial (i.e., triers); triers will on average shop twice during the trial period. 20% of those trial users will eventually become paying customers.\n\n\nWe think from the bottom up. To get 1 new member, how many triers do we need?\n\n\n\nCode\n# clicker_to_trier_rate is the % of trier customers from clickers\ntrier_to_member_rate &lt;-\n\n\n\nTo get 1 trier, how many clickers do we need?\n\n\n\nCode\n# trier_to_member_rate is the % of a new member from triers\nclicker_to_trier_rate &lt;-\n\n\n\nHow many clickers are needed to get 1 trier and eventually convert 1 new member?\n\n\n\nCode\nn_clickers_for_1newmember &lt;- (1 / clicker_to_trier_rate) * (1 / trier_to_member_rate)\n\n\n\nNow, let’s calculate the total cost of acquiring 1 new member by multiplying the number of clickers needed by the cost per click.\n\n\n\nCode\ntotal_cost_clicks &lt;- 0.4 * n_clickers_for_1newmember",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#cac-part-ii",
    "href": "Week2-Lecture2.html#cac-part-ii",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "2.4 CAC Part II",
    "text": "2.4 CAC Part II\nCAC Part II: total costs of £10 promo for first order each trier customer\n\nWhat is the total promo cost for these “trier” customers’ first order? These are free goods offered to customers in addition to their usual £100 shopping.\n\n\n\nCode\nprofit_margin &lt;- 0.07\npromo_first_order_each_trier &lt;- 10\n\ntotal_cost_promo &lt;- promo_first_order_each_trier * # promotion amount = £10\n    (1 - profit_margin) * # 7% profit margin\n    (1 / trier_to_member_rate) # num of triers = 5",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#compute-customer-acquisition-costs",
    "href": "Week2-Lecture2.html#compute-customer-acquisition-costs",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "2.5 Compute customer acquisition costs",
    "text": "2.5 Compute customer acquisition costs\nCAC Part III: total costs from selling groceries during the trial period\n\nFor each trier, compute the net profits and marketing costs from the two free visits.\n\n\n\nCode\nprofit_each_trier &lt;- revenue_each_visit * # £100 per visit\n    profit_margin * # 7% profit margin\n    2 # a trier shops twice\n\n\n\nFor each trier, the 2 visits are free of delivery charges, which are marketing costs to M&S\n\n\n\nCode\ndeliverycost_each_trier &lt;- 5 * 2\n\n\n\nFor each trier, compute net marketing costs from the 2 visits (marketing costs - earned profits)\n\n\n\nCode\nnetcost_each_trier &lt;- deliverycost_each_trier - profit_each_trier\n\n\n\nTotal net profits from all triers\n\n\n\nCode\ntotalcosts_from_all_triers &lt;- netcost_each_trier * (1 / trier_to_member_rate)",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-6-compute-customer-acquisition-costs",
    "href": "Week2-Lecture2.html#step-6-compute-customer-acquisition-costs",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "2.6 Step 6: Compute customer acquisition costs",
    "text": "2.6 Step 6: Compute customer acquisition costs\n\nCAC = total costs for customer ad clicks (for all clickers) + total costs of £10 promo (for all triers) + total costs of selling groceries (for all triers)\n\n\n\nCode\nCAC &lt;- total_cost_clicks + total_cost_promo + totalcosts_from_all_triers",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-1-determine-time-unit-of-analysis",
    "href": "Week2-Lecture2.html#step-1-determine-time-unit-of-analysis",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.1 Step 1: Determine time unit of analysis",
    "text": "3.1 Step 1: Determine time unit of analysis\n\nTime unit of analysis\n\n[…] (find info in the case study)\n\nWhen should we use monthly analysis or other units of time?",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-2-determine-number-of-years",
    "href": "Week2-Lecture2.html#step-2-determine-number-of-years",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.2 Step 2: Determine number of years",
    "text": "3.2 Step 2: Determine number of years\n\n\\(N\\): the number of years over which the customer relationship is assessed\n\n[…] (find info in the case study)\nHow can you do better here?\n\n\n\n\nCode\nN &lt;-",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-3-compute-g-for-each-period",
    "href": "Week2-Lecture2.html#step-3-compute-g-for-each-period",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.3 Step 3: Compute g for each period",
    "text": "3.3 Step 3: Compute g for each period\n\\(g = M - c\\): net profit each year; Remember, \\(M\\) is the gross profit from membership fees and grocery purchases, and \\(c\\) is the cost of delivering goods to customers.\n\nmost customers paid the £89 annual membership fee\n\n\n\nCode\nmembership &lt;-",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-3-compute-g-for-each-period-1",
    "href": "Week2-Lecture2.html#step-3-compute-g-for-each-period-1",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.4 Step 3: Compute g for each period",
    "text": "3.4 Step 3: Compute g for each period\n\n40 times each year; each time £100; with profit margin 7% (COGS 93%)\n\n\n\nCode\nn_visit &lt;-\nrevenue_each_visit &lt;-\nprofit_margin &lt;-\n\nM &lt;-\n\n\n\nVariable delivery costs each order. Find info in the case study about delivery costs\n\n\n\nCode\ndeliverycost_each_visit &lt;-\n\nc &lt;- deliverycost_each_visit * n_visit",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-3-compute-g-for-each-period-2",
    "href": "Week2-Lecture2.html#step-3-compute-g-for-each-period-2",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.5 Step 3: Compute g for each period",
    "text": "3.5 Step 3: Compute g for each period\n\nThe annual g from customers regular grocery shopping\n\n\n\nCode\n# CF is the cash flow for one year\n\ng &lt;-\n\n# create a sequence of CF for N years\n\ng_seq &lt;- rep(g, N)",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-4-compute-sequence-of-retention-rate",
    "href": "Week2-Lecture2.html#step-4-compute-sequence-of-retention-rate",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.6 Step 4: Compute sequence of retention rate",
    "text": "3.6 Step 4: Compute sequence of retention rate\n\n\\(r\\): retention rate\n\n\n[…] (find info in the case study)\n\n\n\nCode\n# retention_rate is the probability of customer staying with us after 1 year\nr &lt;-\n    # create a geometric sequence of accumulative retention rate for N years\nr_seq &lt;-",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-5-compute-sequence-of-discount-factors",
    "href": "Week2-Lecture2.html#step-5-compute-sequence-of-discount-factors",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.7 Step 5: Compute sequence of discount factors",
    "text": "3.7 Step 5: Compute sequence of discount factors\n\n\\(k\\): the discount rate\n\n\n[…] A yearly discount rate of 10%\n\n\n\nCode\nk &lt;- 0.1\nd &lt;- \nd_seq &lt;-\n\n\n\n[…] The team decided to take a conservative approach whereby all profits are booked at the end of year. Why is this a conservative approach?",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#step-6-compute-clv",
    "href": "Week2-Lecture2.html#step-6-compute-clv",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "3.8 Step 6: Compute CLV",
    "text": "3.8 Step 6: Compute CLV\n\nCompute the CLV based on the CLV formula (Table A)\n\n\ng for the next 5 years\n\n\n\nCode\ng_seq\n\n\n\nApply retention rate r\n\n\n\nCode\ng_seq_after_churn &lt;- g_seq *\n\n\n\nApply discount factor d\n\n\n\nCode\ng_seq_after_churn_discount &lt;- g_seq_after_churn *\n\n\n\nCompute CLV by summing up future expected profits\n\n\n\nCode\nCLV &lt;- sum(g_seq_after_churn_discount) - CAC",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#clv-as-a-key-management-tool",
    "href": "Week2-Lecture2.html#clv-as-a-key-management-tool",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "4.1 CLV as a Key Management Tool",
    "text": "4.1 CLV as a Key Management Tool\n\n\n\n\n\n\n\n\n\nWe can use CLV as the key managerial tool for evaluating different marketing initiatives!",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#scenario",
    "href": "Week2-Lecture2.html#scenario",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "4.2 Scenario",
    "text": "4.2 Scenario\n\nIf M&S decides to reduce the pass price to £79 per year, then the retention rate will increase to 75%\nWe can write down an R user-defined function to solve the question.",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture2.html#after-class",
    "href": "Week2-Lecture2.html#after-class",
    "title": "Class 4: (Case Study) CLV Analysis for M&S’s Delivery Pass",
    "section": "4.3 After-class",
    "text": "4.3 After-class\n\n(To guide customer acquisition) What if the company only offers £5 for first time purchase? This will save some CAC but the clicker-to-trier rate will decrease to 5%. Please compute the new CLV. Should you go ahead with the proposed change?\n(To guide customer retention) What if the company increases the annual membership fee to $119? This will increase revenue from memberships but will also make some customers unhappy so their retention rate reduce to 55%. Please compute the new CLV. Should you go ahead with the proposed change?",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 2: Case Study: CLV for M&S"
    ]
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Module leader: Dr. Wei Miao [pronounced as “Way Meow”] [personal website]\nEmail: wei.miao@ucl.ac.uk\nTeaching assistants for office hours\n\nDivyansh Agrawal, divyansh.agrawal.23@ucl.ac.uk (UCL MSc BA 2023)\nJiafan Lu, jiafan.lu.22@ucl.ac.uk (UCL PhD Management 2022)",
    "crumbs": [
      "Lectures",
      "Syllabus"
    ]
  },
  {
    "objectID": "Syllabus.html#if-you-have-questions-about-the-lecture-first-check-the-teams-channel",
    "href": "Syllabus.html#if-you-have-questions-about-the-lecture-first-check-the-teams-channel",
    "title": "Syllabus",
    "section": "4.1 If you have questions about the lecture, first check the Teams channel",
    "text": "4.1 If you have questions about the lecture, first check the Teams channel\nWe will use Microsoft Teams for the module. Please utilize the Microsoft Teams Q&A channels as an interactive place for peer learning.\nSince some of your questions may have been asked by your peers, for any questions, please first check the MS Teams channel and see if the questions are already posted and answered there. If not, please post your questions in the Teams channels. The teaching team will monitor the questions posted on Teams and provide answers accordingly.",
    "crumbs": [
      "Lectures",
      "Syllabus"
    ]
  },
  {
    "objectID": "Syllabus.html#if-you-have-more-questions-and-would-like-to-use-office-hours-make-an-appointment",
    "href": "Syllabus.html#if-you-have-more-questions-and-would-like-to-use-office-hours-make-an-appointment",
    "title": "Syllabus",
    "section": "4.2 If you have more questions and would like to use office hours, make an appointment",
    "text": "4.2 If you have more questions and would like to use office hours, make an appointment\nYou can also ask questions during office hours hosted by me or teaching assistants during term time.\nTo make an appointment, use the link below. The links are also available under “Module Overview” Section on Moodle. Note that you need to log in with your UCL email account to make an appointment.\n\nFor lecture-related questions, please make an appointment with Wei here.\nFor R trouble-shooting, please make an appointment with teaching assistants here",
    "crumbs": [
      "Lectures",
      "Syllabus"
    ]
  },
  {
    "objectID": "Syllabus.html#footnotes",
    "href": "Syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJust a little heads up -- unlike undergrad marketing modules where you mainly learn marketing strategies and concepts, this module focuses on “analytics” (which comes first) and its application in “marketing” (which comes next). So, be prepared to learn a lot of data wrangling, machine learning, and causal inference tools in this module.↩︎",
    "crumbs": [
      "Lectures",
      "Syllabus"
    ]
  },
  {
    "objectID": "Case-IV.html",
    "href": "Case-IV.html",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "",
    "text": "The sharing economy has been booming in recent years, leading to a rapid increase in jobs in the “gig” economy. According to Hossain (2020), in the US alone, the sharing economy sector has created 6.23 million jobs with 78 million service providers, and 800 million people engage with it. The transportation sector is one of the most salient beneficiaries of the burgeoning sharing economy. For instance, commuting to work by shared bicycle (e.g., Citi Bike) has become an increasingly popular transportation option (Ford et al. 2019). The ride-sharing service (e.g., Uber) allows drivers to enjoy more flexibility in work, which is proven valuable to drivers and has improved capacity utilization (Cramer and Krueger 2016).\nThe COVID-19 pandemic has brought unprecedented disruptions to many industries, and the transportation industry is among the most disrupted ones. Further, the COVID-19 has raised concerns about the survivability of the sharing economy in general. It is reported that gross bookings on Uber rides were down by 75% in the three months through June 2020, and that Lyft’s April ridership was down by 75% from April 2019. Some Uber drivers were extremely cautious about their shift decisions and taking measures to prevent COVID from spreading.\nUnlike the traditional taxi market, where taxi drivers rent vehicles from taxi companies and then directly provide transportation services to consumers, modern ride-sharing platforms typically serve as the matching intermediary between drivers and passengers. Due to such two-sided market nature, the profitability of modern ride-sharing platforms (and sharing economy in general) highly depends on the interdependence or externality between the two sides of economic agents (Rysman 2009). Therefore, a ride-sharing platform would benefit from the network effect if more drivers work for them. It is thus managerially important for the ride-sharing platform to understand whether COVID-19 has affected drivers’ labor supply patterns and if yes, the magnitude of the effect across drivers and over time.\nIn this case study, we will answer the above causal question using the instrumental variable method."
  },
  {
    "objectID": "Case-IV.html#driver-daily-trip-data",
    "href": "Case-IV.html#driver-daily-trip-data",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "2.1 Driver Daily Trip Data",
    "text": "2.1 Driver Daily Trip Data\nIn a ride-sharing company’s database, the raw trip log records each trip’s details, including the driver’s ID, the passenger’s ID, the booking date and time, the trip’s start and end locations, the trip’s distance, the trip’s fare, and the trip’s status (e.g., completed, cancelled, or passenger no shows). The data science team has aggregated the raw trip-level data into a driver-day level panel data.\nPanel data structure refers to a dataset that includes multiple observations over time for the same units (in our case, drivers). It combines cross-sectional data (observations at a single point in time) and time series data (observations of a single subject over multiple time periods), thus enabling analysis that captures both individual dynamics and temporal variations.\nOur first data set summarizes drivers’ daily shift each day in April 2020, right during the period when the pandemic began to spread in the UK. The data set consists of a random sample of around 4000 drivers across 3 UK cities (anonymized as g, s, and c) in 2020.\n\n\nRows: 93,467\nColumns: 7\n$ driver_id    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ booking_date &lt;chr&gt; \"2020-04-01\", \"2020-04-02\", \"2020-04-03\", \"2020-04-04\", \"…\n$ is_work      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ income       &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0…\n$ n_order      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 4, 5, 4, 3, 3, 4, 3, …\n$ avg_distance &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.00000…\n$ city         &lt;chr&gt; \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g\", \"g…"
  },
  {
    "objectID": "Case-IV.html#covid-19-data",
    "href": "Case-IV.html#covid-19-data",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "2.2 COVID-19 Data",
    "text": "2.2 COVID-19 Data\nTo measure the severity of COVID-19, the data science team collected daily number of new cases in each city from the government database.\n\n\nCode\ndata_covid &lt;- read.csv(\"https://www.dropbox.com/s/j5vs1egwl5j51f4/data_covid.csv?dl=1\")"
  },
  {
    "objectID": "Case-IV.html#data-wrangling",
    "href": "Case-IV.html#data-wrangling",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\nQuestion 1Answer\n\n\n\nJoin the two datasets using dplyr. Please observe the data structure of the two datasets and carefully think about how we should do the data join in this case. Explain your rationale.\n\n\n\n\n\nCode\ndata_driver &lt;- data_driver %&gt;%\n    left_join(data_covid, by = c(\"city\" = \"city\", \"booking_date\" = \"booking_date\"))\n\n\n\nWe use left_join() to join the two datasets. The left_join() function keeps all rows from the left data frame (data_driver) and only the rows from the right data frame (data_covid) that match based on the common columns (city and booking_date). In layman’s terms, we are adding the COVID-19 data for each city and each day to the driver data. Since the driver data is the main data frame, we use it as the left data frame.\nThe by argument specifies the columns to match on. In this case, we match the city and booking_date columns in both data frames. In other words, drivers in the same city and on the same day will have the corresponding COVID-19 data joined to their records.\nAfter the join, the data_driver data frame will contain the COVID-19 data for each city and each day, which can be used for further analysis.\n\n\n\nCode\ndata_driver %&gt;%\n    slice(1:10)"
  },
  {
    "objectID": "Case-IV.html#key-dependent-variables",
    "href": "Case-IV.html#key-dependent-variables",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "2.4 Key Dependent Variables",
    "text": "2.4 Key Dependent Variables\nTo facilitate the empirical analysis of drivers’ responses to COVID-19, the data science team has computed key outcome variables of interest for each driver, including both extensive margin (i.e., whether to work) and intensive margin (i.e., how much to work) of drivers’ labor supply.\n\nWhether or not to work on a day, a binary outcome variable which equals 1 if a driver has at least one ride request on the day and 0 otherwise. We can use this variable to measure drivers’ shift decision, i.e., willingness to work on a day, which proxies for the extensive margin of drivers’ labor supply. It is ambiguous ex-ante how the number of new cases affects a driver’s shift decision. On the one hand, more new cases may increase the risk of infection, which decease drivers’ expected wellbeing, and therefore discourage drivers from working on a specific day; on the other hand, fewer drivers on the street suggest less competition among drivers and therefore higher chances of getting a passenger and potentially higher hourly earnings, which may motivate drivers to work. It is important for the ride-sharing company to understand how the severity of COVID-19 affects drivers’ willingness to work, so that the company can adjust their stimulus plans for drivers accordingly.\nTotal number of completed orders, which contain three aspects of information which are of policy and managerial interest. First, the variable can proxy for the length of drivers’ daily labor supply. Conditional on working, if a driver decides to work for longer hours, then we expect the driver to have a larger number of requests/orders. Second, both variables contain information on consumer demand. We expect the total number of requests/orders to decrease if there is a lower demand for ride-sharing service from consumers due to the COVID-19 outbreak. Finally, both variables can measure the intensity of competition among drivers. Keeping the level of demand fixed, the total number of requests/orders would be larger when there are fewer drivers working on the day. Due to the complexity of information contained, ex-ante, it is not straightforward how the COVID-19 measures affect the total number of orders for individual drivers.\nEarnings. Earnings measure the driver’s income from providing ride-sharing services, which is highly correlated with the number of completed orders and total trip distance. It allows us to directly assess the impact of the COVID-19 on drivers’ financial wellbeing.\nAverage trip distance. In our empirical context, drivers cannot reject a booking request once being matched with a passenger, therefore, the trip distance is largely determined by passengers. Since passengers may be reluctant to take long distance trips during the pandemic, we expect a negative impact of the number of new cases on the average trip distance."
  },
  {
    "objectID": "Case-IV.html#ols-linear-regression",
    "href": "Case-IV.html#ols-linear-regression",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "3.1 OLS Linear Regression",
    "text": "3.1 OLS Linear Regression\nTo empirically investigate the causal impact of COVID-19 cases on driver behavior, we can first try linear regressions to regress the labor supply measures of driver \\(i\\), in city \\(j\\), on day \\(t\\) on the COVID-19 measure and other covariates as follows:\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha NewCases_{jt} + \\varepsilon_{ijt}\n\\tag{1}\\]\nwhere \\(LaborOutcome_{ijt}\\) is the dependent variable of interest, \\(NewCases_{jt}\\) is the daily new COVID-19 cases in city \\(j\\) on day \\(t\\), and \\(\\varepsilon_{ijt}\\) is the error term.\n\nQuestion 2Answer\n\n\nPlease run linear regressions based on the above equation, with the outcome being the aforementioned dependent variables and explanatory variable being new cases. Please report the results in a single table.\n\n\n\n\nCode\npacman::p_load(fixest, modelsummary)\n\n# run the 4 OLS regressions below\nOLS_is_work &lt;- feols(\n    fml = is_work ~ new_cases,\n    data = data_driver\n)\nOLS_income &lt;- feols(\n    fml = income ~ new_cases,\n    data = data_driver\n)\nOLS_n_order &lt;- feols(\n    fml = n_order ~ new_cases,\n    data = data_driver\n)\nOLS_avg_distance &lt;- feols(\n    fml = avg_distance ~ new_cases,\n    data = data_driver\n)\n\n# Use a list() to store the 4 OLS regression results into one R list, and then use modelsummary() to output the results in a single table\nlist(\n    \"Work\" = OLS_is_work,\n    \"shift income\" = OLS_income,\n    \"# orders\" = OLS_n_order,\n    \"avg distance\" = OLS_avg_distance\n) %&gt;%\n    modelsummary(\n        gof_map = c(\"nobs\", \"r.squared\"),\n        stars = TRUE\n    )\n\n\n\n\nTable 1: OLS Regression Results\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Work\n                shift income\n                # orders\n                avg distance\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.182***\n                  5.957*** \n                  0.885***\n                  1.656***\n                \n                \n                             \n                  (0.001) \n                  (0.071)  \n                  (0.010) \n                  (0.017) \n                \n                \n                  new_cases  \n                  0.000   \n                  -0.122***\n                  -0.012* \n                  -0.003  \n                \n                \n                             \n                  (0.001) \n                  (0.036)  \n                  (0.005) \n                  (0.009) \n                \n                \n                  Num.Obs.   \n                  93467   \n                  93467    \n                  93467   \n                  93467   \n                \n                \n                  R2         \n                  0.000   \n                  0.000    \n                  0.000   \n                  0.000"
  },
  {
    "objectID": "Case-IV.html#fixed-effect-ols-regressions",
    "href": "Case-IV.html#fixed-effect-ols-regressions",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "3.2 Fixed Effect OLS Regressions",
    "text": "3.2 Fixed Effect OLS Regressions\nWhat confounding factors do we need to control in the above OLS regressions to mitigate the omitted variable bias?\nWe first need to include driver fixed effects to control for driver-specific characteristics that may affect drivers’ labor supply patterns. Such characteristics include, but are not limited to, the driver’s socio-demographic characteristics (e.g., gender and age), the driver’s degree of risk aversion, whether a driver is driving full-time or part-time, and the driver’s innate abilities to search for passengers, etc.\nFor instance, less risk-averse drivers may prefer to work on days when there are more new cases because they expect less competition from peer drivers and potentially higher profitability on such days. Another example is that, full-time drivers can be more subject to the impact of new cases compared to part-time drivers, because full-time drivers’ income largely comes from providing ride-sharing services via the focal company. Driver fixed effects can mitigate such driver-specific time-invariant confounding effects and help us obtain more accurate estimates for our focal explanatory variable NewCases.\nIn addition to driver fixed effects that remove cross-sectional confounding effects across drivers, we also include time fixed effects in Equation (1) to mitigate the inter-temporal confounding effects. We consider time fixed effects at the day level.\nMoreover, given that the local government in each city may have enacted different policies on fighting COVID-19 and/or stimulating economy (e.g., subsidizing drivers) during our data period, we further control for city fixed effects.\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha NewCases_{jt} + DriverFE + DayFE + CityFE + \\varepsilon_{ijt}\n\\]\n\nQuestion 3Answer\n\n\nRun the fixed effect regressions for the dependent variables. Please report the results in a single table.\n\n\n\n\nCode\nFE_is_work &lt;- feols(\n    fml = is_work ~ new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\nFE_income &lt;- feols(\n    fml = income ~ new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\nFE_n_order &lt;- feols(\n    fml = n_order ~ new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\nFE_avg_distance &lt;- feols(\n    fml = avg_distance ~ new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\n\n\nlist(\n    \"Work\" = FE_is_work,\n    \"shift income\" = FE_income,\n    \"# orders\" = FE_n_order,\n    \"avg distance\" = FE_avg_distance\n) %&gt;%\n    modelsummary(\n        stars = TRUE,\n        gof_map = \"nobs\"\n    )\n\n\n\n\nTable 2: Fixed Effects Regression\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Work\n                shift income\n                # orders\n                avg distance\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  new_cases\n                  0.000  \n                  0.029  \n                  0.003  \n                  0.002  \n                \n                \n                           \n                  (0.000)\n                  (0.022)\n                  (0.003)\n                  (0.008)\n                \n                \n                  Num.Obs. \n                  93467  \n                  93467  \n                  93467  \n                  93467"
  },
  {
    "objectID": "Case-IV.html#potential-endogeneity",
    "href": "Case-IV.html#potential-endogeneity",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "4.1 Potential Endogeneity",
    "text": "4.1 Potential Endogeneity\nAfter including the driver, city, date fixed effects fixed effects in the above regression, the remaining challenge to obtaining causal inference is the potential reverse causality of NewCases.\nEquation (1) could be subject to simultaneity issues because drivers’ labor supply decisions and number of new cases may be interdependent. On the one hand, drivers may adjust their labor supply accordingly to the number of new cases. On the other hand, prior research has demonstrated the potential effect of mobility on the COVID-19 case growth rate. If a city has a higher volume of private transportation through ride-sharing services, given the highly contagious nature of COVID-19, the city may have a higher number of new cases."
  },
  {
    "objectID": "Case-IV.html#instrumental-variables",
    "href": "Case-IV.html#instrumental-variables",
    "title": "Estimating Causal Effects for Ride-sharing Platforms with Instrumental Variables: A Case Study",
    "section": "4.2 Instrumental Variables",
    "text": "4.2 Instrumental Variables\nTo tackle the potential endogeneity issue, we use the instrumental variable (IV) method, leveraging exogenous sources of variation in the explanatory variable that are uncorrelated with the error term in Equation (1) using two-stage least squares (2SLS). We can potentially select two instrumental variables.\nThe first instrumental variable is imported new cases, which measures the number of infected travelers from overseas in each city as disclosed by local government. Because the imported cases relate to travelers from overseas, it should be exogenous to local confirmed cases and meet the exogeneity requirement.\nThe second instrumental variable is other city new cases, which is the number of new cases confirmed in neighboring cities. Since confirmed cases in other cities should not directly affect the focal city’s ride-sharing market, the variable other city new cases should also satisfy the exogeneity requirement.\nThe first-stage regression is specified below in Equation Equation 2, where the definitions of variables are the same as in Equation Equation 1:\n\\[\nNewCases_{jt} = \\pi_0 + \\pi_1 OtherCityNewCases_{jt} + DriverFE + DayFE + CityFE + \\varepsilon_{ijt}\n\\tag{2}\\]\n\nQuestion 4Answer\n\n\nRun the first stage regression and report the results.\n\n\n\n\nCode\n# Run first stage regression: new_cases ~ other_city_new_cases + controls\nIV_is_work_1ststage &lt;- feols(\n    fml = new_cases ~ other_city_new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\n\n# mutate predicted new_cases in data_driver\ndata_driver &lt;- data_driver %&gt;%\n    mutate(predicted_new_cases = predict(IV_is_work_1ststage))\n\n\nFrom the first stage result, we observe that the coefficient of other_city_new_cases is significantly different from zero, indicating that the instrumental variable is relevant. Therefore, we have confirmed that the relevance condition is satisfied.\n\n\n\nIn the second stage regression, we regress the outcome variables on the predicted new cases from the 1st stage, controlling for the same set of control variables:\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha \\hat{NewCases}_{jt} + DriverFE + DayFE + CityFE + \\varepsilon_{ijt}\n\\]\n\nQuestion 5Answer\n\n\nRun the second stage regression and report the results.\n\n\n\n\nCode\n# Run second stage regression: is_work ~ predicted_new_cases + controls\nIV_is_work_2ndstage &lt;- feols(\n    fml = is_work ~ predicted_new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\n\nmodelsummary(list(IV_is_work_1ststage, IV_is_work_2ndstage),\n    stars = TRUE\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  other_city_new_cases\n                  -0.466***    \n                               \n                \n                \n                                      \n                  (0.012)      \n                               \n                \n                \n                  predicted_new_cases \n                               \n                  0.000        \n                \n                \n                                      \n                               \n                  (0.001)      \n                \n                \n                  Num.Obs.            \n                  93467        \n                  93467        \n                \n                \n                  R2                  \n                  0.491        \n                  0.662        \n                \n                \n                  R2 Adj.             \n                  0.473        \n                  0.650        \n                \n                \n                  R2 Within           \n                  0.225        \n                  0.000        \n                \n                \n                  R2 Within Adj.      \n                  0.225        \n                  0.000        \n                \n                \n                  AIC                 \n                  328117.6     \n                  -7593.9      \n                \n                \n                  BIC                 \n                  358852.8     \n                  23141.3      \n                \n                \n                  RMSE                \n                  1.35         \n                  0.22         \n                \n                \n                  Std.Errors          \n                  by: driver_id\n                  by: driver_id\n                \n                \n                  FE: driver_id       \n                  X            \n                  X            \n                \n                \n                  FE: booking_date    \n                  X            \n                  X            \n                \n                \n                  FE: city            \n                  X            \n                  X            \n                \n        \n      \n    \n\n\n\n\n\nCode\n# We can also run the second stage regressions for other dependent variables\nIV_income_2ndstage &lt;- feols(\n    fml = income ~ predicted_new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\n\nIV_n_order_2ndstage &lt;- feols(\n    fml = n_order ~ predicted_new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\n\nIV_avg_distance_2ndstage &lt;- feols(\n    fml = avg_distance ~ predicted_new_cases |\n        driver_id + booking_date + city,\n    data = data_driver\n)\n\nlist(\n    \"Work\" = IV_is_work_2ndstage,\n    \"shift income\" = IV_income_2ndstage,\n    \"# orders\" = IV_n_order_2ndstage,\n    \"avg distance\" = IV_avg_distance_2ndstage\n) %&gt;%\n    modelsummary(\n        stars = TRUE,\n        gof_map = \"nobs\"\n    )\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Work\n                shift income\n                # orders\n                avg distance\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  predicted_new_cases\n                  0.000  \n                  0.122* \n                  0.019**\n                  0.023  \n                \n                \n                                     \n                  (0.001)\n                  (0.052)\n                  (0.007)\n                  (0.015)\n                \n                \n                  Num.Obs.           \n                  93467  \n                  93467  \n                  93467  \n                  93467  \n                \n        \n      \n    \n\n\n\nThe instrumental variable analyses reveal that the causal effect of new cases on shift decisions is not different from zero, suggesting that the number of new cases does not significantly affect drivers’ willingness to work. However, the number of new cases has a significant positve effect on the number of orders and a significant positive effect on shift income, indicating that the number of new cases increases the drivers’ earnings during the sample period."
  },
  {
    "objectID": "R-InstallR.html",
    "href": "R-InstallR.html",
    "title": "Install and Setup R",
    "section": "",
    "text": "Uninstall Old R and RStudio\n\n\n\nIf you have used R or RStudio before, please uninstall both R and RStudio and follow the guide below to install the latest versions. Otherwise, Quarto may not work properly on older versions.\nIf you switch to a new laptop later on, please come back to this tutorial and reinstall R and RStudio following the same procedures.",
    "crumbs": [
      "R Tutorials",
      "Install and Setup R"
    ]
  },
  {
    "objectID": "R-InstallR.html#for-windows-computers",
    "href": "R-InstallR.html#for-windows-computers",
    "title": "Install and Setup R",
    "section": "1.1 For Windows computers",
    "text": "1.1 For Windows computers\n\nGo to R’s official website in this link\nClick CRAN under Download section\n\n\n\n\n\n\n\nThese are different mirrors for R download. Basically they store the same installation files but on different servers in different places. Simply click into any mirror.\n\n\n\n\n\n\n\nClick into download R for Windows for installation files for Windows computers\n\n\n\n\n\n\n\nDownload and install (1) base and (2) Rtools. It’s recommended to use the default options during the installations.\n\nNotes: The former is the R program, and the latter is the tool to compile R packages.\nIt’s highly recommended to change your system language to English before proceeding, or there could be weird bugs later on.\n\n\n\n\nClick into this link. Download and instsall Quarto CLI plugin.",
    "crumbs": [
      "R Tutorials",
      "Install and Setup R"
    ]
  },
  {
    "objectID": "R-InstallR.html#for-mac-computers",
    "href": "R-InstallR.html#for-mac-computers",
    "title": "Install and Setup R",
    "section": "1.2 For Mac computers",
    "text": "1.2 For Mac computers\n\nGo to R’s official website in this link\nClick CRAN under download section\n\n\n\n\n\n\n\nThese are different mirrors for R download. Basically they store the same installation files but on different servers in different places. Simply click into any mirror.\n\n\n\n\n\n\n\nClick into download R for macOS for the download file\n\n\n\n\n\n\n\nDownload the correct pkg file to install R\n\nif you use Intel based CPU, download the R-4.X.X-x86_64.pkg under “For older Intel Macs”, where X.X.X is the version number.\nif you use Apple’s silicon chip such as M1, M1 pro, or M2, download the R-4.X.X-arm64.pkg under “For Apple Silicon Macs”, where X.X.X is the version number.\nrefer to this link if you don’t know how to check Intel or Apple CPU\n\n\n\n\n\n\n\n\nInstall Command Line Tools following the steps below. This is essential for R to be able to compile packages so do not skip this step.\n\nOpen terminal app on your mac (the icon is in the screenshot)\n\n\nType the following code xcode-select --install into terminal and hit enter to run the code. Admin passwords may be required to proceed. Note that when you type the password, you won’t see any characters on the screen, but the passwords have been entered.\n\n\nThe code may run for a few minutes. This step is to install MacOS tools that can help compile R packages. - If your terminal says “xcode-select: error: command line tools are already installed, use”Software Update” to install updates”. It means the needed tool is already installed on your computer. Then there is nothing needed in this step.\n\n\n\nClick into this link. Download and install Quarto CLI plugin for Mac OS.",
    "crumbs": [
      "R Tutorials",
      "Install and Setup R"
    ]
  },
  {
    "objectID": "R-InstallR.html#check-r-and-rstudio-are-properly-installed",
    "href": "R-InstallR.html#check-r-and-rstudio-are-properly-installed",
    "title": "Install and Setup R",
    "section": "4.1 Check R and RStudio are properly installed",
    "text": "4.1 Check R and RStudio are properly installed\nPlease follow the following steps to make sure you have successfully installed R and RStudio.\n\nStep 1: Launch RStudio from your computer. Check if you see the following screen without any error messages\n\n\n\nStep 2: As shown in the picture, type 1+1 behind the &gt; and hit enter, check if you see a 2 output\n\nIf there are no error messages and you see exactly the same output, then congrats, you have successfully installed R and RStudio!",
    "crumbs": [
      "R Tutorials",
      "Install and Setup R"
    ]
  },
  {
    "objectID": "R-InstallR.html#check-quarto-is-properly-installed",
    "href": "R-InstallR.html#check-quarto-is-properly-installed",
    "title": "Install and Setup R",
    "section": "4.2 Check Quarto is properly installed",
    "text": "4.2 Check Quarto is properly installed\n\nStep 1: Click the green plus sign circled below, and select Quarto Document\n\n\n\nStep 2: Select word and click Create.\n\n\n\nStep 3: Click the Render button. You may be asked to save the qmd file to a location. Save it to your download folder. R will then render the qmd document and generate a docx file, named “untitled.docx” in the same location.\n\n\n\nStep 4: The docx file should look like below. If you can generate the docx file without issues, Quarto has successfully run on your computer!\n\n\nNote that if you see a banner prompting that rmarkdown is not installed, you need click the Install button to install it. In the future, if you ever see a similar banner that prompts “Package XXX required but is not installed”, you should click the Install button to install the missing package, because this package is needed for the current Quarto file to work properly.",
    "crumbs": [
      "R Tutorials",
      "Install and Setup R"
    ]
  },
  {
    "objectID": "R-InstallR.html#why-do-we-need-to-install-rtools-and-commandline-tools",
    "href": "R-InstallR.html#why-do-we-need-to-install-rtools-and-commandline-tools",
    "title": "Install and Setup R",
    "section": "6.1 Why do we need to install Rtools and Commandline tools?",
    "text": "6.1 Why do we need to install Rtools and Commandline tools?\nMany R packages are written in R. Since R is an interpreted language, source code written in R doesn’t have to be translated into system-specific machine language before running. However, some R packages have significant portions written in compiled languages, such as C/C++ or Fortran. These languages need accessory software tools to translate (“compile”) their source code into machine language that can run on a particular system.\nPackage developers have two choices when distributing code for compiled languages:\n\nThey can prepare compiled, realdy-to-use “binaries” matched against common systems, so that people can simply download the binaries and directly use their packages code without having to know how to compile it.\nThey can distribute source code (i.e., the raw C++ codes) only, and expect the user to have the right compiler software to build system-specific runnable code themselves. Rtools and Commandline tools are the compliers that do the job, therefore needed as an additional installation step.\n\nOn UNIX/Linux, only source code is distributed and all packages are compiled from source during installation (for packages written entirely in R, this is trivial!). For Windows and Mac, CRAN makes pre-compiled binaries available. On Windows, install.packages() will only install precompiled binaries, unless explicitly forced to install from source (you can read a lot more about this in the R Installation and Administration guide).",
    "crumbs": [
      "R Tutorials",
      "Install and Setup R"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html",
    "href": "Case-PreliminaryCustomerAnalysis.html",
    "title": "Descriptive Analytics for M&S",
    "section": "",
    "text": "The amount of data created worldwide has been increasing exponentially over the past decade with some estimates placing the total at over 60 zettabytes as of 2030 (Source: Statista). Data without analytics, however, is of little value to business decision-makers aiming to improve performance and increase growth. It is therefore no surprise that top-tier consulting companies, analytics ﬁrms and business schools have been promoting the positive returns to greater usage of analytics technology. It also explains why an increasing number of data analytics enthusiasts (like Tom and you!) are willing to pay up to £40k tuition fee (that is 10,000 bubble teas!) to join the world’s best MSc Business Analytics program at the UCL School of Management (hmm, it’s now already week 3, too late to ask David for a refund!).\nBy identifying patterns and trends in massive amounts of data, business analytics enables organizations to make better decisions and improve performance. Descriptive analytics is the simplest and most widely used type of analytics; it is used to generate key performance indicators (KPIs) and metrics for business reports and dashboards. Research shows that, even with the adoption of very simple descriptive analytics, businesses can improve their performance by a large extent — Berman and Israeli (2022) analyze the staggered adoption of a retail analytics dashboard by more than 1,500 e-commerce websites,1 and find an increase of 4%–10% in average weekly revenues post-adoption. The increase in revenue is not explained by price changes or advertising optimization. Instead, it is consistent with the improvement on customer relationship management, personalization, and prospecting technologies to retailer websites. The adoption and usage of descriptive analytics also increases the diversity of products sold, the number of transactions, the numbers of website visitors and unique customers, and the revenue from repeat customers. These findings are consistent with a complementary effect of descriptive analytics that serve as a monitoring device that helps business analysts control additional martech tools and amplify their value. Without using the descriptive dashboard, companies are unable to reap the benefits associated with these technologies.\nIn the remaining of this case, we will explore (1) how to conduct data wrangling and data cleaning using R and (2) how to conduct descriptive analytics for M&S to achieve marketing efficiency. You might ask, where is Tom? Well, M&S rewarded Tom with a few days off for his successful calculation of CLV last week, and he is now considering setting up his bubble tea business in Canary Wharf. You are the only one who can help M&S now until Tom’s return!",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#demographic-information",
    "href": "Case-PreliminaryCustomerAnalysis.html#demographic-information",
    "title": "Descriptive Analytics for M&S",
    "section": "2.1 Demographic Information",
    "text": "2.1 Demographic Information\nKnowing your consumer is a vital concept of running any business. Is the business selling fertilizer to farmers, apparel to teenage girls, or vacations to senior citizens? The distinctions are readily apparent in this comparison.\nDemographics define the qualities of clients. To be successful, business owners must understand the demographics of their clients and the trends or changes that are occurring within those specific traits.\nThe following demographic information is usually of interest to business managers:\nAge: Consumer behavior is strongly influenced by age. Younger consumers are more affluent and willing to spend more on entertainment, fashion, and movies. Seniors spend less on these items; they are less active, spend more time indoors, and require more medical treatment. Additionally, market segments can be defined by age groups. For instance, digital devices such as iPhones are targeted more towards millennials than at seniors. While older adults are increasingly utilizing technology, they remain less digitally savvy than millennials and purchase fewer digital products.\nGender: Gender also matters. Males and females have vastly diverse demands and tastes, which influence their purchasing decisions. As a result, some products are created with a specific gender in mind. Macy’s, Nordstrom, and The Gap all have departments dedicated to teenage girls’ clothes, while Seiko has a specific line of diver watches for men only.\nIncome: Income has a substantial influence on consumer behavior and product purchases. Middle-income customers make purchases with due regard for the utility of money. They do not have unlimited money to spend, and hence the money spent on one item may be used on something else. On the other hand, consumers with higher incomes tend to be less price sensitive and have a higher willingness to pay.\nEducation: Consumers’ level of education has an effect on their impressions of the world around them and on the amount of research they conduct prior to making a purchase. Individuals with a higher level of education will spend more time educating themselves before investing their money. Education has an impact on fashion, film, and television programming. Consumers with a higher level of education can be more distrustful of commercials and the facts offered.\nM&S has collected rich customer demographic information through its loyalty program, M&S Sparks membership. The data scientist team has the following demographic variables:\n\nID: Customer’s unique identifier\nYear_Birth: Customer’s birth year\nEducation: Customer’s education level\nMarital_Status: Customer’s marital status\nIncome: Customer’s yearly household income\nKidhome: Number of children in customer’s household\nTeenhome: Number of teenagers in customer’s household\nDt_Customer: Date of customer’s enrollment with the company",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#purchase-history",
    "href": "Case-PreliminaryCustomerAnalysis.html#purchase-history",
    "title": "Descriptive Analytics for M&S",
    "section": "2.2 Purchase History",
    "text": "2.2 Purchase History\n“History doesn’t repeat itself, but it often rhymes.”\nThis popular aphorism, frequently (and perhaps incorrectly) attributed to Mark Twain, is frequently invoked to demonstrate that, while past events do not always provide a clear indication of future events, they do provide valuable context. This sentiment is especially true for marketing managers, where a consumer’s purchase history provides invaluable insight into their future purchasing habits.\nM&S’s data engineering team has assembled a cross-sectional customer purchase history data today, with variables including2:\n\nMntWines: Amount spent on wine in last 2 years\nMntFruits: Amount spent on fruits in last 2 years\nMntMeatProducts: Amount spent on meat in last 2 years\nMntFishProducts: Amount spent on fish in last 2 years\nMntSweetProducts: Amount spent on sweets in last 2 years\nMntGoldProds: Amount spent on gold and jewelry in last 2 years\nNumDealsPurchases: Number of purchases made with a discount\nNumWebPurchases: Number of purchases made through Ocado’s web site\nNumCatalogPurchases: Number of purchases made using a catalogue\nNumStorePurchases: Number of purchases made directly in stores\nNumWebVisitsMonth: Number of visits to company’s web site in the last month\nComplain: 1 if customer complained in the last 2 years, 0 otherwise\nResponse: 1 if customer accepted the offer in the last campaign, 0 otherwise\nRecency: Number of days since customer’s last purchase",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#data-loading",
    "href": "Case-PreliminaryCustomerAnalysis.html#data-loading",
    "title": "Descriptive Analytics for M&S",
    "section": "3.1 Data Loading",
    "text": "3.1 Data Loading\nTo work on the datasets, we first need to load the raw data into R. In R, we can use read.csv(filepath) to load the data into R environment.\nFor your convenience, I have stored the data file on my Dropbox and the download link will be provided. We can directly feed the url links to read.csv() function in R to download and create the dataset in R’s environment.\nThe data is also provided to you in .csv format, which you can also load from your local disk.\n\nQuestion 1Answer\n\n\nLoad the dataset into your R environment. Name the dataset as data_full.\n\n\n\n\nCode\n## use read.csv() to download and load the data, assign it to an R data object\n## header = T argument is to tell read.csv() to keep the dataset header (first row)\n\npacman::p_load(dplyr, modelsummary)\n\ndata_full &lt;- read.csv(\"https://www.dropbox.com/scl/fi/2q7ppqtyca0pd3j486osl/data_full.csv?rlkey=gsyk51q27vd1skek4qpn5ikgm&dl=1\")\n\n\n\n\n\nAfter running the above code blocks, you should see the dataset named data_full in your RStudio environment.\nNow, click into the dataset, take a look, and get a sense of how it looks like.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#data-types",
    "href": "Case-PreliminaryCustomerAnalysis.html#data-types",
    "title": "Descriptive Analytics for M&S",
    "section": "3.2 Data Types",
    "text": "3.2 Data Types\n\nQuestion 2Answer\n\n\nCheck all data types in data_full are correct and as expected\n\n\n\n\nCode\n# can use str() to get the structure of data\n# Lina covered this in the induction week\nstr(data_full)\n\n\n'data.frame':   2000 obs. of  22 variables:\n $ ID                 : int  5524 2174 4141 6182 5324 7446 965 6177 4855 5899 ...\n $ MntWines           : int  635 11 426 11 173 520 235 76 14 28 ...\n $ MntFruits          : int  88 1 49 4 43 42 65 10 0 0 ...\n $ MntMeatProducts    : int  546 6 127 20 118 98 164 56 24 6 ...\n $ MntFishProducts    : int  172 2 111 10 46 0 50 3 3 1 ...\n $ MntSweetProducts   : int  88 1 21 3 27 42 49 1 3 1 ...\n $ MntGoldProds       : int  88 6 42 5 15 14 27 23 2 13 ...\n $ NumDealsPurchases  : int  3 2 1 2 5 2 4 2 1 1 ...\n $ NumWebPurchases    : int  8 1 8 2 5 6 7 4 3 1 ...\n $ NumCatalogPurchases: int  10 1 2 0 3 4 3 0 0 0 ...\n $ NumStorePurchases  : int  4 2 10 4 6 10 7 4 2 0 ...\n $ NumWebVisitsMonth  : int  7 5 4 6 5 6 6 8 9 20 ...\n $ Complain           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Response           : int  1 0 0 0 0 0 0 0 1 0 ...\n $ Year_Birth         : int  1957 1954 1965 1984 1981 1967 1971 1985 1974 1950 ...\n $ Education          : chr  \"Graduation\" \"Graduation\" \"Graduation\" \"Graduation\" ...\n $ Marital_Status     : chr  \"Single\" \"Single\" \"Together\" \"Together\" ...\n $ Income             : int  58138 46344 71613 26646 58293 62513 55635 33454 30351 5648 ...\n $ Kidhome            : int  0 1 0 1 1 0 0 1 1 1 ...\n $ Teenhome           : int  0 1 0 0 0 1 1 0 0 1 ...\n $ Dt_Customer        : chr  \"04/09/2012\" \"08/03/2014\" \"21/08/2013\" \"10/02/2014\" ...\n $ Recency            : int  58 38 26 26 94 16 34 32 19 68 ...",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#data-cleaning",
    "href": "Case-PreliminaryCustomerAnalysis.html#data-cleaning",
    "title": "Descriptive Analytics for M&S",
    "section": "3.3 Data Cleaning",
    "text": "3.3 Data Cleaning\nData cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset. We often need to deal with missing values and outliers. In this case, we will focus on missing values.\n\nQuestion 3Answer\n\n\nCheck if any variables in data_full have missing values. If so, how many missing values are there in each variable? Use the variable average to replace missing values in the dataset.\n\n\n\n\nCode\ndatasummary_skim(data_full, fmt = 3)\n\n\nWarning: These variables were omitted because they include more than 50 levels:\nDt_Customer.\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID\n                  2000\n                  0\n                  5599.172\n                  3242.019\n                  0.000\n                  5492.000\n                  11191.000\n                  \n                \n                \n                  MntWines\n                  738\n                  0\n                  306.058\n                  338.276\n                  0.000\n                  176.500\n                  1493.000\n                  \n                \n                \n                  MntFruits\n                  157\n                  0\n                  26.358\n                  39.885\n                  0.000\n                  8.000\n                  199.000\n                  \n                \n                \n                  MntMeatProducts\n                  532\n                  0\n                  167.885\n                  225.292\n                  0.000\n                  68.000\n                  1725.000\n                  \n                \n                \n                  MntFishProducts\n                  179\n                  0\n                  37.614\n                  54.556\n                  0.000\n                  12.000\n                  259.000\n                  \n                \n                \n                  MntSweetProducts\n                  175\n                  0\n                  27.495\n                  41.756\n                  0.000\n                  8.000\n                  263.000\n                  \n                \n                \n                  MntGoldProds\n                  207\n                  0\n                  43.786\n                  51.736\n                  0.000\n                  24.000\n                  362.000\n                  \n                \n                \n                  NumDealsPurchases\n                  15\n                  0\n                  2.321\n                  1.958\n                  0.000\n                  2.000\n                  15.000\n                  \n                \n                \n                  NumWebPurchases\n                  15\n                  0\n                  4.082\n                  2.773\n                  0.000\n                  4.000\n                  27.000\n                  \n                \n                \n                  NumCatalogPurchases\n                  14\n                  0\n                  2.691\n                  2.956\n                  0.000\n                  2.000\n                  28.000\n                  \n                \n                \n                  NumStorePurchases\n                  14\n                  0\n                  5.819\n                  3.256\n                  0.000\n                  5.000\n                  13.000\n                  \n                \n                \n                  NumWebVisitsMonth\n                  15\n                  0\n                  5.308\n                  2.452\n                  0.000\n                  6.000\n                  20.000\n                  \n                \n                \n                  Complain\n                  2\n                  0\n                  0.010\n                  0.100\n                  0.000\n                  0.000\n                  1.000\n                  \n                \n                \n                  Response\n                  2\n                  0\n                  0.150\n                  0.358\n                  0.000\n                  0.000\n                  1.000\n                  \n                \n                \n                  Year_Birth\n                  59\n                  0\n                  1968.843\n                  12.020\n                  1893.000\n                  1970.000\n                  1996.000\n                  \n                \n                \n                  Income\n                  1783\n                  1\n                  52139.697\n                  21492.372\n                  1730.000\n                  51518.000\n                  162397.000\n                  \n                \n                \n                  Kidhome\n                  3\n                  0\n                  0.444\n                  0.540\n                  0.000\n                  0.000\n                  2.000\n                  \n                \n                \n                  Teenhome\n                  3\n                  0\n                  0.501\n                  0.546\n                  0.000\n                  0.000\n                  2.000\n                  \n                \n                \n                  Recency\n                  100\n                  0\n                  49.165\n                  28.951\n                  0.000\n                  50.000\n                  99.000\n                  \n                \n                \n                   \n                    \n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Education\n                  2n Cycle\n                  185\n                  9.250\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Basic\n                  43\n                  2.150\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Graduation\n                  992\n                  49.600\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Master\n                  327\n                  16.350\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  PhD\n                  453\n                  22.650\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  Marital_Status\n                  Alone\n                  3\n                  0.150\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Divorced\n                  206\n                  10.300\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Married\n                  767\n                  38.350\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Single\n                  436\n                  21.800\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Together\n                  521\n                  26.050\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  Widow\n                  67\n                  3.350\n                  \n                  \n                  \n                  \n                  \n                \n        \n      \n    \n\n\n\n\nWe find that income has missing values for a few customers. We can replace the missing values with the average income of all customers.\n\nTip: use mutate() and ifelse() functions in dplyr to replace missing values\n\n\n\n\nCode\ndata_full &lt;- data_full %&gt;%\n    mutate(Income = ifelse(is.na(Income), mean(Income, na.rm = T), Income))",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Case-PreliminaryCustomerAnalysis.html#footnotes",
    "href": "Case-PreliminaryCustomerAnalysis.html#footnotes",
    "title": "Descriptive Analytics for M&S",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe will cover the difference-in-differences technique to establish causal inference later in the module.↩︎\nCross-sectional data is data collected at a single point in time. It is a snapshot of customers at a particular moment. In contrast, longitudinal data is collected over time, allowing analysts to track changes in customer behavior over time.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Case Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html",
    "href": "Week9-Lecture1.html",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "",
    "text": "Understand the importance of causal inference for platform businesses.\nLearn how to estimate causal effects using instrumental variables with an application to ride-sharing platforms.\n\n\n\n\n\nPlatform businesses often need to answer critical causal questions to optimize their operations:\n\nMeasuring Network effects: How does increasing supply affect demand (and vice versa)?\nPricing: How does surge pricing affect consumer demand and driver supply?\n\nWhen relying on secondary non-experimental data, these questions often face endogeneity challenges that require careful empirical strategies.\n\n\n\n\n\n\n\n\n\n\nHow to estimate the causal effect of surge prices on driver work decisions using historical data?\n\n\n\n\nWe can run a linear regression model on Uber’s historical data, where the dependent variable is the number of drivers on the road in an hour; the key explanatory variable is the surge multiplier during that hour.\n\n\\[\n\\text{NumberDrivers} = \\beta_0 + \\beta_1 \\text{SurgeMultiplier} + \\varepsilon\n\\]\n\nIs there endogeneity in this model?1\n\n\n\nOmitted variable bias: The surge multiplier is likely correlated with other factors that affect driver decisions, such as weather conditions or time of day. Therefore, the OLS estimate of \\(\\beta_1\\) may be biased due to OVB.\n\n\n\n\nReverse causality: When the surge multiplier increases, drivers are more likely to work, leading to higher labor supply. However, when there are more drivers on the road, the surge multiplier decreases. Therefore, there is a potential reverse causality problem.\n\n\n\n\n\n\nCore case question: How did new COVID-19 cases causally affect drivers’ labor supply patterns?\n\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha NewCases_{jt} + \\varepsilon_{ijt}\n\\]\n\nOLS linear regression model\n\n\\(LaborOutcome_{ijt}\\): Driver i’s labor supply (e.g., whether worked that day) in city j on day t\n\\(NewCases_{jt}\\): Daily new COVID-19 cases in the city t on day t",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#class-objectives",
    "href": "Week9-Lecture1.html#class-objectives",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "",
    "text": "Understand the importance of causal inference for platform businesses.\nLearn how to estimate causal effects using instrumental variables with an application to ride-sharing platforms.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#causal-questions-for-platform-businesses-1",
    "href": "Week9-Lecture1.html#causal-questions-for-platform-businesses-1",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "",
    "text": "Platform businesses often need to answer critical causal questions to optimize their operations:\n\nMeasuring Network effects: How does increasing supply affect demand (and vice versa)?\nPricing: How does surge pricing affect consumer demand and driver supply?\n\nWhen relying on secondary non-experimental data, these questions often face endogeneity challenges that require careful empirical strategies.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#causal-question-example",
    "href": "Week9-Lecture1.html#causal-question-example",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "",
    "text": "How to estimate the causal effect of surge prices on driver work decisions using historical data?\n\n\n\n\nWe can run a linear regression model on Uber’s historical data, where the dependent variable is the number of drivers on the road in an hour; the key explanatory variable is the surge multiplier during that hour.\n\n\\[\n\\text{NumberDrivers} = \\beta_0 + \\beta_1 \\text{SurgeMultiplier} + \\varepsilon\n\\]\n\nIs there endogeneity in this model?1\n\n\n\nOmitted variable bias: The surge multiplier is likely correlated with other factors that affect driver decisions, such as weather conditions or time of day. Therefore, the OLS estimate of \\(\\beta_1\\) may be biased due to OVB.\n\n\n\n\nReverse causality: When the surge multiplier increases, drivers are more likely to work, leading to higher labor supply. However, when there are more drivers on the road, the surge multiplier decreases. Therefore, there is a potential reverse causality problem.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#case-study-background",
    "href": "Week9-Lecture1.html#case-study-background",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "",
    "text": "Core case question: How did new COVID-19 cases causally affect drivers’ labor supply patterns?\n\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha NewCases_{jt} + \\varepsilon_{ijt}\n\\]\n\nOLS linear regression model\n\n\\(LaborOutcome_{ijt}\\): Driver i’s labor supply (e.g., whether worked that day) in city j on day t\n\\(NewCases_{jt}\\): Daily new COVID-19 cases in the city t on day t",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#driver-daily-trip-data",
    "href": "Week9-Lecture1.html#driver-daily-trip-data",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "2.1 Driver Daily Trip Data",
    "text": "2.1 Driver Daily Trip Data\n\nDriver-level daily trip data from a ride-sharing platform. About 4000 drivers across 3 UK cities in April 2020.\n\n\n\nCode\npacman::p_load(dplyr, tidyr, broom)\n# load the driver data from dropbox\ndata_driver &lt;- read.csv(\"https://www.dropbox.com/s/9qisr9zau53gix6/data_driver.csv?dl=1\")\ndata_covid &lt;- read.csv(\"https://www.dropbox.com/s/j5vs1egwl5j51f4/data_covid.csv?dl=1\")\n\n\n\n\nCode\ndata_driver %&gt;%\n    slice(1:5)",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#covid-19-data",
    "href": "Week9-Lecture1.html#covid-19-data",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "2.2 COVID-19 Data",
    "text": "2.2 COVID-19 Data\n\nDaily new cases by city, which serves as key explanatory variable the X\n\n\n\nCode\ndata_covid %&gt;%\n    slice(1:5)",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#join-multiple-data-frames",
    "href": "Week9-Lecture1.html#join-multiple-data-frames",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "2.3 Join Multiple Data Frames",
    "text": "2.3 Join Multiple Data Frames\n\nWe can consolidate (marge, join) multiple data frames using the left_join() function in the dplyr package.\nWe need to determine the main data frame that will be retained as the final data for analyses. The other data frame will be used as the supplementary data to provide additional information.\n\nWe often use the most granular data frame (usually panel data) as the main data frame\nThe less granular data such as demographic data can be joined onto the main data frame\n\nIn this case, we will use the driver data as the main data frame and join the COVID-19 data onto it.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#left_join",
    "href": "Week9-Lecture1.html#left_join",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "2.4 left_join()",
    "text": "2.4 left_join()\n\nleft_join keeps everything from the left data frame and matches as much as it can from the right data frame based on the chosen IDs.\n\nChoose the longer data frame as the left data frame. All IDs in the left data frame will be retained\nIf a match can be found, value from the right data frame will be filled in\nIf a match cannot be found, a missing value will be returned\n\n\n\n\nCode\ndf_1 %&gt;%\n    left_join(df_2, by = c(\"ID\" = \"ID\"))\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: Join the driver data with the COVID-19 data using the left_join() function.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#other-joins-in-dplyr",
    "href": "Week9-Lecture1.html#other-joins-in-dplyr",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "2.5 Other joins in dplyr",
    "text": "2.5 Other joins in dplyr\n\ninner_join(): Keeps only the IDs that are present in both data frames\nright_join(): Keeps everything from the right data frame and matches as much as it can from the left data frame based on the chosen IDs\nfull_join(): Keeps everything from both data frames and matches as much as it can based on the chosen IDs",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#ols-linear-regression",
    "href": "Week9-Lecture1.html#ols-linear-regression",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "3.1 OLS Linear Regression",
    "text": "3.1 OLS Linear Regression\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha NewCases_{jt} + \\varepsilon_{ijt}\n\\]\n\nOmitted variable bias: local city policies which affect both COVID cases and driver behavior (lockdowns, mask mandates, etc.)\nReverse causality: Drivers may reduce labor supply in response to COVID cases, but COVID cases may also increase due to driver behavior\nExercise: Run the linear regression model on Quarto.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#fixed-effects-regression",
    "href": "Week9-Lecture1.html#fixed-effects-regression",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "3.2 Fixed Effects Regression",
    "text": "3.2 Fixed Effects Regression\n\nExtended model with fixed effects:\n\nDriver fixed effects: Control for inherent, time-invariant driver characteristics\nTime fixed effects: Controls for temporal trends common in all cities\nCity fixed effects: Control for local policies and other time-invariant city characteristics\n\n\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha NewCases_{jt} + DriverFE + DayFE + CityFE + \\varepsilon_{ijt}\n\\]\n\nExercise: Run the fixed effects regression model on Quarto.\nIs there still endogeneity in this model?2\n\n\nYes, fixed effects regression can control for time-invariant unobserved factors. However, it does not address the endogeneity issue due to reverse causality. COVID cases may still be affected by driver behavior.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#instrumental-variables-regression",
    "href": "Week9-Lecture1.html#instrumental-variables-regression",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "3.3 Instrumental Variables Regression",
    "text": "3.3 Instrumental Variables Regression\n\nInstruments that satisfy (1) relevance and (2) exogeneity and (3) exclusion restriction:\n\nCandidate 1: Imported new cases from overseas\nCandidate 2: Cases from neighboring cities",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#two-stage-least-squares-2sls-estimation-first-stage",
    "href": "Week9-Lecture1.html#two-stage-least-squares-2sls-estimation-first-stage",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "3.4 Two-Stage Least Squares (2SLS) Estimation: First Stage",
    "text": "3.4 Two-Stage Least Squares (2SLS) Estimation: First Stage\n\nFirst stage: Regress endogenous variable on instruments including all control variables.\n\n\\[\nNewCases_{ijt} = \\pi_0 + \\pi_1 Z_{ijt} + DriverFE + DayFE + CityFE + \\varepsilon_{ijt}\n\\]\n\nPractical considerations:\n\nCheck for instrument relevance: Instruments should be correlated with the endogenous variable, which can be tested whether the coefficient of Z is significantly different from zero\nExogeneity and exclusion restriction are untestable assumptions, and we need to justify them based on the context.\nThe same set of control variables must be included in both stages. In our case, the 3 sets of fixed effects must be included in both stages.\n\nExercise: Run the first stage regression model on Quarto.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#two-stage-least-squares-2sls-estimation-second-stage",
    "href": "Week9-Lecture1.html#two-stage-least-squares-2sls-estimation-second-stage",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "3.5 Two-Stage Least Squares (2SLS) Estimation: Second Stage",
    "text": "3.5 Two-Stage Least Squares (2SLS) Estimation: Second Stage\n\nSecond stage: Regress labor outcome on predicted new cases from the first stage and all control variables\n\n\\[\nLaborOutcome_{ijt} = \\beta_0 + \\alpha \\hat{NewCases}_{ijt} + DriverFE + DayFE + CityFE + \\varepsilon_{ijt}\n\\]\n\nThe coefficient \\(\\alpha\\) is the causal effect of new COVID cases on driver labor supply.\nExercise: Run the 2SLS regression model on Quarto.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#after-class-reading",
    "href": "Week9-Lecture1.html#after-class-reading",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "3.6 After-Class Reading",
    "text": "3.6 After-Class Reading\n\n(highly recommended) Encouragement Designs and Instrumental Variables for A/B Testing at Spotify",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "Week9-Lecture1.html#footnotes",
    "href": "Week9-Lecture1.html#footnotes",
    "title": "Class 17 Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnswers are available on the HTML version.↩︎\nAnswers are available on the HTML version.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 1: Case Study: Estimating Causal Effects for Platform Businesses Using Instrumental Variables"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is the online supplement to the MSIN0094 Marketing Analytics Module for the MSc Business Analytics program at UCL School of Management.\nFor more information about me and my research, please refer to my personal website."
  },
  {
    "objectID": "WeeklyArrangements.html",
    "href": "WeeklyArrangements.html",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "This page provides a detailed weekly arrangement for the module. You can find the pre-class preparation, in-class topics, and after-class exercises for each week here.\nSince marketing is evolving rapidly, we will cover a wide range of topics in this module. I’m also updating the contents each year to keep up with the latest trends in marketing analytics. Therefore, remember to check this page each week before class to ensure you are well-prepared for the lecture and case study workshop.\n\n\nEach week, you are required to complete pre-class preparation tasks.\nThe preparation usually includes reading case studies, watching videos, or completing coding exercises.\nYou can find the pre-class preparation under each week’s topic in this guide and on Moodle.\nIt’s mandatory to finish the pre-class preparation for best learning outcomes. Otherwise, you may find it hard to follow the lecture and case study workshop.\n\n\n\nWe will have a 3-hour session on each Wednesday for 10 weeks.\nEach week, I will cover a new analytics tool, followed by a case study workshop for you to practice the new technique. This way, you can further reflect on your understanding of the technique by practicing your skills with a real-life application.\nFor instance, in week 1 and week 2, I will first introduce the concepts of marketing and marketing process, and then will cover the concept of break-even analysis, net present value, customer lifetime value (CLV) and how to compute CLV with R. We will then solve a case study that helps you practice your knowledge of CLV, so you can understand how to use CLV for better marketing decisions in your future projects/jobs.\nSimilarly for the rest of the weeks, we will cover a new analytics tool and then practice it with a case study.\n\n\n\nAfter each week’s lecture, you will find a list of After-class reading and exercises. Some are essential, while others are optional.\n\nEssential: contents and R exercises core to this week’s materials. All pre-class preparations are expected to be completed.\nOptional: supplemental readings for those interested in learning more about the topic.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#before-the-lecture",
    "href": "WeeklyArrangements.html#before-the-lecture",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "Each week, you are required to complete pre-class preparation tasks.\nThe preparation usually includes reading case studies, watching videos, or completing coding exercises.\nYou can find the pre-class preparation under each week’s topic in this guide and on Moodle.\nIt’s mandatory to finish the pre-class preparation for best learning outcomes. Otherwise, you may find it hard to follow the lecture and case study workshop.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#during-the-lecture",
    "href": "WeeklyArrangements.html#during-the-lecture",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "We will have a 3-hour session on each Wednesday for 10 weeks.\nEach week, I will cover a new analytics tool, followed by a case study workshop for you to practice the new technique. This way, you can further reflect on your understanding of the technique by practicing your skills with a real-life application.\nFor instance, in week 1 and week 2, I will first introduce the concepts of marketing and marketing process, and then will cover the concept of break-even analysis, net present value, customer lifetime value (CLV) and how to compute CLV with R. We will then solve a case study that helps you practice your knowledge of CLV, so you can understand how to use CLV for better marketing decisions in your future projects/jobs.\nSimilarly for the rest of the weeks, we will cover a new analytics tool and then practice it with a case study.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#after-the-lecture",
    "href": "WeeklyArrangements.html#after-the-lecture",
    "title": "Weekly Arrangements",
    "section": "",
    "text": "After each week’s lecture, you will find a list of After-class reading and exercises. Some are essential, while others are optional.\n\nEssential: contents and R exercises core to this week’s materials. All pre-class preparations are expected to be completed.\nOptional: supplemental readings for those interested in learning more about the topic.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-1-module-introduction",
    "href": "WeeklyArrangements.html#class-1-module-introduction",
    "title": "Weekly Arrangements",
    "section": "Class 1: Module Introduction",
    "text": "Class 1: Module Introduction\n\nWhat you will learn\n\nAn overview of the course topics\nConcepts of marketing and the marketing process (5Cs, STP, and 4Ps)\nHow marketing analytics can empower marketers in the digital era\n\nAfter-class reading and exercise\n\n(optional) The Definitive Guide to Strategic Marketing Planning. Highly recommended if you didn’t take marketing undergrad courses and would like to know more about the conventional marketing process.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-2-profitability-analysis",
    "href": "WeeklyArrangements.html#class-2-profitability-analysis",
    "title": "Weekly Arrangements",
    "section": "Class 2: Profitability Analysis",
    "text": "Class 2: Profitability Analysis\n\nWhat you will learn\n\nHow to conduct break-even quantity for a marketing proposal\nHow to conduct net present value analysis for a marketing proposal",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-3-customer-lifetime-value",
    "href": "WeeklyArrangements.html#class-3-customer-lifetime-value",
    "title": "Weekly Arrangements",
    "section": "Class 3: Customer Lifetime Value",
    "text": "Class 3: Customer Lifetime Value\n\nWhat you will learn\n\nThe concept of customer lifecycle and customer lifetime value (CLV)\nHow to compute customer acquisition costs (CAC)\nHow to compute customer lifetime value (CLV)",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-4-case-study-customer-lifetime-value-for-guiding-marketing-decisions",
    "href": "WeeklyArrangements.html#class-4-case-study-customer-lifetime-value-for-guiding-marketing-decisions",
    "title": "Weekly Arrangements",
    "section": "Class 4: (Case Study) Customer Lifetime Value for Guiding Marketing Decisions",
    "text": "Class 4: (Case Study) Customer Lifetime Value for Guiding Marketing Decisions\n\nWhat you will learn\n\nHow to apply CLV calculation in a real-life case study\nDiscuss how CLV can be used by marketers to guide marketing decisions\n\nAfter-class reading and exercise\n\nAfter-class exercise for Week 2\n(optional) “Hubspot: How to compute CLV”. This article introduces alternative ways to compute CLV, which are used in many companies.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-5-data-wrangling-with-r",
    "href": "WeeklyArrangements.html#class-5-data-wrangling-with-r",
    "title": "Weekly Arrangements",
    "section": "Class 5: Data Wrangling with R",
    "text": "Class 5: Data Wrangling with R\n\nWhat you will learn\n\nProcess of a typical data analytics project (such as your term 3 dissertation)\nHow to use filter, mutate, arrange, and group_by for data manipulation with dplyr package in R",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-6-case-study-descriptive-analytics-for-ms",
    "href": "WeeklyArrangements.html#class-6-case-study-descriptive-analytics-for-ms",
    "title": "Weekly Arrangements",
    "section": "Class 6: (Case Study) Descriptive Analytics for M&S",
    "text": "Class 6: (Case Study) Descriptive Analytics for M&S\n\nWhat you will learn\n\nHow to use dplyr to conduct preliminary customer analyses for Marks & Spencer\n\nAfter-class reading and exercise\n\n(essential) Cheatsheet for dplyr. This cheatsheet provides a quick reference for the most commonly used functions in the dplyr package. It’s very important to familiarize yourself with these functions as you will use them a lot in your future projects.\n(optional) Complete the after-class exercise for Week 3. If you still have time, you can also complete the data camp exercise on the dplyr package. The link is here.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-7-unsupervised-learning-and-k-means-clustering",
    "href": "WeeklyArrangements.html#class-7-unsupervised-learning-and-k-means-clustering",
    "title": "Weekly Arrangements",
    "section": "Class 7: Unsupervised Learning and K-Means Clustering",
    "text": "Class 7: Unsupervised Learning and K-Means Clustering\n\nWhat you will learn\n\nThe concept of predictive analytics\nThe difference between supervised and unsupervised learning\nImportant concepts in predictive analytics\nConcept of unsupervised learning\nHow to run K-means clustering in R",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-8-case-study-customer-segmentation-using-k-means-for-ms",
    "href": "WeeklyArrangements.html#class-8-case-study-customer-segmentation-using-k-means-for-ms",
    "title": "Weekly Arrangements",
    "section": "Class 8: (Case Study) Customer Segmentation Using K-Means for M&S",
    "text": "Class 8: (Case Study) Customer Segmentation Using K-Means for M&S\n\nWhat you will learn\n\nHow to apply K-means clustering to help Marks & Spencer segment its customers\n\nAfter-class reading and exercise\n\n(optional) K-means Cluster Analysis, which provides more details on the maths behind the K-means clustering",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-9-supervised-learning-and-tree-based-models",
    "href": "WeeklyArrangements.html#class-9-supervised-learning-and-tree-based-models",
    "title": "Weekly Arrangements",
    "section": "Class 9: Supervised Learning and Tree-based Models",
    "text": "Class 9: Supervised Learning and Tree-based Models\n\nWhat you will learn\n\nDefinition of supervised learning\nTypes of supervised learning\nFundamental tradeoffs in supervised learning\nOverfitting and underfitting issues and how to overcome them\nIntuition behind decision tree and random forest models\nHow to build random forest models in R\n\nAfter-class reading and exercise\n\n(optional) Varian, Hal R. “Big data: New tricks for econometrics.” Journal of Economic Perspectives 28, no. 2 (2014): 3-28\n(optional) Decision tree in R and Random forest in R. Both tutorials introduce the detailed maths behind the two models if you would like to learn more",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-10-case-study-improve-marketing-efficiency-for-marks-spencer-using-supervised-learning",
    "href": "WeeklyArrangements.html#class-10-case-study-improve-marketing-efficiency-for-marks-spencer-using-supervised-learning",
    "title": "Weekly Arrangements",
    "section": "Class 10: (Case Study) Improve Marketing Efficiency for Marks & Spencer Using Supervised Learning",
    "text": "Class 10: (Case Study) Improve Marketing Efficiency for Marks & Spencer Using Supervised Learning\n\nWhat you will learn\n\nHow to apply supervised learning models (random forest and others) to help Marks & Spencer improve its marketing efficiency",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-11-causal-inference-potential-outcome-framework",
    "href": "WeeklyArrangements.html#class-11-causal-inference-potential-outcome-framework",
    "title": "Weekly Arrangements",
    "section": "Class 11: Causal Inference, Potential Outcome Framework",
    "text": "Class 11: Causal Inference, Potential Outcome Framework\n\nWhat you will learn\n\nConcept of causal inference\nConcept of Rubin’s potential outcome framework and treatment effects\nWhy randomized experiments (A/B testings) is the gold standard of causal inference\n\nAfter-class reading and exercise\n\n(optional) Varian, Hal R. ‘Causal Inference in Economics and Marketing’. Proceedings of the National Academy of Sciences 113, no. 27 (5 July 2016): 7310–15.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-12-ab-testing",
    "href": "WeeklyArrangements.html#class-12-ab-testing",
    "title": "Weekly Arrangements",
    "section": "Class 12: A/B Testing",
    "text": "Class 12: A/B Testing\n\nWhat you will learn\n\nHow to design and conduct randomized experiments\nHow to interpret the results of randomized experiments\nHow to use randomized experiments to solve real-life marketing problems",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-13-case-study-improve-user-engagement-for-instagram-using-abn-testing",
    "href": "WeeklyArrangements.html#class-13-case-study-improve-user-engagement-for-instagram-using-abn-testing",
    "title": "Weekly Arrangements",
    "section": "Class 13: (Case Study) Improve User Engagement for Instagram Using A/B/N Testing",
    "text": "Class 13: (Case Study) Improve User Engagement for Instagram Using A/B/N Testing\n\nWhat you will learn\n\nSteps to design and conduct a randomized experiment (A/B testing)\nThe business model of social media platforms\nDesign an A/B testing to help Instagram to improve user engagement\n\nAfter-class reading and exercise\n\n(optional) Test and learn: How a culture of experimentation can help grow your business",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-14-linear-regression-for-causal-inference",
    "href": "WeeklyArrangements.html#class-14-linear-regression-for-causal-inference",
    "title": "Weekly Arrangements",
    "section": "Class 14: Linear Regression for Causal Inference",
    "text": "Class 14: Linear Regression for Causal Inference\n\nWhat you will learn\n\nReview of concept for Data Generating Process (DGP) and a model\nThe intuition behind coefficient estimation of linear regression models\nHow to run linear regression models in R\nHow to interpret the regression coefficients and statistics\nHow to interpret the coefficients of categorical variables\n\nAfter-class reading and exercise\n\n(optional) Introduction to Econometrics with R, Chapters 4-7. These 4 chapters cover very detailed applied knowledge of linear regressions. Due to limited time, we cannot cover all contents in class, so it would be great if you can take time to go through these chapters thoroughly.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-15-endogeneity",
    "href": "WeeklyArrangements.html#class-15-endogeneity",
    "title": "Weekly Arrangements",
    "section": "Class 15: Endogeneity",
    "text": "Class 15: Endogeneity\n\nWhat you will learn\n\nUnderstand the reasoning why linear regression can almost never provide causal effects from non-experimental data.\nUnderstand the concept of endogeneity and its causes.\n\nAfter-class reading and exercise\n\nA complete guide to Marketing Mix Modeling",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-16-instrumental-variables-and-two-stage-least-square",
    "href": "WeeklyArrangements.html#class-16-instrumental-variables-and-two-stage-least-square",
    "title": "Weekly Arrangements",
    "section": "Class 16: Instrumental Variables and Two-Stage Least Square",
    "text": "Class 16: Instrumental Variables and Two-Stage Least Square\n\nWhat you will learn\n\nIntuition of why instrumental variables solve endogeneity problems\nThe requirements of a valid instrumental variable and how to find good instruments\nApply two-stage least square method to estimate the causal effects using instrumental variables\n\nAfter-class reading and exercise\n\n(optional) Econometrics with R: Instrumental Variables Regression",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-17-case-study-estimating-causal-effects-for-platform-businesses-using-instrumental-variables",
    "href": "WeeklyArrangements.html#class-17-case-study-estimating-causal-effects-for-platform-businesses-using-instrumental-variables",
    "title": "Weekly Arrangements",
    "section": "Class 17: (Case Study) Estimating Causal Effects for Platform Businesses Using Instrumental Variables",
    "text": "Class 17: (Case Study) Estimating Causal Effects for Platform Businesses Using Instrumental Variables\n\nWhat you will learn\n\nUnderstand the importance of causal inference for platform businesses\nLearn how to estimate causal effects using instrumental variables with an application to ride-sharing platforms\n\n(highly recommended) Encouragement Designs and Instrumental Variables for A/B Testing at Spotify",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-18-natural-experiment-i-regression-discontinuity-design",
    "href": "WeeklyArrangements.html#class-18-natural-experiment-i-regression-discontinuity-design",
    "title": "Weekly Arrangements",
    "section": "Class 18: Natural Experiment I: Regression Discontinuity Design",
    "text": "Class 18: Natural Experiment I: Regression Discontinuity Design\n\nWhat you will learn\n\nConcept of regression discontinuity design\nEstimation of causal effects using regression discontinuity design\nApplication of regression discontinuity design in the business field\n\nAfter-class reading and exercise\n\n(optional) Econometrics with R: Quasi-experiments",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-19-natural-experiment-ii-difference-in-differences-design",
    "href": "WeeklyArrangements.html#class-19-natural-experiment-ii-difference-in-differences-design",
    "title": "Weekly Arrangements",
    "section": "Class 19: Natural Experiment II: Difference-in-Differences Design",
    "text": "Class 19: Natural Experiment II: Difference-in-Differences Design\n\nWhat you will learn\n\nConcept of difference-in-differences (DiD) design\nEstimation of causal effects using the DiD design\nApplication of DiD design in the business field",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "WeeklyArrangements.html#class-20-frontiers-of-marketing-analytics",
    "href": "WeeklyArrangements.html#class-20-frontiers-of-marketing-analytics",
    "title": "Weekly Arrangements",
    "section": "Class 20: Frontiers of Marketing Analytics",
    "text": "Class 20: Frontiers of Marketing Analytics\n\nWhat you will learn\n\nCausal machine learning with causal forest\nHeterogeneous treatment effect estimation with causal forest in R\n\nAfter-class reading and exercise\n\nEstimate causal effects using ML by Microsoft Research\nAthey, Susan, and Stefan Wager. ‘Estimating Treatment Effects with Causal Forests: An Application’. ArXiv:1902.07409 [Stat], 20 February 2019. http://arxiv.org/abs/1902.07409.",
    "crumbs": [
      "Lectures",
      "Weekly Arrangements"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsI.html",
    "href": "Case-PredictiveAnalyticsI.html",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (I): Customer Segmentation Using K-Means Clustering",
    "section": "",
    "text": "Machine learning (ML) refers to the study of methods or algorithms designed to learn the underlying patterns in the data and make predictions based on these patterns. A key characteristic of predictive analytics techniques is their ability to produce accurate out-of-sample predictions. Consider the problem of predicting whether a user will click on an ad. We do not have a comprehensive theory of users’ clicking behavior. Predictive analytics methods can automatically learn which of these factors affect user behavior and how they interact with each other, potentially in a highly non-linear fashion, to derive the best functional form that explains user behavior virtually in real time. Predictive analytics methods typically assume a model or structure to learn, but they use a general class of models that can be very rich.\nPredictive analytics models can be divided into two major groups: supervised learning and unsupervised learning.\nSupervised learning requires input data that has both predictor (independent) variables and a target (dependent) variable whose value is to be estimated. If the goal of an analysis is to predict the value of some target variable (e.g., whether customer responds to our marketing offers; whether customers churn at some point in time), then supervised learning is used.\nOn the other hand, unsupervised learning does not identify a target variable, but rather treats all of the variables equally as inputs. In this case, the goal is not to predict the value of a variable, but rather to look for patterns, groupings, or other ways to characterize the data that may lead to an understanding of the way the data interrelate. Clustering analysis is an example of unsupervised learning, which helps data analysts find customer segments based on provided characteristics.\nIn this case study, we are going to analyze the same dataset as in “Descriptive Analytics for M&S”. Our task is to use predictive analytics tools to help M&S conduct more effective targeted marketing.\nAs a quick recap, the variable definitions are as follows,\nDemographic Variables\n\nID: Customer’s unique identifier\nYear_Birth: Customer’s birth year\nEducation: Customer’s education level\nMarital_Status: Customer’s marital status\nIncome: Customer’s yearly household income\nKidhome: Number of children in customer’s household\nTeenhome: Number of teenagers in customer’s household\nDt_Customer: Date of customer’s enrollment with the company\n\nCustomer Purchase History Data\n\nID: Customer’s unique identifier\nMntWines: Amount spent on wine in last 2 years\nMntFruits: Amount spent on fruits in last 2 years\nMntMeatProducts: Amount spent on meat in last 2 years\nMntFishProducts: Amount spent on fish in last 2 years\nMntSweetProducts: Amount spent on sweets in last 2 years\nNumDealsPurchases: Number of purchases made with a discount\nNumWebPurchases: Number of purchases made through the company’s web site\nNumCatalogPurchases: Number of purchases made using a catalogue\nNumStorePurchases: Number of purchases made directly in stores\nNumWebVisitsMonth: Number of visits to company’s web site in the last month\nComplain: 1 if customer complained in the last 2 years, 0 otherwise\nResponse: 1 if customer accepted the offer in the last campaign, 0 otherwise\nRecency: Number of days since customer’s last purchase",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsI.html#data-collection",
    "href": "Case-PredictiveAnalyticsI.html#data-collection",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (I): Customer Segmentation Using K-Means Clustering",
    "section": "2.1 Data Collection",
    "text": "2.1 Data Collection\nPlease load the M&S dataset from your local machine. You can use the “import dataset” button in RStudio to upload the dataset. The dataset is named data_full.csv. Name the dataset data_full.\n\nQuestion 1Answer\n\n\nload data_full, create total_spending, and select total_spending and Income as the clustering variables into a new data frame data_kmeans.\n\n\n\n\nCode\npacman::p_load(dplyr, ggplot2, ggthemes, broom)\ndata_full &lt;- read.csv(\"images/data_full.csv\") %&gt;%\n    mutate(total_spending = MntWines + MntFruits + MntMeatProducts + MntFishProducts + MntSweetProducts + MntGoldProds)\n\ndata_kmeans &lt;- data_full %&gt;%\n    select(total_spending, Income)",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsI.html#data-preprocessing",
    "href": "Case-PredictiveAnalyticsI.html#data-preprocessing",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (I): Customer Segmentation Using K-Means Clustering",
    "section": "2.2 Data Preprocessing",
    "text": "2.2 Data Preprocessing\nFirst, we need to check missing values and resolve them as the k-means algorithm cannot handle missing values directly.\n\nQuestion 2Answer\n\n\nWe find that the Income variable has missing values. We can replace the missing values with the mean of the Income variable.\n\n\n\n\nCode\ndata_kmeans &lt;- data_kmeans %&gt;%\n    mutate(Income = ifelse(is.na(Income), mean(Income, na.rm = TRUE), Income))\n\n\n\n\n\nNext, we need to re-scale the two variables using scale(), because the two variables are of very different scales.\n\nQuestion 3Answer\n\n\nScale the variables and create a new data frame data_kmeans_scaled.\n\n\n\n\nCode\n# method 1\ndata_kmeans_scaled &lt;- data_kmeans %&gt;%\n    select(total_spending, Income) %&gt;%\n    mutate(\n        total_spending = scale(total_spending),\n        Income = scale(Income)\n    )\n\n# method 2: using across when there are many variables with the same transformation\ndata_kmeans_scaled &lt;- data_kmeans %&gt;%\n    select(total_spending, Income) %&gt;%\n    mutate(across(everything(), scale))",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsI.html#data-analytics",
    "href": "Case-PredictiveAnalyticsI.html#data-analytics",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (I): Customer Segmentation Using K-Means Clustering",
    "section": "2.3 Data Analytics",
    "text": "2.3 Data Analytics\n\nQuestion 4Answer\n\n\n\nApply K-Means Clustering with 2 Clusters\n\n\n\n\n\nCode\nset.seed(888)\nresult_kmeans &lt;- kmeans(data_kmeans_scaled,\n    centers = 2,\n    nstart = 10\n)\n\n\n\n\n\n\nQuestion 5Answer\n\n\n\nWe can examine the structure of the result_kmeans using tidy()\n\n\n\n\n\nCode\nresult_kmeans %&gt;%\n    tidy()\n\n\n\n  \n\n\n\n\n\n\n\nQuestion 6Answer\n\n\n\nVisualize the clustering\n\n\n\n\n\nCode\npacman::p_load(cluster, factoextra)\nset.seed(888)\nfviz_cluster(result_kmeans,\n    data = data_kmeans\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7Answer\n\n\n\nDetermine the optimal number of clusters using statistical criteria\n\n\n\nGap Method\n\n\nCode\nset.seed(888)\ngap_stat &lt;- clusGap(data_kmeans_scaled,\n    FUN = kmeans,\n    K.max = 10,\n    B = 50\n)\n\n\nWarning: did not converge in 10 iterations\n\n\nCode\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\n\nSilhouette method\n\n\nCode\nset.seed(888)\nfviz_nbclust(data_kmeans_scaled, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\n\nFrom both methods, it seems K = 2 is the best choice.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsI.html#business-recommendations",
    "href": "Case-PredictiveAnalyticsI.html#business-recommendations",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (I): Customer Segmentation Using K-Means Clustering",
    "section": "2.4 Business Recommendations",
    "text": "2.4 Business Recommendations\nAfter segmenting the customers into two groups, we can now analyze the two segments to understand their characteristics and behaviors For example, we can compare the average response rates to marketing offers in the two segments, and decide which segment to serve when launching the next marketing compagine to save marketing costs.\nAs we would like to save marketing costs by sending marketing offers to responsive customers, we need to compute the average response rate for each segment generated by the K-means algorithm\n\nGenerate the customer segment in data_full\n\n\n\nCode\ndata_full &lt;- data_full %&gt;%\n    mutate(segment = result_kmeans$cluster)\n\n\n\nCompute the average response rate for each segment\n\n\n\nCode\ndata_full %&gt;%\n    group_by(segment) %&gt;%\n    summarise(avg_response_rate = mean(Response, na.rm = T)) %&gt;%\n    ungroup()\n\n\n\n  \n\n\n\nWe observe that Segment 1 has a much higher average response rate of more than 20%, so we should send marketing offers to Segment 1 customers.",
    "crumbs": [
      "Lectures",
      "[Week 4] Unsupervised Learning for Customer Segmentation",
      "Case Study: Customer Segmentation Using K-Means for M&S"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html",
    "href": "Week7-Lecture1.html",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "",
    "text": "Instagram aims to increase user engagement and activity.\nWe can propose gamification strategies based on scientific theories.\nNeed to empirically test whether proposed gamification strategies are effective using A/B/N testings.\n\n\n\n\nConduct a situation analysis to assess Instagram’s business environment in the UK:1\n\nWhat is Instagram’s business model?\nHow does Instagram make revenues?\nWho are Instagram’s customers?\nWhat are the major competitors and their relative strengths and weaknesses compared with Instagram?\nWho are the collaborators of Instagram?\nPESTLE analysis: any particular legal and regulatory issues that Instagram needs to be aware of?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#business-objective",
    "href": "Week7-Lecture1.html#business-objective",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "",
    "text": "Instagram aims to increase user engagement and activity.\nWe can propose gamification strategies based on scientific theories.\nNeed to empirically test whether proposed gamification strategies are effective using A/B/N testings.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#situation-analysis",
    "href": "Week7-Lecture1.html#situation-analysis",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "",
    "text": "Conduct a situation analysis to assess Instagram’s business environment in the UK:1\n\nWhat is Instagram’s business model?\nHow does Instagram make revenues?\nWho are Instagram’s customers?\nWhat are the major competitors and their relative strengths and weaknesses compared with Instagram?\nWho are the collaborators of Instagram?\nPESTLE analysis: any particular legal and regulatory issues that Instagram needs to be aware of?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#theoretical-motivation-for-business-ideas",
    "href": "Week7-Lecture1.html#theoretical-motivation-for-business-ideas",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.1 Theoretical Motivation for Business Ideas",
    "text": "2.1 Theoretical Motivation for Business Ideas\n\nWhen proposing business ideas, we should base our proposals on scientific, well-established theories from different disciplines.\n\nBottom-up approach: start with theories and then generate business ideas\nTop-down approach: start with business ideas and then find theories to support them\n\nLet’s first see some examples of behavioral economics theories!",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#behavioral-theories",
    "href": "Week7-Lecture1.html#behavioral-theories",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.2 Behavioral Theories",
    "text": "2.2 Behavioral Theories\n\nFraming effect\nEndowment effect\nMental accounting",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#default-effect",
    "href": "Week7-Lecture1.html#default-effect",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.3 Default Effect",
    "text": "2.3 Default Effect\n\nDefault effect",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#left-digit-bias",
    "href": "Week7-Lecture1.html#left-digit-bias",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.4 Left-Digit Bias",
    "text": "2.4 Left-Digit Bias\n\nLeft-digit bias",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#social-comparison-theory",
    "href": "Week7-Lecture1.html#social-comparison-theory",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.5 Social Comparison Theory",
    "text": "2.5 Social Comparison Theory\n\nPeople evaluate their own opinions and abilities by comparing themselves to others, especially when comparing oneself to similar others.\nSocial comparison can be upward or downward.\nSocial comparison can motivate people to improve their performance; however, it can also lead to negative emotions.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#prospect-theory",
    "href": "Week7-Lecture1.html#prospect-theory",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.6 Prospect Theory",
    "text": "2.6 Prospect Theory\n\nProspect theory posits that people feel more pain from losing something than pleasure from gaining something.\nThis theory can be used to explain why people are more likely to engage in activities that prevent loss than those that promote gain.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#business-proposal",
    "href": "Week7-Lecture1.html#business-proposal",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.7 Business Proposal",
    "text": "2.7 Business Proposal\n\nImplement gamification features on Instagram to increase user activity based on the theories of Social Comparison and Prospect Theory.\nLet’s think about ideas that can boost user engagement on Instagram.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#potential-strategies",
    "href": "Week7-Lecture1.html#potential-strategies",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.8 Potential Strategies",
    "text": "2.8 Potential Strategies\n\nEndowment effect: Implementing a points and badge system to create sense of ownership and encourage engagement (e.g., likes, comments, shares).\nSocial comparison theory: Leaderboards showing top users; Social comparison through activity rankings\nProspect theory: Time-limited rewards and achievements",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#step-1-decide-on-the-unit-of-randomization",
    "href": "Week7-Lecture1.html#step-1-decide-on-the-unit-of-randomization",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.1 Step 1: Decide on the Unit of Randomization",
    "text": "3.1 Step 1: Decide on the Unit of Randomization\n\nWhat would be the best unit of randomization?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#step-2-mitigate-spillover-and-crossover-effects",
    "href": "Week7-Lecture1.html#step-2-mitigate-spillover-and-crossover-effects",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.2 Step 2: Mitigate Spillover and Crossover Effects",
    "text": "3.2 Step 2: Mitigate Spillover and Crossover Effects\n\nWhat are the potential problems for spillover and crossover?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#step-3-decide-on-randomization-allocation-scheme",
    "href": "Week7-Lecture1.html#step-3-decide-on-randomization-allocation-scheme",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.3 Step 3: Decide on Randomization Allocation Scheme",
    "text": "3.3 Step 3: Decide on Randomization Allocation Scheme\n\nHow should we determine the randomization scheme?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#step-4-collect-data",
    "href": "Week7-Lecture1.html#step-4-collect-data",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.4 Step 4: Collect Data",
    "text": "3.4 Step 4: Collect Data\n\nWhat is the sample size we need?\nWhat data should we collect?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#step-5-data-analytics",
    "href": "Week7-Lecture1.html#step-5-data-analytics",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.5 Step 5: Data analytics",
    "text": "3.5 Step 5: Data analytics\n\nRandomization checks\nHow to estimate the treatment effects?",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#after-class",
    "href": "Week7-Lecture1.html#after-class",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.6 After-Class",
    "text": "3.6 After-Class\n\n(optional) Test and learn: How a culture of experimentation can help grow your business",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture1.html#footnotes",
    "href": "Week7-Lecture1.html#footnotes",
    "title": "Class 13 Case Study: Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuggested answers are on the course website.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 1: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html",
    "href": "Week7-Lecture2.html",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "",
    "text": "A simple linear regression is a model as follows. \\[\ny_i = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k + \\epsilon_i\n\\]\n\\(y_i\\): Dependent variable/outcome variable\n\\(x_k\\): Independent variable/explanatory variable/control variable\n\\(\\beta\\): Regression coefficients; \\(\\beta_0\\): intercept (should always be included)\n\\(\\epsilon_i\\): Error term, which captures the deviation of Y from the line. Expected mean should be 0, i.e., \\(E[\\epsilon|X]=0\\)\n\n\n\n\n\nIf we take the expectation of Y, we should have \\[\nE[Y|X] = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe term “regression” was first coined by Francis Galton to describe a biological phenomenon: The heights of descendants of tall ancestors tend to regress down towards a normal average.\nThe term “regression” was later extended by statisticians Udny Yule and Karl Pearson to a more general statistical context (Pearson, 1903).\nIn supervised learning models, “regression” has a different meaning: when the outcome variable to be predicted is continuous, the task is called a regression task. This is because ML models are developed by computer science; causal inference models are developed by statisticians and economists.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#linear-regression-models",
    "href": "Week7-Lecture2.html#linear-regression-models",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "",
    "text": "A simple linear regression is a model as follows. \\[\ny_i = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k + \\epsilon_i\n\\]\n\\(y_i\\): Dependent variable/outcome variable\n\\(x_k\\): Independent variable/explanatory variable/control variable\n\\(\\beta\\): Regression coefficients; \\(\\beta_0\\): intercept (should always be included)\n\\(\\epsilon_i\\): Error term, which captures the deviation of Y from the line. Expected mean should be 0, i.e., \\(E[\\epsilon|X]=0\\)",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#linear-regression-models-1",
    "href": "Week7-Lecture2.html#linear-regression-models-1",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "",
    "text": "If we take the expectation of Y, we should have \\[\nE[Y|X] = \\beta_0 + x_1 \\beta_1 + x_2\\beta_2+ \\ldots + x_k\\beta_k\n\\]",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#origin-of-the-name-regression",
    "href": "Week7-Lecture2.html#origin-of-the-name-regression",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "",
    "text": "The term “regression” was first coined by Francis Galton to describe a biological phenomenon: The heights of descendants of tall ancestors tend to regress down towards a normal average.\nThe term “regression” was later extended by statisticians Udny Yule and Karl Pearson to a more general statistical context (Pearson, 1903).\nIn supervised learning models, “regression” has a different meaning: when the outcome variable to be predicted is continuous, the task is called a regression task. This is because ML models are developed by computer science; causal inference models are developed by statisticians and economists.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#how-to-run-regression-in-r",
    "href": "Week7-Lecture2.html#how-to-run-regression-in-r",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "2.1 How to Run Regression in R",
    "text": "2.1 How to Run Regression in R\n\nIn R, there are many packages that can run OLS regression. The basic function is lm().\nIn this module, we will be using the fixest package, because it’s able to accommodate more complex regressions, especially high-dimensional fixed effects.1\n\n\n\nCode\npacman::p_load(modelsummary, fixest)\n\nOLS_result &lt;- feols(\n    fml = total_spending ~ Income, # Y ~ X\n    data = data_full, # dataset from M&S\n)",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#report-regression-results",
    "href": "Week7-Lecture2.html#report-regression-results",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "2.2 Report Regression Results",
    "text": "2.2 Report Regression Results\n\n\nCode\nmodelsummary(OLS_result,\n    stars = TRUE # export statistical significance\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -556.823***\n                \n                \n                             \n                  (21.654)   \n                \n                \n                  Income     \n                  0.022***   \n                \n                \n                             \n                  (0.000)    \n                \n                \n                  Num.Obs.   \n                  2000       \n                \n                \n                  R2         \n                  0.629      \n                \n                \n                  R2 Adj.    \n                  0.629      \n                \n                \n                  AIC        \n                  29306.1    \n                \n                \n                  BIC        \n                  29317.3    \n                \n                \n                  RMSE       \n                  367.45     \n                \n                \n                  Std.Errors \n                  IID",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#parameter-estimation-univariate-regression-case",
    "href": "Week7-Lecture2.html#parameter-estimation-univariate-regression-case",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "2.3 Parameter Estimation: Univariate Regression Case",
    "text": "2.3 Parameter Estimation: Univariate Regression Case\n\nRegressions with a single regressor are called univariate regressions. Let’s take a univariate regression as an example:\n\n\\[\ntotal\\_spending = a + b \\cdot income  + \\epsilon\n\\]\n\nFor each guess of a and b, we can compute the error for customer \\(i\\),\n\n\\[\ne_i = total\\_spending_{i}-a-b \\cdot income_{i}\n\\]\n\nWe can compute the sum of squared residuals (SSR) across all customers\n\n\\[\nSSR =\\sum_{i=1}^{n}\\left(total\\_spending_{i}-a-b \\cdot income_{i}\\right)^{2}\n\\]\n\nObjective of estimation: Search for the unique set of \\(a\\) and \\(b\\) that can minimize the SSR.\nThis estimation method that minimizes SSR is called Ordinary Least Square (OLS).",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#visualization-estimation-of-univariate-regression",
    "href": "Week7-Lecture2.html#visualization-estimation-of-univariate-regression",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "2.4 Visualization: Estimation of Univariate Regression",
    "text": "2.4 Visualization: Estimation of Univariate Regression\n\nIf in the M&S dataset, if we regress total spending (Y) on income (X)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nColor\nSum of Squared Error\n\n\n\n\n\\(Y = -556.823 + 0.06 * X\\)\nPurple\n1.5403487^{13}\n\n\n\\(Y = 0 + 0.004 * X\\)\nRed\n6.420375^{11}\n\n\n\\(Y = -556.823 + 0.022 * X\\)\nGreen\n1.4356017^{9}",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#multivariate-regression",
    "href": "Week7-Lecture2.html#multivariate-regression",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "2.5 Multivariate Regression",
    "text": "2.5 Multivariate Regression\n\nThe OLS estimation also applies to multivariate regression with multiple regressors.\n\n\\[\ny_i = b_0 + b_1 x_{1} + ... + b_k x_{k}+\\epsilon_i\n\\]\n\nObjective of estimation: Search for the unique set of \\(b\\) that can minimize the sum of squared residuals.\n\n\\[\nSSR= \\sum_{i=1}^{n}\\left(y_{i}-b_0 - b_1 x_{1} - ... - b_k x_{k} \\right)^{2}\n\\]",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#coefficients-interpretation",
    "href": "Week7-Lecture2.html#coefficients-interpretation",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "3.1 Coefficients Interpretation",
    "text": "3.1 Coefficients Interpretation\n\nNow on your Quarto document, let’s run a new regression, where the DV is \\(total\\_spending\\), and X includes \\(Income\\) and \\(Kidhome\\).\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -299.119***\n                \n                \n                             \n                  (28.069)   \n                \n                \n                  Income     \n                  0.019***   \n                \n                \n                             \n                  (0.000)    \n                \n                \n                  Kidhome    \n                  -230.610***\n                \n                \n                             \n                  (16.945)   \n                \n                \n                  Num.Obs.   \n                  2000       \n                \n                \n                  R2         \n                  0.661      \n                \n                \n                  R2 Adj.    \n                  0.660      \n                \n                \n                  AIC        \n                  29130.7    \n                \n                \n                  BIC        \n                  29147.5    \n                \n                \n                  RMSE       \n                  351.51     \n                \n                \n                  Std.Errors \n                  IID        \n                \n        \n      \n    \n\n\n\n\nControlling for Kidhome, one unit increase in Income increases totalspending by £0.019.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#standard-errors-and-p-values",
    "href": "Week7-Lecture2.html#standard-errors-and-p-values",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "3.2 Standard Errors and P-Values",
    "text": "3.2 Standard Errors and P-Values\n\nIf we collect all data from the whole population, the regression coefficient is called the population regression coefficient.\nBecause the regression is estimated on a random sample of the population, if we rerun the regression on different samples from the same population, we would obtain a different set of sample regression coefficients each time.\nIn theory, the sample regression coefficients estimates follows a t-distribution: the mean is the true \\(\\beta\\). The standard error of the estimates is the estimated standard deviation of the error.\nKnowing that the coefficients follow a t-distribution, we can test whether the coefficients are statistically different from 0 using hypothesis testing.\nIncome/Kidhome is statistically significant at the 1% level.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#r-squared",
    "href": "Week7-Lecture2.html#r-squared",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "3.3 R-Squared",
    "text": "3.3 R-Squared\n\nR-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by all included variables in a regression.\nInterpretation: 66% of the variation in total_spending can be explained by Income and Kidhome.\nAs the number of variables increases, the \\(R^2\\) will naturally increase, so sometimes we may need to penalize the number of variables using the so-called adjusted R-squared.\n\n\n\n\n\n\n\nImportant\n\n\n\nR-Squared is only important for supervised learning prediction tasks, because it measures the predictive power of the X. However, in causal inference tasks, \\(R^2\\) does not matter much.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#categorical-variables",
    "href": "Week7-Lecture2.html#categorical-variables",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.1 Categorical variables",
    "text": "4.1 Categorical variables\n\n\nCode\npacman::p_load(dplyr,ggplot2,ggthemes)\n# Load both datasets\ndata_full &lt;- read.csv(file = \"https://www.dropbox.com/scl/fi/hhweiqsuwgcwgd1jiuyte/data_full.csv?rlkey=jwyd9z409b5wpwz41ow8d1otj&dl=1\", \n                      header = T)\n\n\n\nSo far, the independent variables we have used are Income and Kidhome, which are continuous variables.\nSome variables are intrinsically not countable; we need to treat them as categorical variables, e.g., gender, education group, city.\nIn A/B/N testings, the treatment assignment is also a categorical variable.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#handling-categorical-variables-in-r-using-factor",
    "href": "Week7-Lecture2.html#handling-categorical-variables-in-r-using-factor",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.2 Handling Categorical Variables in R using factor()",
    "text": "4.2 Handling Categorical Variables in R using factor()\n\nIn R, we need to use a function factor() to explicitly inform R that this variable is a categorical variable, such that statistical models will treat them differently from continuous variables.\n\ne.g., we can use factor(Education) to indicate that, Education is a categorical variable.\n\n\n\n\nCode\ndata_full &lt;- data_full %&gt;%\n  mutate(Education_factor = factor(Education))\n\n\n\nWe can use levels() to check how many categories there are in the factor variable.\n\ne.g., Education has 5 different levels.\n\n\n\n\nCode\n# check levels of a factor\nlevels(data_full$Education_factor)\n\n\n[1] \"2n Cycle\"   \"Basic\"      \"Graduation\" \"Master\"     \"PhD\"",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#handling-categorical-variables-using-factor",
    "href": "Week7-Lecture2.html#handling-categorical-variables-using-factor",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.3 Handling Categorical Variables using factor()",
    "text": "4.3 Handling Categorical Variables using factor()\n\nfactor() will check all levels of the categorical variables, and then choose the default level based on alphabetic order.\nIf needed, we can revise the baseline group to another group using relevel() function.\n\n\n\nCode\n# Create a new factor variable, with Basic as the baseline.\ndata_full &lt;- data_full %&gt;%\n    mutate(Education_factor_2 = relevel(Education_factor,\n        ref = \"Basic\"\n    ))\n\nlevels(data_full$Education_factor_2)\n\n\n[1] \"Basic\"      \"2n Cycle\"   \"Graduation\" \"Master\"     \"PhD\"",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#running-regression-with-factor-variables",
    "href": "Week7-Lecture2.html#running-regression-with-factor-variables",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.4 Running Regression with Factor Variables",
    "text": "4.4 Running Regression with Factor Variables\n\n\nCode\npacman::p_load(fixest, modelsummary)\n\nfeols_categorical &lt;- feols(\n    data = data_full,\n    fml = total_spending ~ Income + Kidhome + Education_factor_2\n)\n\nmodelsummary(feols_categorical,\n    stars = T,\n    gof_map = c(\"nobs\", \"r.squared\"))",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#interpretation-of-coefficients-for-categorical-variables",
    "href": "Week7-Lecture2.html#interpretation-of-coefficients-for-categorical-variables",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.5 Interpretation of Coefficients for Categorical Variables",
    "text": "4.5 Interpretation of Coefficients for Categorical Variables\n\nIn general, R encode factor variables with K levels into K-1 coefficients, with one level as the baseline group.\nThe interpretation of coefficients for factor variables: Ceteris paribus, compared with the [baseline group], the [outcome variable] of [group X] is higher/lower by [coefficient], and the coefficient is statistically [significant/insignificant].\n\nCeteris paribus, compared with the basic education group, the total spending of PhD group is lower by 153.190 dollars. The coefficient is statistically significant at the 1% level.\n\nNow please rerun the regression using Education_factor and interpret the coefficients. What’s your finding?\n\nConclusion: factor variables can only measure the relative difference in the outcome variable across different groups rather than telling us about the absolute levels of each group.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#application-of-categorical-variables-in-marketing",
    "href": "Week7-Lecture2.html#application-of-categorical-variables-in-marketing",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.6 Application of Categorical Variables in Marketing",
    "text": "4.6 Application of Categorical Variables in Marketing\n\nQuantify the treatment effects in A/B/N testing, where \\(Treatment_i\\) is a categorical variable that specifies the treatment group customer \\(i\\) is in:\n\n\\[\nOutcome_i = \\beta_0 + \\delta Treatment_i + \\epsilon\n\\]\n\nQuantify the brand premiums or country-of-origin effects:\n\n\\[\nSales_i = \\beta_0 + \\beta_1 Brand_i + \\beta_2 Country_i + X\\beta +\\epsilon\n\\]",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#application-abn-testing-analysis-using-regression",
    "href": "Week7-Lecture2.html#application-abn-testing-analysis-using-regression",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "4.7 Application: A/B/N Testing Analysis Using Regression",
    "text": "4.7 Application: A/B/N Testing Analysis Using Regression\n\nLet’s analyze our Instagram gamification experiment data using linear regression.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week7-Lecture2.html#footnotes",
    "href": "Week7-Lecture2.html#footnotes",
    "title": "Class 14 Linear Regression for Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFixed effects are a type of control variable that is constant within a group, such as country, year, or individual, to control for unobserved heterogeneity. See this link.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Lecture 2: Linear Regression for Causal Inference"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html",
    "href": "Week6-Lecture1.html",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "Any business activity brings benefits and costs. We’re given the benefit information in all the case studies so far\n\nApple (Week 1): influencer marketing increases sales by 2.5%\n1st assignment: loyalty program increases retention rate to 95%\n\nIn reality, this benefit information is often not readily available, and we need to measure them using causal inference tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstand key concepts of causal inference\n\nRubin’s potential outcome framework\nFundamental problem of causal inference\nAverage treatment effect (ATE) and randomization\n\nLearn the steps to conduct randomized controlled trials (RCTs)\n\n\n\n\nTom purchases paid ads on Instagram to advertise his new bubble tea shop. Instagram ads are targeted to individuals who are predicted to have a higher likelihood of being bubble tea lovers. In the end, some Instagram users saw no ads and some saw the ads. The purchase rates for each group are shown below.\n\n\n\n\n\n\n\n\n\nQuestion: Can Tom be confident to conclude the Instagram ads are effective in converting new customers?\n\n\n\nTom bought a marketing survey data from a consulting agency. The survey collected prices and store visits (sales) for different bubble tea shops in Canary Wharf. Tom finds that there seems to be a positive relationship between prices and store visits.\n\n\n\n\n\n\n\n\n\nQuestion: Can Tom conclude that he should also increase the prices for his bubble tea shop to increase the store visits?\n\n\n\nThis is a fighter plane that just returned from the battlefield. Red dots are bullet holes.\nWhich part of A, B, and C would you reinforce to increase the pilot’s survival rate?\n\n\n\n\n\n\n\n\n\n\n\n\nI have held a secret from you for a long time, it’s time for me to confess ….\n\n\n\n\n\n\n\n\n\n\n\n\n\n[…] the other half jointly to Joshua D. Angrist and Guido W. Imbens “for their methodological contributions to the analysis of causal relationships.”\n\n\n\n\n\nCausal inference is the process of estimating the unbiased causal effects of a particular policy/business intervention on the outcome variables of interest.\nCorrelation != Causation: Machine learning models are good at finding correlations, but not causations. For example, on rainy days, we observe more umbrellas on the street\n\ncorrect correlation statement: number of umbrellas is positively correlated with rainfall\ncorrect causal statement: heavier rain leads to more umbrellas\nincorrect causal statement: more umbrellas lead to heavier rain\n\nCausality becomes more complex in the business world. Managers can easily make mistakes without causal inference training.\n\nImagine if the actual causal effect of Instagram ads on incremental profit per customer is £1 for Tom, and Tom pays £1.5 for each click",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#our-journey-so-far",
    "href": "Week6-Lecture1.html#our-journey-so-far",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "Any business activity brings benefits and costs. We’re given the benefit information in all the case studies so far\n\nApple (Week 1): influencer marketing increases sales by 2.5%\n1st assignment: loyalty program increases retention rate to 95%\n\nIn reality, this benefit information is often not readily available, and we need to measure them using causal inference tools.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#learning-objectives",
    "href": "Week6-Lecture1.html#learning-objectives",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "Understand key concepts of causal inference\n\nRubin’s potential outcome framework\nFundamental problem of causal inference\nAverage treatment effect (ATE) and randomization\n\nLearn the steps to conduct randomized controlled trials (RCTs)",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#why-causal-inference-matters-example-1",
    "href": "Week6-Lecture1.html#why-causal-inference-matters-example-1",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "Tom purchases paid ads on Instagram to advertise his new bubble tea shop. Instagram ads are targeted to individuals who are predicted to have a higher likelihood of being bubble tea lovers. In the end, some Instagram users saw no ads and some saw the ads. The purchase rates for each group are shown below.\n\n\n\n\n\n\n\n\n\nQuestion: Can Tom be confident to conclude the Instagram ads are effective in converting new customers?",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#why-causal-inference-matters-example-2",
    "href": "Week6-Lecture1.html#why-causal-inference-matters-example-2",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "Tom bought a marketing survey data from a consulting agency. The survey collected prices and store visits (sales) for different bubble tea shops in Canary Wharf. Tom finds that there seems to be a positive relationship between prices and store visits.\n\n\n\n\n\n\n\n\n\nQuestion: Can Tom conclude that he should also increase the prices for his bubble tea shop to increase the store visits?",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#why-causal-inference-matters-example-3",
    "href": "Week6-Lecture1.html#why-causal-inference-matters-example-3",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "This is a fighter plane that just returned from the battlefield. Red dots are bullet holes.\nWhich part of A, B, and C would you reinforce to increase the pilot’s survival rate?",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#why-causal-inference-matters-example-4",
    "href": "Week6-Lecture1.html#why-causal-inference-matters-example-4",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "I have held a secret from you for a long time, it’s time for me to confess ….",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#nobel-prize-in-economics-2021",
    "href": "Week6-Lecture1.html#nobel-prize-in-economics-2021",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "[…] the other half jointly to Joshua D. Angrist and Guido W. Imbens “for their methodological contributions to the analysis of causal relationships.”",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#causal-inference-1",
    "href": "Week6-Lecture1.html#causal-inference-1",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "",
    "text": "Causal inference is the process of estimating the unbiased causal effects of a particular policy/business intervention on the outcome variables of interest.\nCorrelation != Causation: Machine learning models are good at finding correlations, but not causations. For example, on rainy days, we observe more umbrellas on the street\n\ncorrect correlation statement: number of umbrellas is positively correlated with rainfall\ncorrect causal statement: heavier rain leads to more umbrellas\nincorrect causal statement: more umbrellas lead to heavier rain\n\nCausality becomes more complex in the business world. Managers can easily make mistakes without causal inference training.\n\nImagine if the actual causal effect of Instagram ads on incremental profit per customer is £1 for Tom, and Tom pays £1.5 for each click",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#rubin-causal-model-and-the-potential-outcome-framework",
    "href": "Week6-Lecture1.html#rubin-causal-model-and-the-potential-outcome-framework",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.1 Rubin Causal Model and the Potential Outcome Framework",
    "text": "2.1 Rubin Causal Model and the Potential Outcome Framework\n\nThe Rubin causal model (RCM) or the Potential Outcome Framework is the well accepted framework for thinking about causal effects.\nFor each customer \\(i\\), we can define the potential outcomes in order to evaluate the causal effect of a treatment on their outcomes:\n\n\\(Y^{1}_i\\): the outcome if the customer is exposed to the treatment, ceteris paribus\n\\(Y^{0}_i\\): the outcome if the customer is not exposed to the treatment, ceteris paribus",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#definition-individual-treatment-effects",
    "href": "Week6-Lecture1.html#definition-individual-treatment-effects",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.2 Definition: Individual Treatment Effects",
    "text": "2.2 Definition: Individual Treatment Effects\n\nThe causal effect of the treatment on the individual \\(i\\) is the difference between the two potential outcomes. We define the individual treatment effect \\(\\delta_i\\) as follows\n\n\\[\n        \\delta_i = Y^1_i - Y^0_i\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nThese two potential outcomes do not depend on whether or not the individual is exposed to the treatment in reality.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#example",
    "href": "Week6-Lecture1.html#example",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.3 Example",
    "text": "2.3 Example\nTom would like to measure the causal effect of seeing a displayed ad on customer purchase probability.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#examples-of-individual-treatment-effects",
    "href": "Week6-Lecture1.html#examples-of-individual-treatment-effects",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.4 Examples of Individual Treatment Effects",
    "text": "2.4 Examples of Individual Treatment Effects\nLet’s say we have retrieved all infinity stones from Thanos, and have created 2 parallel universes\n\nIn Universe 1, with a displayed ad\n\nDr Strange has a rate of 70%\n\nIn Universe 2, without a displayed ad\n\nDr Strange has a rate of 60%\n\n\n\n\n\n\n\nCustomer\nY1\nY0\nTE\n\n\n\n\nDr Strange\n0.7\n0.6\n0.1",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#a-motivating-example-a-group-of-customers",
    "href": "Week6-Lecture1.html#a-motivating-example-a-group-of-customers",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.5 A Motivating Example: A Group of Customers",
    "text": "2.5 A Motivating Example: A Group of Customers\n\nConceptually, we can collect a sample of customers, and estimate individual treatment effect for each of them\n\n\n\n\n\n\nCustomer\nY1\nY0\nTE\n\n\n\n\nDr Strange\n0.70\n0.60\n0.10\n\n\nIron Man\n0.55\n0.50\n0.05\n\n\nThor\n0.80\n0.72\n0.08\n\n\nHulk\n0.60\n0.62\n-0.02",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#fundamental-problem-of-causal-inference",
    "href": "Week6-Lecture1.html#fundamental-problem-of-causal-inference",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.6 Fundamental Problem of Causal Inference",
    "text": "2.6 Fundamental Problem of Causal Inference\n\nTo measure the individual treatment effect of the ads on a customer’s purchase rate, we need to observe all potential outcomes of the same individual in parallel universes (i.e., with and without being exposed to the treatment).\nWe use \\(D_i = 1\\) to denote the treated/treatment group and \\(D_i = 0\\) to denote the untreated/control group in reality.\nWe only observe one potential outcome, the realized outcome, in our reality\n\nFor treated customers, we only observe their realized outcomes of being treated: \\(Y^1_i|D_i = 1\\)\nFor untreated customers, we only observe their realized outcomes of being untreated \\(Y^0_i|D_i = 0\\)\n\nThe remaining potential outcomes, i.e., counterfactual outcomes, are never observed in our reality\n\nFor treated customers, we never observe their outcomes when being untreated: \\(Y^0_i|D_i = 1\\)\nFor untreated customers, we never observe their outcomes when treated: \\(Y^1_i|D_i = 0\\)",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#exercise",
    "href": "Week6-Lecture1.html#exercise",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.7 Exercise",
    "text": "2.7 Exercise\n\nIn the previous table, let’s say, Dr Strange and Hulk are treated (by the ads) in reality, while Iron Man and Thor are not treated.\nBased on this information, we can find out the values of the realized outcomes and counterfactual outcomes for each customer.\nFor example, for Dr Strange:\n\nRealized outcome: \\(Y^1_i |D_i = 1 = 0.7\\) (Note that we only observe this value in reality)\nCounterfactual outcome: \\(Y^0_i |D_i = 1 = 0.6\\) (Note that we don’t observe this value in reality)\n\n\n\n\n\n\n\nCustomer\nY1\nY0\nTreated\n\n\n\n\nDr Strange\n0.7\n?\nYes\n\n\nIron Man\n?\n0.5\nNo\n\n\nThor\n?\n0.72\nNo\n\n\nHulk\n0.6\n?\nYes",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#fundamental-problem-of-causal-inference-1",
    "href": "Week6-Lecture1.html#fundamental-problem-of-causal-inference-1",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "2.8 Fundamental Problem of Causal Inference",
    "text": "2.8 Fundamental Problem of Causal Inference\n\nSince it is impossible to see both potential outcomes at once, one of the potential outcomes is always missing, so we can never quantify the individual treatment effects. This dilemma is called the Fundamental Problem of Causal Inference.\n\n\n\n\n\n\nsubject\nTreated\nY1\nY0\nY1-Y0\n\n\n\n\nDr Strange\nYes\n0.7\n?\n?\n\n\nIron Man\nNo\n?\n0.5\n?\n\n\nThor\nNo\n?\n0.72\n?\n\n\nHulk\nYes\n0.6\n?\n?",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#the-average-treatment-effect",
    "href": "Week6-Lecture1.html#the-average-treatment-effect",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "3.1 The Average Treatment Effect",
    "text": "3.1 The Average Treatment Effect\n\nSince individual treatment effects are unobservable, we often care more about the average treatment effects (ATE) on the population level. The ATE is defined as the average of individual treatment effects across the population.\n\n\\[\nATE = E[Y^1_i - Y^0_i] = \\frac{1}{N} \\sum_{i=1}^{N} (Y^1_i - Y^0_i)\n\\]\n\nFor display ads, can we obtain the ATE by directly calculating the difference in the average outcomes between the treatment group and control group. Why?\n\n\\[\\begin{align*}\n& E[Y^1|D_i = 1] - E[Y^0|D_i = 0] \\\\\n& = E[Y^1|D_i = 1] - E[Y^0|D_i = 1] \\\\\n&+ E[Y^0|D_i = 1] - E[Y^0|D_i = 0]\n\\end{align*}\\]",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#data-example",
    "href": "Week6-Lecture1.html#data-example",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "3.2 Data Example",
    "text": "3.2 Data Example\n\nPlease load data_treatmenteffect in the data folder into your RStudio.\n\n\n\nCode\ndata_treatmenteffect &lt;- read.csv(\"https://www.dropbox.com/scl/fi/ndvvwr298xkdtsj42y6az/individual-treatment-effects.csv?rlkey=z3mluwbqt9k1gtnb63xlc3l88&dl=1\")\n\n\n\nExercise: This data are generated from Instagram’s paid ads, treated customers are those who see Instagram’s ads.\n\nCompute the difference in the average rates between the treated and untreated customers.\nCompute the ATE based on the individual treatment effects.\nCompare the two results.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#the-average-treatment-effect-1",
    "href": "Week6-Lecture1.html#the-average-treatment-effect-1",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "3.3 The Average Treatment Effect",
    "text": "3.3 The Average Treatment Effect\n\nTo quantify the correct ATE, we must randomize who receives the treatment, instead of targeting or letting the individuals choose the treatment.\nAfter randomization, we can then obtain the ATE by comparing the difference in the average outcomes across the treatment group and control group. Because randomization ensures that\n\nSelection bias is fully removed1\nThe treatment effects on the treatment group individuals and the control group individuals should be equal. The former is called the average treatment effects on the treated (ATT), and the latter is called the average treatment effects on the untreated (ATU).\n\n\nExercise: Let’s go back to the previous data example and compute the ATT and ATU after randomization.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#gold-standard-of-causal-inference",
    "href": "Week6-Lecture1.html#gold-standard-of-causal-inference",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "3.4 Gold Standard of Causal Inference",
    "text": "3.4 Gold Standard of Causal Inference\n\nMathematically, the previous slide can be represented by the Basic Identity of Causal Inference:\n\n\\[\\begin{align*}\n& E[Y^1|D_i = 1] - E[Y^0|D_i = 0] \\\\\n& = E[Y^1|D_i = 1] - E[Y^0|D_i = 1] \\\\\n&+ E[Y^0|D_i = 1] - E[Y^0|D_i = 0]\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\nRandomized experiments makes ATT equal to ATE, and removes selection bias. Thus, randomized experiments are the gold standard of causal inference.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture1.html#footnotes",
    "href": "Week6-Lecture1.html#footnotes",
    "title": "Class 11 Causal Inference & Potential Outcome Framework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSelection bias refers to the pre-existing difference between the treatment group and control group even without the treatment↩︎",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 1: Causal Inference and Potential Outcome Framework"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html",
    "href": "Week6-Lecture2.html",
    "title": "Class 12 A/B Testing",
    "section": "",
    "text": "A randomized controlled trial (RCT) is an experimental form of impact evaluation in which the population receiving the intervention is chosen at random from the eligible population, and a control group is also chosen at random from the same eligible population.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab Experiment\nField Experiment\n\n\n\n\nLocation\nIn a controlled, laboratory environment\nIn the field\n\n\nInternal validity\nHigh\nLow\n\n\nExternal validity\nLow (Hawthorne effect)\nHigh\n\n\n\n\nInternal validity refers to the extent to which the experiment is free from selection bias.\nExternal validity refers to the extent to which the results can be generalized to the real world or other contexts.\n\n\n\n\n\nA/B testing (treatment group + control group)\n\nLoyalty program\nNo loyalty program\n\nA/B/N testing (multiple treatment groups + control group)\n\nPoint-based loyalty program; points can be redeemed for price vouchers\nPoint-based loyalty program; points can be redeemed for gifts\nPoint-based loyalty program; points can be redeemed for free top ups\nNo loyalty program\n\nFactorial design\n\nmore than 2 dimensions of treatments, used if we care about the interaction effects",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#randomized-controlled-trials-1",
    "href": "Week6-Lecture2.html#randomized-controlled-trials-1",
    "title": "Class 12 A/B Testing",
    "section": "",
    "text": "A randomized controlled trial (RCT) is an experimental form of impact evaluation in which the population receiving the intervention is chosen at random from the eligible population, and a control group is also chosen at random from the same eligible population.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#types-of-rcts-based-on-location",
    "href": "Week6-Lecture2.html#types-of-rcts-based-on-location",
    "title": "Class 12 A/B Testing",
    "section": "",
    "text": "Lab Experiment\nField Experiment\n\n\n\n\nLocation\nIn a controlled, laboratory environment\nIn the field\n\n\nInternal validity\nHigh\nLow\n\n\nExternal validity\nLow (Hawthorne effect)\nHigh\n\n\n\n\nInternal validity refers to the extent to which the experiment is free from selection bias.\nExternal validity refers to the extent to which the results can be generalized to the real world or other contexts.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#types-of-rcts-based-on-treatment-design",
    "href": "Week6-Lecture2.html#types-of-rcts-based-on-treatment-design",
    "title": "Class 12 A/B Testing",
    "section": "",
    "text": "A/B testing (treatment group + control group)\n\nLoyalty program\nNo loyalty program\n\nA/B/N testing (multiple treatment groups + control group)\n\nPoint-based loyalty program; points can be redeemed for price vouchers\nPoint-based loyalty program; points can be redeemed for gifts\nPoint-based loyalty program; points can be redeemed for free top ups\nNo loyalty program\n\nFactorial design\n\nmore than 2 dimensions of treatments, used if we care about the interaction effects",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#motivating-example-of-toms-loyalty-program",
    "href": "Week6-Lecture2.html#motivating-example-of-toms-loyalty-program",
    "title": "Class 12 A/B Testing",
    "section": "2.1 Motivating Example of Tom’s Loyalty Program",
    "text": "2.1 Motivating Example of Tom’s Loyalty Program\n\nShould we introduce a loyalty program for our customers?\n\nCons: increased costs due to free drinks\nPros: it may increase spending and retention rate, and hence future CLV\n\nHow to estimate the causal effect of introducing a loyalty program?",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-1-decide-on-the-unit-of-randomization",
    "href": "Week6-Lecture2.html#step-1-decide-on-the-unit-of-randomization",
    "title": "Class 12 A/B Testing",
    "section": "2.2 Step 1: Decide on the Unit of Randomization",
    "text": "2.2 Step 1: Decide on the Unit of Randomization\nWe decide the granularity of randomization unit based on the research question at hand.\n\nindividual\nhousehold\nstore\nother levels more granular (e.g., device level) or even less granular (e.g., city level)",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-1-decide-on-the-unit-of-randomization-1",
    "href": "Week6-Lecture2.html#step-1-decide-on-the-unit-of-randomization-1",
    "title": "Class 12 A/B Testing",
    "section": "2.3 Step 1: Decide on the Unit of Randomization",
    "text": "2.3 Step 1: Decide on the Unit of Randomization\nProposal 1: Randomize the treatment based on West London and East London.\n\nDo you expect the “randomize” to be true randomization?1\n\n\nNo, because East London and West London are intrinsically different. Randomization can only work well when the number of randomization units are large enough.\n\nProposal 2: Randomize the treatment among individual customers.\n\nIs this true randomization?\n\n\nYes, as long as we have a large number of individual customers, after randomization, we should see the treatment group and control group customers to have balanced characteristics.\n\n\nWhat problems can we still have?\n\n\n\nSpillover: Customers may talk to each other, so individual customers in the control group (who are not supposed to see the loyalty program) may also become aware of the loyalty program.\nCrossover: the same individual may accidentally receive different treatments.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-1-pros-and-cons-of-granularity",
    "href": "Week6-Lecture2.html#step-1-pros-and-cons-of-granularity",
    "title": "Class 12 A/B Testing",
    "section": "2.4 Step 1: Pros and Cons of Granularity",
    "text": "2.4 Step 1: Pros and Cons of Granularity\nDisadvantages of granularity:\n\nCosts and logistics\nSpillovers and crossovers\n\nAdvantages of granularity:\n\nIncrease the chance of successful randomization, thereby mitigating any systematic unbalance of covariates before the experiment.\n\nExercise:\n\nIf we would like to randomize prices, how can we randomize individualized price discounts to customers?\n\n\nWe can send individually personalized coupons to customers, e.g., Co-op weekly offers, Uber’s dynamic pricing.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-2-mitigate-spillover-and-crossover-effects",
    "href": "Week6-Lecture2.html#step-2-mitigate-spillover-and-crossover-effects",
    "title": "Class 12 A/B Testing",
    "section": "2.5 Step 2: Mitigate Spillover and Crossover Effects",
    "text": "2.5 Step 2: Mitigate Spillover and Crossover Effects\n\nCrossover Effects: A crossover occurs when an individual who was supposed to be assigned to one treatment is accidentally exposed to another or more treatments.\n\ne.g., For online A/B testing, a notorious crossover effect is that when browsers reset the cookies, the same individual customer may be treated as a different new customer.\n\nSpillover effects: The behavior of the treatment group can affect control group as well\n\ne.g., customers may share the promotions with family members and friends.\n\n\nQuestion: How should we mitigate spillover and crossover effects?\n\n\nMake sure that the same unit receives the same treatment throughout the experiment, e.g., forcing the customers to log into the website using their user accounts. User IDs should be unique.\nRandomize at the level of plausibly isolated social networks such as a community, rather than individual level.\nHowever, we must acknowledge that, it is really challenging to implement an A/B testing without any crossover or spillover in the field.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-3-decide-on-randomization-allocation-scheme",
    "href": "Week6-Lecture2.html#step-3-decide-on-randomization-allocation-scheme",
    "title": "Class 12 A/B Testing",
    "section": "2.6 Step 3: Decide on Randomization Allocation Scheme",
    "text": "2.6 Step 3: Decide on Randomization Allocation Scheme\n\nIndividuals (or the relevant unit of randomization) are allocated at random into a treatment condition based on some decision rules.\nDue to the high costs and potential risks of A/B testing, we often select a small percentage of customers into the treatment condition, while the remaining customer should do “business-as-usual”.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-4-collect-data",
    "href": "Week6-Lecture2.html#step-4-collect-data",
    "title": "Class 12 A/B Testing",
    "section": "2.7 Step 4: Collect Data",
    "text": "2.7 Step 4: Collect Data\n\nAny field experiment should be aware of the need for a sufficiently large sample size, or sufficient statistical power.\n\nThe larger sample size, the higher statistical power for the experiment; meanwhile, larger sample size brings higher costs and risks.\nRun a power calculation in R\n\nCollect both data on the outcome variables of interest and consumer characteristics data\n\nProposal: We need to collect customers’ spending and retention data and link the data with their treatment assignment.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#step-5-interpreting-results-from-a-field-experiment",
    "href": "Week6-Lecture2.html#step-5-interpreting-results-from-a-field-experiment",
    "title": "Class 12 A/B Testing",
    "section": "2.8 Step 5: Interpreting Results from a Field Experiment",
    "text": "2.8 Step 5: Interpreting Results from a Field Experiment\nStep 5.1: Randomization check\n\nWe need to check if the treatment group and control group are well-balanced in terms of their pre-treatment characteristics, especially the outcome variables.\n\nStep 5.2: Analyze the data and estimate the ATE\n\nt-test to examine the difference in the average outcome between the treatment group and control group. In R, we can use t.test()\nRegression analysis when analyzing A/B/N testing or multivariate experiments.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#after-class-readings",
    "href": "Week6-Lecture2.html#after-class-readings",
    "title": "Class 12 A/B Testing",
    "section": "2.9 After-Class Readings",
    "text": "2.9 After-Class Readings\n\n(optional) Varian, Hal R. ‘Causal Inference in Economics and Marketing’. Proceedings of the National Academy of Sciences 113, no. 27 (5 July 2016): 7310–15.",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week6-Lecture2.html#footnotes",
    "href": "Week6-Lecture2.html#footnotes",
    "title": "Class 12 A/B Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll answers to questions in the slides are on the webpage version of lecture notes.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 6] Causal Inference and A/B Testing",
      "Lecture 2: A/B Testing"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html",
    "href": "Week1-Lecture2.html",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "",
    "text": "How to conduct break-even analysis for a marketing proposal\nHow to conduct net present value analysis for a marketing proposal\nPractice R basic calculations and vector operations in the case study",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#learning-objectives",
    "href": "Week1-Lecture2.html#learning-objectives",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "",
    "text": "How to conduct break-even analysis for a marketing proposal\nHow to conduct net present value analysis for a marketing proposal\nPractice R basic calculations and vector operations in the case study",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#decisions-for-marketing-managers",
    "href": "Week1-Lecture2.html#decisions-for-marketing-managers",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "2.1 Decisions for Marketing Managers",
    "text": "2.1 Decisions for Marketing Managers\n\nThe ultimate goal of marketing (and other business activities) is to create value and improve profitability for firms.\nAs any marketing activity comes with a cost, data analysts need to correctly evaluate whether a campaign creates or destroys value to the company.\nSuch analyses are called cost-benefit analyses, sometimes referred to as profitability analysis or break-even analysis in different contexts.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#break-even-quantity",
    "href": "Week1-Lecture2.html#break-even-quantity",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "2.2 Break-Even Quantity",
    "text": "2.2 Break-Even Quantity\n\nWe often use break-even analyses to evaluate the financial feasibility of a marketing campaign. In marketing, we often compute the break-even quantity.\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe break-even quantity (BEQ) calculates the number of incremental units the firm needs to sell to cover the cost of the marketing campaign.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#compute-beq",
    "href": "Week1-Lecture2.html#compute-beq",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "2.3 Compute BEQ",
    "text": "2.3 Compute BEQ\nThe difference between the price per unit and variable costs per unit is defined as the contribution margin per unit. That is,\n\n\n\n\n\n\nDefinition\n\n\n\nContribution Margin Per Unit = Price Per Unit - Variable Costs Per Unit\n\n\n\nPrice per unit: retail price customers pay\nVariable costs per unit: Costs of goods sold (COGS)1 + any other variable costs per unit\n\nThis gives the second formula for BEQ:\n\n\n\n\n\n\nDefinition\n\n\n\nBEQ = Marketing Expenditure / Contribution Margin Per Unit",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#break-even-quantity-steps-and-decision-rule",
    "href": "Week1-Lecture2.html#break-even-quantity-steps-and-decision-rule",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "2.4 Break-Even Quantity: Steps and Decision Rule",
    "text": "2.4 Break-Even Quantity: Steps and Decision Rule\n\nSteps to conduct break-even analysis\n\nStep 1: Compute the BEQ based on the company’s product demand and production cost structure. This is the minimum quantity the company needs to sell to cover the costs of the marketing campaign.\nStep 2: Evaluate whether the campaign can generate an incremental quantity larger than BEQ\n\nThe decision rule\n\nif incremental quantity sales &gt; BEQ, the company makes money, so accept the campaign; otherwise, reject the campaign",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#background",
    "href": "Week1-Lecture2.html#background",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "3.1 Background",
    "text": "3.1 Background\n\n\n\n\n\n\n\n\n\n\n\n\nTom, senior marketing manager of Apple UK, is looking to launch a series of marketing campaigns to promote the iPhone 16 series that are just released. Tom was a proud graduate from UCL MSc BA program in 2020. He remembered from the Marketing Analytics module that break-even analysis helps evaluate different types of marketing decisions.\n\n\n\n\nCode\nprice &lt;- 799  # retail price of iPhone 16 in £\nquantity &lt;- 10  # sales quantity in million units\nendorsement_fee &lt;- 100  # endorsement fee in million pounds\nendorsement_sales_increase &lt;- 0.025 # sales increase percentage due to endorsement\nCOGS &lt;- 0.47 # cost of goods sold; 47% of retail price, i.e., 47% of £799\nRD_costs &lt;- 100 # R&D costs in million pounds\nmonthly_sales_increase_1stmonth &lt;- 0.003 # sales increase percentage in the first month\nmonthly_sales_increase_after &lt;- 0.002 # sales increase percentage in the following months\nWACC &lt;- 0.1 # weighted average cost of capital\n\n\nCase objectives:\n\nPractice situation analysis and compare with Uber’s case\nPractice how to conduct break-even analyses for a marketing compaign\nPractice R basic computations and vector operations",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#apple-inc-key-information",
    "href": "Week1-Lecture2.html#apple-inc-key-information",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "3.2 Apple Inc: Key Information",
    "text": "3.2 Apple Inc: Key Information\nFrom the case: The marketing analytics team at Apple Inc had applied sales forecasting models on historical sales data and predicted that the sales this year will reach 10 million units at the retail price of £799, without any additional marketing activities. The team had also collected the information on the Cost of Goods Sold of Apple 16, which is 47%. The Research and Development (R&D) costs for iPhone 16 is 100 million pounds.\n\nOpen the .qmd answer sheet downloaded from Moodle Week 1. And let’s solve this case using the R basics we learned!\nQuestion 1: Conduct a 5C analysis for Apple Inc, and compare with Uber’s case.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#apple-inc-bea-question-2",
    "href": "Week1-Lecture2.html#apple-inc-bea-question-2",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "3.3 Apple Inc: BEA Question 2",
    "text": "3.3 Apple Inc: BEA Question 2\n\nQuestion 2: Compute the contribution margin\n\nDo we need to consider R&D costs?\n\n\n\n\nCode\ncontribution_margin &lt;- (1 - COGS) * price\ncontribution_margin\n\n\n[1] 423.47",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#sunk-costs-and-sunk-costs-fallacy",
    "href": "Week1-Lecture2.html#sunk-costs-and-sunk-costs-fallacy",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "3.4 Sunk Costs and Sunk Costs Fallacy",
    "text": "3.4 Sunk Costs and Sunk Costs Fallacy\n\nSunk costs are costs that have already been incurred in the past and cannot be recovered. They should not be considered in decision-making.\nHowever, behavioral economics research shows that people often fall into the sunk costs fallacy, where they tend to consider sunk costs in decision-making instead of focusing on future costs and benefits.\nWhen making decisions, you should stand in the present and consider only future costs and benefits. In our case, the R&D costs are sunk costs and should not be considered in the break-even analysis.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#apple-inc-bea-question-3",
    "href": "Week1-Lecture2.html#apple-inc-bea-question-3",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "3.5 Apple Inc: BEA Question 3",
    "text": "3.5 Apple Inc: BEA Question 3\n\nQuestion 3: Based on the information at hand, should Tom approve the influencer marketing plan?",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#definition-of-npv",
    "href": "Week1-Lecture2.html#definition-of-npv",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "4.1 Definition of NPV",
    "text": "4.1 Definition of NPV\n\nWhen the effect of the marketing campaign is expected to have a long-term effect or when time value of money is important to the question at hand, we need to take the future into account.\n\n\n\n\n\n\n\nDefinition\n\n\n\nNet present value (NPV) is the difference between the present value of cash inflows and the present value of cash outflows over a period of time.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#formula-of-npv",
    "href": "Week1-Lecture2.html#formula-of-npv",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "4.2 Formula of NPV",
    "text": "4.2 Formula of NPV\n\\[\nN P V=-I_{0}+\\frac{CF_{1}}{(1+k)}+\\frac{C F_{2}}{(1+k)^{2}}+\\cdots+\\frac{C F_{n}}{(1+k)^{n}}\n\\]\n\n\\(I_{0}\\) is the initial marketing investment/expense\n\\(C F_{n}\\) is the incremental profits in period \\(n\\): it must be the additional profits due to the marketing campaign\n\\(k\\) (sometimes \\(r\\) or \\(i\\)) is the discount rate, which reflects the value of time: the same £1 today is worth \\(£1 * (1+k)\\) in the next period.\nThe decision rule\n\nif NPV &gt; 0, then the marketing campaign can bring in more values to the company, so it should be accepted.\nif NPV &lt; 0, then the marketing campaign will decrease the company’s value, so it should be rejected.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#apple-inc-npv-influencer-marketing-i",
    "href": "Week1-Lecture2.html#apple-inc-npv-influencer-marketing-i",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "4.3 Apple Inc: NPV Influencer Marketing I",
    "text": "4.3 Apple Inc: NPV Influencer Marketing I\nQuestion 4: Based on the information at hand, should Tom approve the influencer marketing plan based on Net Present Value method?\n\nCompute the sequence of monthly cash flows\n\nGenerate a sequence of incremental sales for 12 months (a vector with 12 elements)\n\nHint: use rep(), c(), and vector element-wise multiplication\n\n\n\n\n\nCode\nmonthly_sales_increase_1stmonth &lt;- 0.003\nmonthly_sales_increase_after &lt;- 0.002\n# incremental profit each month\nmonthly_incremental_sales &lt;- c(monthly_sales_increase_1stmonth,\n                     rep(monthly_sales_increase_after,11))\n\nCF &lt;-  monthly_incremental_sales * \n                    quantity * \n                    contribution_margin \n\n\nThe resulting monthly CFs are: 12.7041, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694, 8.4694",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#apple-inc-npv-influencer-marketing-ii",
    "href": "Week1-Lecture2.html#apple-inc-npv-influencer-marketing-ii",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "4.4 Apple Inc: NPV Influencer Marketing II",
    "text": "4.4 Apple Inc: NPV Influencer Marketing II\n\nCompute the sequence of discount factors\n\nGenerate a sequence of WACC for 12 months (a vector with 12 elements)\nGenerate a sequence of discount rate for 12 months (a vector with 12 elements)\n\nHint: use seq() to generate geometric sequence with patterns\n\n\n\n\n\nCode\nmonthly_WACC &lt;- 0.1/12 # this is the discount rate\ndiscount_factor &lt;- (1/(1+monthly_WACC))^c(1:12) # this is the discount factor\n\n\nThe resulting monthly discount factors are: 0.9917355, 0.9835394, 0.975411, 0.9673497, 0.9593551, 0.9514265, 0.9435635, 0.9357654, 0.9280319, 0.9203622, 0.9127559, 0.9052124",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#apple-inc-npv-influencer-marketing-iii",
    "href": "Week1-Lecture2.html#apple-inc-npv-influencer-marketing-iii",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "4.5 Apple Inc: NPV Influencer Marketing III",
    "text": "4.5 Apple Inc: NPV Influencer Marketing III\n\nCompute the NPV\n\nGenerate a sequence of discounted CFs for 12 months\nSum up all discounted CFs across the 12 months using sum()\nSubtract endorsement fee from the sum to get NPV\n\n\n\n\nCode\nNetPresentValue &lt;- sum(CF * discount_factor) - endorsement_fee\n\n\n\nThe NPV is 0.5349641",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#after-class",
    "href": "Week1-Lecture2.html#after-class",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "4.6 After-class",
    "text": "4.6 After-class\n\nReview the coding practice from today’s class and ensure you understand how to calculate NPV in R. Complete the R coding exercise for Week 1, and feel free to bring any questions to next week’s class.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture2.html#footnotes",
    "href": "Week1-Lecture2.html#footnotes",
    "title": "Class 2 Marketing Profitability Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaterial and production labor costs for producing a unit of product, often represented in percentage terms, e.g., COGS of 60% means the costs are 60% of retail prices. An iPhone with a retail price of £1000 and COGS of 30% means the COGS is £300.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 2: Marketing Profitability Analysis"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html",
    "href": "Week1-Lecture1.html",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "Hi there, I’m Wei!\nI did my PhD in Quant Marketing at NUS (Singapore) and an undergrad degree in Finance at Fudan (Shanghai)\nI love musical instruments, video games, and food (bubble tea is my soul mate)!\nMy research focuses on digital marketing, sharing economy, and platform design.\n\n\n\n\n\nDetailed weekly arrangements can be found in this link\n\nAdd bookmark for easier reference\n\nEach week, we have a 3-hour lecture on Wednesday, usually with 3 sessions\n\nA short quiz at the beginning of each class to review the previous week’s content\nA methodology session, in which we learn a new data analytics tool\nA case study session, to learn how the newly learned analytics tool can be applied to a real-life business scenario\n\n\n\n\n\n\nNo exams; 3 individual assignments, which are similar to case studies in class, and you will apply what you’ve learned in class to solve real-life marketing analytics problems.\n\n1st assignment, 30% weight, 1500 words, due on Oct 25\n2nd assignment, 30% weight, 1500 words, due on Nov 15\n3rd assignment, 40% weight, 2000 words, due on Dec 13\n\nHow to submit?\n\nPlease ensure submissions are in PDF (preferred) or Word format; submissions in other formats won’t be accepted for marking.\nThe quarto-based answer sheets (qmd files) will be given to you.\n\n\n\nWord count and late submission penalties will be applied by BA admin. For related queries and EC applications, please directly contact BA admin at mgmt.ba-admin@ucl.ac.uk.\nWe have random second marking in place to mitigate marking errors. Please refrain from emailing teaching assistants for re-marking as re-marking is not allowed by school policy.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#about-me",
    "href": "Week1-Lecture1.html#about-me",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "Hi there, I’m Wei!\nI did my PhD in Quant Marketing at NUS (Singapore) and an undergrad degree in Finance at Fudan (Shanghai)\nI love musical instruments, video games, and food (bubble tea is my soul mate)!\nMy research focuses on digital marketing, sharing economy, and platform design.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#weekly-arrangements",
    "href": "Week1-Lecture1.html#weekly-arrangements",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "Detailed weekly arrangements can be found in this link\n\nAdd bookmark for easier reference\n\nEach week, we have a 3-hour lecture on Wednesday, usually with 3 sessions\n\nA short quiz at the beginning of each class to review the previous week’s content\nA methodology session, in which we learn a new data analytics tool\nA case study session, to learn how the newly learned analytics tool can be applied to a real-life business scenario",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#assignments",
    "href": "Week1-Lecture1.html#assignments",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "",
    "text": "No exams; 3 individual assignments, which are similar to case studies in class, and you will apply what you’ve learned in class to solve real-life marketing analytics problems.\n\n1st assignment, 30% weight, 1500 words, due on Oct 25\n2nd assignment, 30% weight, 1500 words, due on Nov 15\n3rd assignment, 40% weight, 2000 words, due on Dec 13\n\nHow to submit?\n\nPlease ensure submissions are in PDF (preferred) or Word format; submissions in other formats won’t be accepted for marking.\nThe quarto-based answer sheets (qmd files) will be given to you.\n\n\n\nWord count and late submission penalties will be applied by BA admin. For related queries and EC applications, please directly contact BA admin at mgmt.ba-admin@ucl.ac.uk.\nWe have random second marking in place to mitigate marking errors. Please refrain from emailing teaching assistants for re-marking as re-marking is not allowed by school policy.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#role-of-marketing",
    "href": "Week1-Lecture1.html#role-of-marketing",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.1 Role of Marketing",
    "text": "2.1 Role of Marketing\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinance (finance a company’s business activities)\nAccounting (bookkeeping of past transactions)\nOperations (supply chain, manufacturing, inventory management)\nMarketing (directly deal with consumer; value exchange and value realization)",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#what-is-marketing-2",
    "href": "Week1-Lecture1.html#what-is-marketing-2",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.2 What is Marketing?",
    "text": "2.2 What is Marketing?\n\nKotler (1991): “Marketing is a social and managerial process by which individuals and groups obtain what they want and need through creating, offering and exchanging products of value with others.”\nBritish Chartered Institute of Marketing (2000s): “Marketing is the management process responsible for identifying, anticipating and satisfying customers’ requirements profitably.”\nAmerican Marketing Association (2017): “Marketing is the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large.”",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#what-is-marketing-a-text-mining-approach",
    "href": "Week1-Lecture1.html#what-is-marketing-a-text-mining-approach",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.3 What is Marketing? A Text Mining Approach",
    "text": "2.3 What is Marketing? A Text Mining Approach\n\n\nCode\npacman::p_load(tm, wordcloud, RColorBrewer,wordcloud2, data.table)\n# generate text corpus\ndf_mkt &lt;-  'Marketing is a social and managerial process by which individuals and groups obtain what they  want and need through creating, offering and exchanging products of value with others.\nMarketing is the management process responsible for identifying, anticipating and satisfying customers requirements profitably.\nMarketing is the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large.'\n\ndf_mkt_corpus &lt;- Corpus(VectorSource(df_mkt))\n# text cleaning \ndf_mkt_corpus &lt;- df_mkt_corpus |&gt; \n  tm_map(removePunctuation) |&gt;\n  tm_map(stripWhitespace) |&gt;\n  tm_map(content_transformer(tolower)) |&gt;\n  tm_map(removeWords, stopwords(\"english\"))\n# Create a document-term-matrix\ndf_mkt_dtm &lt;- TermDocumentMatrix(df_mkt_corpus)\ndf_mkt_matrix &lt;-  as.matrix(df_mkt_dtm)\ndf &lt;- data.table(words = rownames(df_mkt_matrix),\n                 freq = df_mkt_matrix[,1])\n# draw wordcloud\nset.seed(888)\nwordcloud(words = df$words, freq = df$freq, min.freq = 1, max.words=200, random.order=FALSE, colors=brewer.pal(8, \"Dark2\"))\n\n\n\n\n\n\n\n\n\n\nMarketing is a management process that creates and exchanges values for the company by selling the right products to the right customers. - Wei, 2024",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#marketing-process",
    "href": "Week1-Lecture1.html#marketing-process",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.4 Marketing Process",
    "text": "2.4 Marketing Process\n\n\n\n\n\n\n\n\n\n\nWe will go through the four steps using Uber and Apple.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-5-cs",
    "href": "Week1-Lecture1.html#situation-analysis-5-cs",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.5 Situation Analysis: 5 C’s",
    "text": "2.5 Situation Analysis: 5 C’s\n\nAny marketing decision can benefit from a deep understanding of the players within the market ecosystem—your own company, current and potential customers, collaborators and competitors—and the context they interact within: the 5Cs for short.\n\n\n\n\n\n\n\n\nCompany\n\nBusiness model\nGoals, objectives, and culture\n\n\n\nCustomers\n\nWho are the customers\nMarket size, segments\nOverall customer satisfaction and perceived value by customers\n\n\n\nCompetitors\n\nDirect/indirect/potential competitors\nStrengths, weaknesses, opportunities, and threats (SWOT)\n\n\n\nCollaborators\n\nSuppliers/distributors/alliances and partners\n\n\n\nClimate\n\nPESTLE",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-company",
    "href": "Week1-Lecture1.html#situation-analysis-company",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.6 Situation Analysis: Company",
    "text": "2.6 Situation Analysis: Company\n\nCompany analysis is a strategic planning method used to assess the internal strengths and weaknesses of your company.\n\nBusiness model is the way your company makes money.\nGoals, objectives, and culture are the guiding principles that shape your company’s actions.\n\nCustomer is the most important player in the market ecosystem.\n\nMarket size, segments are the number of potential customers and the different groups they can be divided into.\nOverall customer satisfaction and perceived value by customers are the key to marketing success.\n\n\n\n\n\n\n\n\n\nWhat is the business model of Uber? Does it directly provide transportation services? If not, who does?\nWho are Uber’s customers?",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-collaborators",
    "href": "Week1-Lecture1.html#situation-analysis-collaborators",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.7 Situation Analysis: Collaborators",
    "text": "2.7 Situation Analysis: Collaborators\n\nCollaborators are entities that work with your company to help you deliver your product or service to the customer.\n\nSuppliers provide the raw materials or components needed to produce your product.\nDistributors help you get your product to the customer.\nRetailers sell your product to the customer.\n\n\n\n\n\n\n\n\n\nWho are the collaborators of Uber?",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-competitors",
    "href": "Week1-Lecture1.html#situation-analysis-competitors",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.8 Situation Analysis: Competitors",
    "text": "2.8 Situation Analysis: Competitors\n\nWe tend to pay more attention towards more salient direct competitors, but we should also consider indirect and potential competitors.\n\nIndirect competitors are companies that satisfy the same customer goals, even if they offer different products or services.\nPotential competitors are those who might pose a competitive threat in the future; who possess equivalent resources that would allow them to enter the market",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#situation-analysis-contextclimate",
    "href": "Week1-Lecture1.html#situation-analysis-contextclimate",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.9 Situation Analysis: Context/Climate",
    "text": "2.9 Situation Analysis: Context/Climate\n\nContext/Climate analysis is a strategic planning method used to assess major external factors that influence the market ecosystem, and is often referred to as PESTLE analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical: Brexit\nEconomic: Minimum wage, inflation, economy recession\nSocial: Gig economy\nTechnological: Big Data, mobile tech penetration\nLegal: GDPR, government regulations (BBC: partner or employee?)\nEnvironmental: sustainability, CSR",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#strategy-stp",
    "href": "Week1-Lecture1.html#strategy-stp",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.10 Strategy: STP",
    "text": "2.10 Strategy: STP\n\nSituation analysis is a critical input into marketing strategy, i.e., the sequential application of the processes of segmentation, targeting, and positioning.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#tactics-4ps",
    "href": "Week1-Lecture1.html#tactics-4ps",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "2.11 Tactics: 4P’s",
    "text": "2.11 Tactics: 4P’s\n\nThe marketing mix provides an implementation of your positioning. Segmentation is here applied at the tactical level, to optimally design the marketing mix or 4Ps.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#big-data-era",
    "href": "Week1-Lecture1.html#big-data-era",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "3.1 Big Data Era",
    "text": "3.1 Big Data Era\nWith the advancement in ICTs and computing power, data scientists nowadays are equipped with data analytics tools powerful than ever!\nFirms now have access to enormously rich information trail of customers\n\nDemographic profiles (DoB, gender, ethnicity, income)\nPurchase history (recency, frequency, monetary value, spending behavior)\nOnline browsing and search history (browsing, click through, add to cart, purchase)\nGPS data from mobile phones for offline store visits\nSocial media (location, consumer preference, social network)",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#our-roadmap",
    "href": "Week1-Lecture1.html#our-roadmap",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "3.2 Our Roadmap",
    "text": "3.2 Our Roadmap\n\nWeeks 1-3: Marketing Process and Profitability Analysis\n\nConcepts and R basics, which lay the foundation for the rest of the course\n\nWeeks 4-5: Machine Learning and Predictive Analytics\n\nHow to reduce the costs of marketing campaigns\n\nWeeks 6-10: Causal Inference\n\nHow to correctly evaluate the benefits of marketing campaigns",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "Week1-Lecture1.html#unique-position-of-marketing-analytics",
    "href": "Week1-Lecture1.html#unique-position-of-marketing-analytics",
    "title": "Class 1 Intro to Marketing Analytics",
    "section": "3.3 Unique Position of Marketing Analytics",
    "text": "3.3 Unique Position of Marketing Analytics",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Lecture 1: Introduction to Marketing Analytics"
    ]
  },
  {
    "objectID": "LectureSlides.html",
    "href": "LectureSlides.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Lecture slides of subsequent weeks will be released on Monday each week. Stay tuned!",
    "crumbs": [
      "Lectures",
      "[Week 10] Frontiers of Marketing Analytics",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "R-errors.html",
    "href": "R-errors.html",
    "title": "Troubleshooting R",
    "section": "",
    "text": "In this page, I summarize the common issues running R and the troubleshooting tips. If you run into any R issues, please refer to this page as the first step for any solution.",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#could-not-find-function-error",
    "href": "R-errors.html#could-not-find-function-error",
    "title": "Troubleshooting R",
    "section": "1 ‘could not find function’ Error",
    "text": "1 ‘could not find function’ Error\nThis error arises when (1) an R package is not loaded properly, or (2) due to misspelling of the functions.\nAs you can see in the screenshot below, when we run the code, we get a could not find function “praise” error in the console. This is because we have not loaded the package “praise” to which the praise() function belongs.\n\nWe need to first load the package that contains the function we want to run using library() as shown below:\n\n\nCode\nlibrary(praise)\n\n\nand then use the function praise() to run it error-free.\n\n\nCode\npraise()\n\n\n[1] \"You are fantabulous!\"\n\n\nMeanwhile, if we misspell the praise() function, for instance, to prais(), this will also throw up a could not find function “prais” error.\n\n\nCode\nprais()\n\n\nError in prais(): could not find function \"prais\"",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#object-not-found-error",
    "href": "R-errors.html#object-not-found-error",
    "title": "Troubleshooting R",
    "section": "2 ‘object not found’ error",
    "text": "2 ‘object not found’ error\nThis error occurs when the particular object used in the code is not yet created or existing in the R environment.\nIn the example below we are trying to compute x plus 4. As you can see, we get an ‘object ’x’ not found’ error as the “x” object is not yet created and missing in our R environment.\n\n\nCode\nx+4\n\n\nError in eval(expr, envir, enclos): object 'x' not found\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nBased on the missing object, go back to your previous codes and check why the object is missing. Did you forget to create it? Did you accidentally delete it?",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#banner-packages-xxx-required-but-are-not-installed",
    "href": "R-errors.html#banner-packages-xxx-required-but-are-not-installed",
    "title": "Troubleshooting R",
    "section": "3 Banner: “Packages XXX required but are not installed”",
    "text": "3 Banner: “Packages XXX required but are not installed”\nIf you see this banner in your RStudio in the screenshot, it means RStudio detects some packages mentioned in the R Markdown file or R script but are not yet installed on your computer. So it prompts you for installation.\n\nFor instance, in this case, the error message means, knitr and pacman are found in the .Rmd file, but they are not installed, so RStudio is smart enough to prompt you to install.\n\n\n\n\n\n\nSolution\n\n\n\nJust click install button in the banner. RStudio will install the missing components.",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#prompt-window-r-packages-not-up-to-date",
    "href": "R-errors.html#prompt-window-r-packages-not-up-to-date",
    "title": "Troubleshooting R",
    "section": "4 Prompt Window: “R packages not up-to-date”",
    "text": "4 Prompt Window: “R packages not up-to-date”\nSince the R packages are being updated every day (just like our mobile apps, there could be bugs so that the developers have to update R packages to fix those bugs), sometimes, though we may have installed some packages, they are too old to run the functions. And you may see this prompted message:\n\nIt means, the aforementioned packages, base64enc, digest, etc. are outdated and must be updated to function.\n\n\n\n\n\n\nSolution\n\n\n\nClick Yes and RStudio will update all the packages for you.",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#latex-not-found-when-kniting-the-.rmd.qmd-file.",
    "href": "R-errors.html#latex-not-found-when-kniting-the-.rmd.qmd-file.",
    "title": "Troubleshooting R",
    "section": "5 Latex not found when kniting the .rmd/.qmd file.",
    "text": "5 Latex not found when kniting the .rmd/.qmd file.\nIf this is your first time to knit the PDF document, you may see an error message as below:\n\nThe error message has usually told us everything on how to troubleshoot (that’s what an error message is for!).\nIn this screenshot, if you read along, you will find the cause of problem:\n\n[…] LaTex failed to compile,\n\nbecause\n\n[…] No LaTeX installation detected (LaTeX is required to create PDF output).\n\nand the solution is also consideratebly given in this error message:\n\n[…] You should install a LaTeX distribution for your platform: https://www.latex-project.org/get/\n\n\nIf you are not sure, you may install TinyTeX in R: tinytex::install_tinytex()\n\nSo this error message tells us the solution:\n\n\n\n\n\n\nSolution\n\n\n\n\nAlternative solution 1:\nRun the following command in Console\ntinytex::install_tinytex()\nin order to install tinytex, a simplied version of LaTex, on your laptop.\n\n\nAlternative solution 2:\nInstall LaTex on your computer following the R installation guide\n\n\n\nIf you run the command in R Console, you will see that Latex is being installed\n\nAfter this progress bar finishes, you will be able to knit the PDF document!",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#error-connection-not-found",
    "href": "R-errors.html#error-connection-not-found",
    "title": "Troubleshooting R",
    "section": "6 Error: Connection Not Found",
    "text": "6 Error: Connection Not Found\nConnection Not Found error is usually caused by RStudio being unable to locate your files on your hard disk.\nIf you don’t know how to find the path names for a file on your computer, please refer to\n\nthis link for Windows\n\nthis link for Mac",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "R-errors.html#more-questions",
    "href": "R-errors.html#more-questions",
    "title": "Troubleshooting R",
    "section": "7 More Questions",
    "text": "7 More Questions\nPlease leave a screenshot of error message in the MSTeams channel named “R QnA”. I will keep updating this webpage as more questions come in.",
    "crumbs": [
      "R Tutorials",
      "Troubleshooting R"
    ]
  },
  {
    "objectID": "Case-CLV.html",
    "href": "Case-CLV.html",
    "title": "CLV Analysis for M&S Delivery Pass Program",
    "section": "",
    "text": "M&S in Jubilee Place, Canary Wharf, London",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Case Study: CLV Analysis for M&S's Delivery Pass"
    ]
  },
  {
    "objectID": "Case-CLV.html#the-business-problem-for-ms-a-delivery-pass-program",
    "href": "Case-CLV.html#the-business-problem-for-ms-a-delivery-pass-program",
    "title": "CLV Analysis for M&S Delivery Pass Program",
    "section": "2.1 The Business Problem for M&S: A Delivery Pass Program",
    "text": "2.1 The Business Problem for M&S: A Delivery Pass Program\nIt was just six months after Tom, our proud (but perhaps overly confident?) graduate from our MSc BA program, made a series of … let’s say, “bold” decisions at his first job at Apple Inc. The influencer marketing campaign didn’t exactly influence anyone—except maybe the CMO to rethink her hiring decisions. Turns out, Tom hadn’t spent enough time learning R basics (despite countless reminders from Dr. Meow that “R is the best language in the world”), and he completely miscalculated the NPV of the campaign. Tom really regretted not having taken Dr. Meow’s advice on spending more time on the Marketing Analytics module instead of the Business Strategy module during the MSc BA program. Obviously, he had underestimated the importance of marketing analytics for his career as a business analyst.\nAs you might have guessed, Tom was promptly shown the door, and it wasn’t to Apple’s employee spa. But luck or maybe just a very sympathetic friend from his UCL days, Jerry, landed him a new gig at Marks & Spencer (M&S). Now, with a slightly more humble outlook on life and an unhealthy amount of bubble tea, Tom is trying to redeem himself as a data scientist in M&S’s marketing analytics team.\nM&S is a multinational retailer that sells groceries, clothing, and home products. M&S has been successful in building a loyal customer base through its Sparks loyalty program, which offers customers exclusive discounts, personalized offers, and early access to sales. However, M&S is looking to further enhance its customer relationships and drive long-term profitability. One area to improve is the online sales channel. Other grocery chains such as Tesco and Sainsbury’s have been offering online delivery passes, which provide customers with unlimited deliveries for a fixed annual fee. M&S is considering launching a similar program to attract more customers and increase customer loyalty.\nTom’s job is to calculate the Customer Lifetime Value (CLV) of this potential new program—a task that would require him to remember at least some of what he learned in Marketing Analytics Week 2’s class (if only he had stayed awake). This time, Tom knows one thing for sure: he better not mess this up. Otherwise, he might end up selling bubble teas at a Canary Wharf stall next to the Rice Guys instead of analyzing data at M&S.\nAs Tom learned in Week 2’s class, customer lifetime value (CLV) can be a powerful metric that can help M&S understand the long-term value of its customers and guide strategic marketing decisions such as the profitability of introducing a delivery pass program. By calculating the CLV of customers who sign up for the delivery pass program, M&S can determine the program’s potential and identify the most valuable customer segments.\nIn order to conduct the CLV analysis, Tom needs to consider the following key factors:\nPrice of the Delivery Pass: As a starting point, Tom needs to determine the price of the delivery pass and the benefits it offers to customers. The price of the delivery pass will surely influence customer acquisition and retention rates, as well as the overall profitability of the program. At the moment, Tesco’s delivery pass is priced at £84 per year, while Sainsbury’s delivery pass is priced at £80 per year. Therefore, Tom would like to price M&S’s delivery pass at £89 per year, which is competitive with other grocery chains and offers customers a cost-effective option for unlimited deliveries.\nTime Unit of Analysis: Although customers frequently shop at M&S throughout the year, the yearly approach is the most convenient, especially since M&S’s Sparks program offers benefits that often encourage long-term loyalty.\nNumber of Years (N): M&S will assess the CLV over a 5-year period. While a longer time horizon could be considered, uncertainty increases with time due to changes in the economy, competition, and customer preferences. Therefore, M&S uses a 5-year horizon for most revenue and profitability projections.\nGross Profit (g): M&S’s analysis starts by calculating the gross profit generated by a typical customer each year. From the historical customer transaction data, the marketing analytics team estimates that an average M&S customer shops at M&S 40 times a year, spending an average of £100 per visit.\nProfit Margin: M&S’s profit margin on each purchase is 7%, meaning that the cost of goods sold (COGS) is 93% of the purchase price. However, M&S incurs costs to provide these services included in the delivery pass, such as delivery fees for online orders. Each delivery costs M&S a £5 to pay for the delivery driver and other operational expenses.\nRetention Rate (r): The retention rate is the probability that a customer continues to shop with M&S year after a year. Based on historical data, M&S’s customer retention rate is 70%, meaning that 70% of customers remain loyal and are likely to renew their delivery pass each year. This retention rate forms the basis for estimating future customer value.\nDiscount Rate (k): M&S applies a yearly discount rate of 10%, as recommended by its finance department. This discount rate reflects the cost of capital and the opportunity cost of investing elsewhere. All future profits will be discounted to reflect their present value, assuming that the profits from customer purchases are received at the end of each year.\nCustomer Acquisition Costs (CAC): M&S invests heavily in customer acquisition, using paid search ads on search engines. Such promotion is also called search engine marketing or pay-per-click marketing. M&S decideds to bid £0.4 for each click.1 Based on the bid, M&S estimates that about 1% of Google users will see the the ads; at the £89 annual subscription price, 10% of exposed customers who click on an ad on search engine or social medias will sign up for a free trial (i.e., triers); triers will on average shop twice during the 2-week trial period. 20% of those triers will eventually become paying customers. In order to increase market penetration, M&S offers a £10 discount on the first purchase on top of a customer’s £100 shopping basket. M&S also offers free delivery for triers during the trial period.\n\n\n\n\n\n\n\n\nFigure 1: M&S’s Marketing Funnel\n\n\n\n\n\nBased on the above information, Tom will conduct a CLV analysis to determine the potential profitability of M&S’s delivery pass program and provide recommendations to the senior marketing manager.",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Case Study: CLV Analysis for M&S's Delivery Pass"
    ]
  },
  {
    "objectID": "Case-CLV.html#compute-customer-acquisition-cost",
    "href": "Case-CLV.html#compute-customer-acquisition-cost",
    "title": "CLV Analysis for M&S Delivery Pass Program",
    "section": "2.2 Compute Customer Acquisition Cost",
    "text": "2.2 Compute Customer Acquisition Cost\n\nQuestion 3Answer\n\n\nWatch the intro video by Google, Introduction to Search Engine Marketing, before class. Understand how SEM works and think about how these concepts apply to M&S’s case as in Figure 1.\nCompute the customer acquisition costs (CAC) for M&S’s delivery pass program\n\n\nCAC = total costs for customer ad clicks + total costs of £10 promo + total costs of free deliveries\nCAC Part I: Costs of paid search ads to get 1 new member.\n\n[…] “about 10% of customers who click on an ad on search engine or social medias will sign up for a free trial, and 20% of those trial users will eventually become paying customers.”\n\n\n\nCode\n# clicker_to_trier_rate is the % of trier customers from clickers\nclicker_to_trier_rate &lt;- 0.1\n\n# trier_to_member_rate is the % of a new member from triers\ntrier_to_member_rate &lt;- 0.2\n\n\n\nHow many customers need to click the ad to get 1 new customer?\n\n\n\nCode\nn_clickers_for_1newmember &lt;- (1 / clicker_to_trier_rate) * (1 / trier_to_member_rate)\n\nn_clickers_for_1newmember\n\n\n[1] 50\n\n\n\nTotal costs for customer clicks\n\n\n\nCode\ntotal_cost_clicks &lt;- 0.4 * n_clickers_for_1newmember\ntotal_cost_clicks\n\n\n[1] 20\n\n\nCAC Part II: total costs of £10 promo for first order each trier customer\n\nWhat is the total promo cost for these trier customers’ first order?\n\n\n\nCode\npromo_first_order_each_trier &lt;- 10\nprofit_margin &lt;- 0.07\n\ntotal_cost_promo &lt;- promo_first_order_each_trier * # promotion amount\n    (1 - profit_margin) * # COGS rate\n    (1 / trier_to_member_rate) # num of triers\n\ntotal_cost_promo\n\n\n[1] 46.5\n\n\nCAC Part III: total costs from selling groceries for each trier\n\n2 visits for each trier, the profits from the 2 visits are\n\n\n\nCode\nrevenue_each_visit &lt;- 100\n\nprofit_each_trier &lt;- revenue_each_visit * profit_margin * 2\n\nprofit_each_trier\n\n\n[1] 14\n\n\n\nThe 2 visits are free of delivery charges, which are costs to M&S\n\n\n\nCode\ndeliverycost_each_trier &lt;- 5 * 2\ndeliverycost_each_trier\n\n\n[1] 10\n\n\n\nNet costs for each trier from the 2 visits\n\n\n\nCode\nnetcost_each_trier &lt;- deliverycost_each_trier - profit_each_trier\n\nnetcost_each_trier\n\n\n[1] -4\n\n\n\nTotal net profits from all triers\n\n\n\nCode\ntotalcosts_from_all_triers &lt;- netcost_each_trier * (1 / trier_to_member_rate)\n\ntotalcosts_from_all_triers\n\n\n[1] -20\n\n\n\nCAC = total costs for customer ad clicks + total costs of £10 promo + total costs of free deliveries\n\n\n\nCode\nCAC &lt;- total_cost_clicks + total_cost_promo + totalcosts_from_all_triers\n\nCAC\n\n\n[1] 46.5",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Case Study: CLV Analysis for M&S's Delivery Pass"
    ]
  },
  {
    "objectID": "Case-CLV.html#compute-customer-lifetime-value",
    "href": "Case-CLV.html#compute-customer-lifetime-value",
    "title": "CLV Analysis for M&S Delivery Pass Program",
    "section": "2.3 Compute Customer Lifetime Value",
    "text": "2.3 Compute Customer Lifetime Value\n\n2.3.1 Step 1: Determine time unit of analysis\n\nQuestion 4Answer\n\n\nFind the time unit of analysis in the case study.\nShould we use monthly analysis?\n\n\nWhen there is strong within-year seasonality of customer purchases\n\n\n\n\n\n2.3.2 Step 2: Determine number of years\n\nQuestion 5Answer\n\n\nFind \\(N\\): the number of years over which the customer relationship is assessed\n\n\n\n\nCode\nN &lt;- 5\n\n\n\n\n\n\n\n2.3.3 Step 3: Compute profit margin for each period\n\nQuestion 6Answer\n\n\n\\(g = M - c\\): profit each year, which is the profit from sales M minus marketing costs c\n\n\n\nThe annual membership fee is £89\n\n\n\nCode\nmembership &lt;- 89\n\n\n\nn_visit: 40 visits each year; each time £100; with profit margin 7% (COGS 93%)\n\n\n\nCode\nn_visit &lt;- 40\n\n\n\nM: profit margin each period\n\n\n\nCode\nM &lt;- membership + revenue_each_visit * n_visit * profit_margin\nM\n\n\n[1] 369\n\n\n\nc: variable delivery costs each order\n\n\n\nCode\ndeliverycost_each_visit &lt;- 5\nc &lt;- deliverycost_each_visit * n_visit\nc\n\n\n[1] 200\n\n\n\ng: the period net profit from customers\n\n\n\nCode\ng &lt;- M - c\ng_seq &lt;- rep(g, N)\ng_seq\n\n\n[1] 169 169 169 169 169\n\n\n\n\n\n\n\n2.3.4 Step 4: Compute sequence of retention rate\n\nQuestion 7Answer\n\n\n\\(r\\): retention rate\n\n\n\n\nCode\n# retention_rate is the probability of customer staying with us after 1 year\nr &lt;- 0.7\n\n# create a geometric sequence of accumulative retention rate for N years\nr_seq &lt;- r^(seq(1, N) - 1)\nr_seq\n\n\n[1] 1.0000 0.7000 0.4900 0.3430 0.2401\n\n\n\n\n\n\n\n2.3.5 Step 5: Compute sequence of discount factors\n\nQuestion 8Answer\n\n\n\\(k\\): the discount rate\n\n\n\n\nCode\nk &lt;- 0.1\nd &lt;- 1 / (1 + k)\nd_seq &lt;- d^(seq(1, N))\nd_seq\n\n\n[1] 0.9090909 0.8264463 0.7513148 0.6830135 0.6209213\n\n\n\n\n\n\n\n2.3.6 Step 6: Compute CLV\n\nQuestion 9Answer\n\n\nCompute the CLV based on the CLV formula\n\n\n\nRevenues, variables costs, and profit for the next 5 years\n\n\n\nCode\ng_seq\n\n\n[1] 169 169 169 169 169\n\n\n\nApply retention rate\n\n\n\nCode\ng_seq_after_churn &lt;- g_seq * r_seq\ng_seq_after_churn\n\n\n[1] 169.0000 118.3000  82.8100  57.9670  40.5769\n\n\n\nApply discount factor\n\n\n\nCode\ng_seq_after_churn_discount &lt;- g_seq_after_churn * d_seq\n\ng_seq_after_churn_discount\n\n\n[1] 153.63636  97.76860  62.21638  39.59224  25.19506\n\n\n\nCompute CLV by summing up future expected profits\n\n\n\nCode\nsum(g_seq_after_churn_discount)\n\n\n[1] 378.4086\n\n\nCode\nsum(g_seq_after_churn_discount) - CAC\n\n\n[1] 331.9086",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Case Study: CLV Analysis for M&S's Delivery Pass"
    ]
  },
  {
    "objectID": "Case-CLV.html#use-clv-to-guide-marketing-decisions-1",
    "href": "Case-CLV.html#use-clv-to-guide-marketing-decisions-1",
    "title": "CLV Analysis for M&S Delivery Pass Program",
    "section": "4.1 Use CLV to Guide Marketing Decisions",
    "text": "4.1 Use CLV to Guide Marketing Decisions\n\n(To guide customer acquisition) What if the company only offers £5 for first time purchase? This will save some CAC but the clicker-to-trier rate will decrease to 5%. Please compute the new CLV. Should you go ahead with the proposed change?\n\n\n\nCode\ncomputeCLV(\n    promo_first_order_each_trier = 5,\n    clicker_to_trier_rate = 0.05\n)\n\n\n[1] 335.1586\n\n\n\n(To guide customer retention) What if the company increases the annual membership fee to $119? This will increase revenue from memberships but will also make some customers unhappy so their retention rate reduce to 55%. Please compute the new CLV. Should you go ahead with the proposed change?\n\n\n\nCode\ncomputeCLV(\n    membership = 119,\n    r = 0.55\n)\n\n\n[1] 304.0114",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Case Study: CLV Analysis for M&S's Delivery Pass"
    ]
  },
  {
    "objectID": "Case-CLV.html#footnotes",
    "href": "Case-CLV.html#footnotes",
    "title": "CLV Analysis for M&S Delivery Pass Program",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCost per click (CPC) is a pricing model used in online advertising where advertisers pay each time a user clicks on their ad. It is a key metric in digital marketing campaigns and is commonly used in platforms like Google Ads, Facebook Ads, and other search engines or social media networks. https://www.investopedia.com/terms/c/cpc.asp↩︎",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Case Study: CLV Analysis for M&S's Delivery Pass"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html",
    "href": "R-ComparisonWithPython.html",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "",
    "text": "Tip\n\n\n\nThis tutorial is designed for those who are familiar with either R, Python or Julia, and would like to learn another language.\nIn this tutorial, I will compare the basics of R, Python, and Julia side by side. We will cover the basic syntax, data types, and functionalities.\nIf you discover any mistakes or outdated content in this tutorial, please let me know. I will be very grateful for your feedback.\nCode\nlibrary(reticulate)\nuse_condaenv(\"base\")\nlibrary(JuliaCall)",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#assignment-of-variables",
    "href": "R-ComparisonWithPython.html#assignment-of-variables",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "1.1 Assignment of variables",
    "text": "1.1 Assignment of variables\n\n\n\n\n\n\nCaution\n\n\n\nIn R and Python, assignment operations do not print the assigned object by default.\nBut Julia does print the assigned object by default. Unless you put a semicolon ; at the end of the line, Julia will not print the assigned object.\n\n\n\nRPythonJulia\n\n\n\n\nCode\n# create an object x with value 3\nx &lt;- 3\nx\n\n\n[1] 3\n\n\n\n\n\n\nCode\n# create an object x with value 3\nx = 3\nx\n\n\n3\n\n\n\n\n\n\nCode\n# create an object x with value 3\nx = 3; # the ; suppresses the output\n\n\n3",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#comment-codes",
    "href": "R-ComparisonWithPython.html#comment-codes",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "1.2 Comment codes",
    "text": "1.2 Comment codes\n\nRPythonJulia\n\n\nYou can put a # before any code, to indicate that any codes after the # on the same line are your comments, and will not be run by R.\nIt’s a good practice to often comment your codes, so that you can help the future you to remember what you were trying to achieve.\n\n\nCode\n# Is x 1 or 2 below?\nx &lt;- 1 # +1\n\n\n\n\nSame as R. You can put a # before any code, to indicate that any codes after the # on the same line are your comments, and will not be run by Python.\n\n\nCode\n# Is x 1 or 2 below?\nx = 1 # +1\n\n\n\n\nSame as R and Python. You can put a # before any code, to indicate that any codes after the # on the same line are your comments, and will not be run by Julia.\n\n\nCode\n# Is x 1 or 2 below?\n\nx = 1 # +1\n\n\n1",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#rules-for-naming-object",
    "href": "R-ComparisonWithPython.html#rules-for-naming-object",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "1.3 Rules for naming object",
    "text": "1.3 Rules for naming object\n\nRPythonJulia\n\n\nFor a variable to be valid, it should follow these rules\n\nIt should contain letters, numbers, and only dot or underscore characters.\nIt cannot start with a number (eg: 2iota), or a dot, or an underscore.\n\n\n\nCode\n# 2iota &lt;- 2\n# .iota &lt;- 2\n# _iota &lt;- 2\n\n\n\nIt should not be a reserved word in R (eg: mean, sum, etc.).\n\n\n\nCode\n# mean &lt;- 2\n\n\n\n\nFor a variable to be valid, it should follow these rules\n\nIt should contain letters, numbers, and only underscore characters.\nIt cannot start with a number (eg: 2iota), or a dot, or an underscore.\n\n\n\nCode\n\n# 2iota = 2\n\n# .iota = 2\n\n# _iota = 2\n\n\n\nIt should not be a reserved word in Python (eg: mean, sum, etc.).\n\n\n\nCode\n\n# mean = 2\n\n\n\n\nSame as R.",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#arithmetic-operations",
    "href": "R-ComparisonWithPython.html#arithmetic-operations",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "3.1 Arithmetic operations",
    "text": "3.1 Arithmetic operations\n\nRPythonJulia\n\n\n\n\nCode\n# arithmatic operations\nx &lt;- 3 \nx + 1 # addition\n\n\n[1] 4\n\n\nCode\nx - 1 # subtraction\n\n\n[1] 2\n\n\nCode\nx * 2 # multiplication\n\n\n[1] 6\n\n\nCode\nx / 2 # division\n\n\n[1] 1.5\n\n\nCode\nx^2 # square\n\n\n[1] 9\n\n\nCode\nx %% 2 # remainder\n\n\n[1] 1\n\n\nCode\nx %/% 2 # integer division\n\n\n[1] 1\n\n\nCode\n# math operations\nlog(x)  # natural logarithm\n\n\n[1] 1.098612\n\n\nCode\nexp(x)  # exponential\n\n\n[1] 20.08554\n\n\nCode\nsqrt(x) # square root\n\n\n[1] 1.732051\n\n\nCode\nlog10(x) # log base 10\n\n\n[1] 0.4771213\n\n\nCode\nround(x/2) # round\n\n\n[1] 2\n\n\nCode\nfloor(x/2) # floor\n\n\n[1] 1\n\n\nCode\nceiling(x/2) # ceiling\n\n\n[1] 2\n\n\n\n\n\n\nCode\n# arithmatic operations\nx = 3\nx + 1 # addition\n\n\n4\n\n\nCode\nx - 1 # subtraction\n\n\n2\n\n\nCode\nx * 2 # multiplication\n\n\n6\n\n\nCode\nx / 2 # division\n\n\n1.5\n\n\nCode\nx ** 2 # square\n\n\n9\n\n\nCode\nx % 2 # remainder\n\n\n1\n\n\nCode\nx // 2 # integer division\n\n\n1\n\n\nCode\n# math operations\nimport math\nmath.log(x)  # natural logarithm\n\n\n1.0986122886681098\n\n\nCode\nmath.exp(x)  # exponential\n\n\n20.085536923187668\n\n\nCode\nmath.sqrt(x) # square root\n\n\n1.7320508075688772\n\n\nCode\nmath.log10(x) # log base 10\n\n\n0.47712125471966244\n\n\nCode\nround(x/2) # round\n\n\n2\n\n\nCode\nmath.floor(x/2) # floor\n\n\n1\n\n\nCode\nmath.ceil(x/2) # ceiling\n\n\n2\n\n\n\n\n\n\nCode\n\n# arithmatic operations\n\nx = 3\n\n\n3\n\n\nCode\n\nx + 1 # addition\n\n\n4\n\n\nCode\n\nx - 1 # subtraction\n\n\n2\n\n\nCode\n\nx * 2 # multiplication\n\n\n6\n\n\nCode\n\nx / 2 # division\n\n\n1.5\n\n\nCode\n\nx ^ 2 # square\n\n\n9\n\n\nCode\n\nx % 2 # remainder\n\n\n1\n\n\nCode\n\ndiv(x, 2) # integer division\n\n\n1\n\n\nCode\n\n# math operations\n\nlog(x)  # natural logarithm\n\n\n1.0986122886681098\n\n\nCode\n\nexp(x)  # exponential\n\n\n20.085536923187668\n\n\nCode\n\nsqrt(x) # square root\n\n\n1.7320508075688772\n\n\nCode\n\nlog10(x) # log base 10\n\n\n0.47712125471966244\n\n\nCode\n\nround(x/2) # round\n\n\n2.0\n\n\nCode\n\nfloor(x/2) # floor\n\n\n1.0\n\n\nCode\n\nceil(x/2) # ceiling\n\n\n2.0",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#logical-operations",
    "href": "R-ComparisonWithPython.html#logical-operations",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "3.2 Logical operations",
    "text": "3.2 Logical operations\n\nRPythonJulia\n\n\n\n\nCode\n# logical operations\nx &lt;- 3\nx &gt; 2 # larger than\n\n\n[1] TRUE\n\n\nCode\nx &lt; 2 # smaller than\n\n\n[1] FALSE\n\n\nCode\nx == 2 # equal to\n\n\n[1] FALSE\n\n\nCode\nx != 2 # not equal to\n\n\n[1] TRUE\n\n\n\n\n\n\nCode\n# logical operations\nx = 3\nx &gt; 2 # larger than\n\n\nTrue\n\n\nCode\nx &lt; 2 # smaller than\n\n\nFalse\n\n\nCode\nx == 2 # equal to\n\n\nFalse\n\n\nCode\nx != 2 # not equal to\n\n\nTrue\n\n\n\n\n\n\nCode\n\n# logical operations\n\nx = 3\n\n\n3\n\n\nCode\n\nx &gt; 2 # larger than\n\n\ntrue\n\n\nCode\n\nx &lt; 2 # smaller than\n\n\nfalse\n\n\nCode\n\nx == 2 # equal to\n\n\nfalse\n\n\nCode\n\nx != 2 # not equal to\n\n\ntrue",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#relational-operations",
    "href": "R-ComparisonWithPython.html#relational-operations",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "3.3 Relational operations",
    "text": "3.3 Relational operations\n\n\n\n\n\n\nCaution\n\n\n\n\nR: Boolean values are TRUE and FALSE.\nPython: Boolean values are True and False (case-sensitive).\n\n\n\n\nRPythonJulia\n\n\n\n\nCode\nT & F # and\n\n\n[1] FALSE\n\n\nCode\nT | F # or\n\n\n[1] TRUE\n\n\nCode\n!T # not\n\n\n[1] FALSE\n\n\n\n\n\n\nCode\nTrue & False # and\n\n\nFalse\n\n\nCode\nTrue | False # or\n\n\nTrue\n\n\nCode\nnot True # not\n\n\nFalse\n\n\n\n\n\n\nCode\n\ntrue & false # and\n\n\nfalse\n\n\nCode\n\ntrue | false # or\n\n\ntrue\n\n\nCode\n\n!true # not\n\n\nfalse",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#creating-vectors",
    "href": "R-ComparisonWithPython.html#creating-vectors",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.1 Creating vectors",
    "text": "4.1 Creating vectors\n\nRPythonJulia\n\n\n\nIn R, a vector is a collection of elements of the same data type, which is often used to store a variable of a dataset. For instance, a vector can store the income of a group of people, the final grades of students, etc.\nVector can be created using the function c() by listing all the values in the parenthesis, separated by comma ‘,’.\nc() stands for “combine”.\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, 10)\nIncome\n\n\n[1]  1  3  5 10\n\n\n\nVectors must contain elements of the same data type. If not, it will automatically convert elements into the same type (usually character type).\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, \"10\")\nIncome\n\n\n[1] \"1\"  \"3\"  \"5\"  \"10\"\n\n\n\n\n\nIn Python, a list is a collection of elements of different data types, which is often used to store a variable of a dataset. For instance, a list can store the income of a group of people, the final grades of students, etc.\nList can be created using the square brackets [] by listing all the values in the brackets, separated by comma ‘,’.\n\n\n\nCode\nIncome = [1, 3, 5, 10]\nIncome\n\n\n[1, 3, 5, 10]\n\n\n\nList can contain elements of different data types.\n\n\n\nCode\nIncome = [1, 3, 5, \"10\"]\nIncome\n\n\n[1, 3, 5, '10']\n\n\n\nIf you want to create a list with elements of the same numeric data type, you can use the numpy package.\n\n\n\nCode\nimport numpy as np\nIncome = np.array([1, 3, 5, 10])\nIncome\n\n\narray([ 1,  3,  5, 10])\n\n\n\n\n\nIn Julia, a vector is a collection of elements of the same data type, which is often used to store a variable of a dataset. For instance, a vector can store the income of a group of people, the final grades of students, etc.\nVector can be created using the square brackets [] by listing all the values in the brackets, separated by comma ‘,’.\n\n\n\nCode\n\nIncome = [1, 3, 5, 10]\n\n\n4-element Vector{Int64}:\n  1\n  3\n  5\n 10\n\n\n\nVector can contain elements of different data types. However, you will note that the data type is now changed to any rather than Int64.\n\n\n\nCode\n\nIncome = [1, 3, 5, \"10\"]\n\n\n4-element Vector{Any}:\n 1\n 3\n 5\n  \"10\"",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#indexing-and-subsetting",
    "href": "R-ComparisonWithPython.html#indexing-and-subsetting",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.2 Indexing and subsetting",
    "text": "4.2 Indexing and subsetting\n\n\n\n\n\n\nCaution\n\n\n\nR, Python, and Julia have different indexing rules.\n\nIn R and Julia, the index starts from 1.\nIn Python, the index starts from 0.\n\n\n\n\nRPythonJulia\n\n\n\nTo extract an element from a vector, we put the index of the element in a square bracket [ ].\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, 10)\nIncome[1] # extract the first element\n\n\n[1] 1\n\n\n\nIf we want to extract multiple elements, we can use a vector of indices.\n\n\n\nCode\nIncome[c(1,3)] # extract the first and third elements\n\n\n[1] 1 5\n\n\n\n\n\nTo extract an element from a list, we put the index of the element in a square bracket [ ].\n\n\n\nCode\nIncome = [1, 3, 5, 10]\nIncome[0] # extract the first element\n\n\n1\n\n\n\nIf we want to extract multiple elements, we can use a slice.\n\n\n\nCode\nIncome[0:3] # extract the first and third elements\n\n\n[1, 3, 5]\n\n\n\nWith numpy array, we can use the same syntax as R.\n\n\n\nCode\nIncome = np.array([1, 3, 5, 10])\nIncome[0] # extract the first element\n\n\n1\n\n\nCode\nIncome[[0,2]] # extract the first and third elements\n\n\narray([1, 5])\n\n\n\n\n\nTo extract an element from a vector, we put the index of the element in a square bracket [ ].\n\n\n\nCode\n\nIncome = [1, 3, 5, 10];\n\nIncome[1] # extract the first element\n\n\n1\n\n\n\nIf we want to extract multiple elements, we can use a slice.\n\n\n\nCode\n\nIncome[1:3] # extract the first and third elements\n\n\n3-element Vector{Int64}:\n 1\n 3\n 5",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#creating-numeric-sequences-with-fixed-steps",
    "href": "R-ComparisonWithPython.html#creating-numeric-sequences-with-fixed-steps",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.3 Creating numeric sequences with fixed steps",
    "text": "4.3 Creating numeric sequences with fixed steps\n\nRPythonJulia\n\n\nIt is also possible to easily create sequences with patterns\n\nuse seq() to create sequence with fixed steps\n\n\n\nCode\n# use seq()\nseq(from = 1, to = 2, by = 0.1)\n\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0\n\n\n\nIf the step is 1, there’s a convenient way using :\n\n\n\nCode\n1:5\n\n\n[1] 1 2 3 4 5\n\n\n\n\n\nIn base Python, we can use range() to create sequence with fixed steps\n\n\n\nCode\n# from 1 to 6, with step 1\nlist(range(1, 6)) # range() returns a range object, we need to convert it to a list\n\n\n[1, 2, 3, 4, 5]\n\n\n\nuse np.arange() to create sequence with fixed steps\n\n\n\nCode\nnp.arange(1, 2, 0.1)\n\n\narray([1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])\n\n\n\n\n\nIn Julia, we can use 1:5 to create sequence with fixed steps\n\n\n\nCode\n\n1:5\n\n\n1:5\n\n\n\nHowever, the julia object is not a integer vector, but a UnitRange{Int64} object.\n\n\n\nCode\n\ntypeof(1:5)\n\n\nUnitRange{Int64}",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#combine-multiple-vectors-into-one-c",
    "href": "R-ComparisonWithPython.html#combine-multiple-vectors-into-one-c",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.4 Combine multiple vectors into one: c()",
    "text": "4.4 Combine multiple vectors into one: c()\n\nRPythonJulia\n\n\n\nSometimes, we may want to combine multiple vectors into one. For instance, we may have collected income data from two different sources, and we want to combine them into one vector.\nWe can use c() to combine different vectors; this is very commonly used to concatenate vectors.\n\n\n\nCode\nIncome1 &lt;- 1:3 \nIncome2 &lt;- c(10, 15) \n\n\n\n\nCode\nc(Income1,Income2)\n\n\n[1]  1  2  3 10 15\n\n\n\n\n\nIn Python, we can use the + operator to concatenate lists.\n\n\n\nCode\nIncome1 = [1, 2, 3]\nIncome2 = [10, 15]\n\n\n\n\nCode\nIncome1 + Income2\n\n\n[1, 2, 3, 10, 15]\n\n\n\nFor numpy arrays, we can use np.concatenate() to concatenate arrays.\n\n\n\nCode\nIncome1 = np.array([1, 2, 3])\nIncome2 = np.array([10, 15])\n\n\n\n\nCode\nnp.concatenate((Income1, Income2))\n\n\narray([ 1,  2,  3, 10, 15])\n\n\n\n\n\nIn Julia, we can use the vcat() function to concatenate vectors.\n\n\n\nCode\n\nIncome1 = [1, 2, 3];\n\nIncome2 = [10, 15]; \n\nvcat(Income1, Income2)\n\n\n5-element Vector{Int64}:\n  1\n  2\n  3\n 10\n 15",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#replicating-elements",
    "href": "R-ComparisonWithPython.html#replicating-elements",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.5 Replicating elements",
    "text": "4.5 Replicating elements\n\nRPythonJulia\n\n\n\nWe can use the rep() function to replicate elements in a vector.\n\n\n\nCode\nrep(1:3, times = 2) # replicate 1:3 twice\n\n\n[1] 1 2 3 1 2 3\n\n\n\n\nCode\nrep(1:3, each = 2) # replicate each element in 1:3 twice\n\n\n[1] 1 1 2 2 3 3\n\n\n\n\n\nWe can use the * operator to replicate elements in a list.\n\n\n\nCode\n[1, 2, 3] * 2 # replicate 1:3 twice\n\n\n[1, 2, 3, 1, 2, 3]\n\n\n\nFor numpy arrays, we can use np.tile() to replicate elements.\n\n\n\nCode\nnp.tile([1, 2, 3], 2) # replicate 1:3 twice\n\n\narray([1, 2, 3, 1, 2, 3])\n\n\n\n\nCode\nnp.repeat([1, 2, 3], 2) # replicate each element in 1:3 twice \n\n\narray([1, 1, 2, 2, 3, 3])\n\n\n\n\n\nWe can use the repeat() function to replicate elements in a vector.\n\n\n\nCode\n\nrepeat([1, 2, 3], 2) # replicate 1:3 twice\n\n\n6-element Vector{Int64}:\n 1\n 2\n 3\n 1\n 2\n 3\n\n\n\n\nCode\n\nrepeat([1, 2, 3], inner = 2) # replicate each element in 1:3 twice\n\n\n6-element Vector{Int64}:\n 1\n 1\n 2\n 2\n 3\n 3",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#maximum-and-minimum",
    "href": "R-ComparisonWithPython.html#maximum-and-minimum",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.6 Maximum and minimum",
    "text": "4.6 Maximum and minimum\n\nRPythonJulia\n\n\n\nWe can use the max() and min() functions to find the maximum and minimum values in a vector.\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, 10)\n\nmax(Income) # maximum\n\n\n[1] 10\n\n\nCode\nmin(Income) # minimum\n\n\n[1] 1\n\n\n\n\n\nWe can use the max() and min() functions to find the maximum and minimum values in a list.\n\n\n\nCode\nIncome = [1, 3, 5, 10]\n\nmax(Income) # maximum\n\n\n10\n\n\nCode\nmin(Income) # minimum\n\n\n1\n\n\n\nFor numpy arrays, we can use np.max() and np.min() to find the maximum and minimum values.\n\n\n\nCode\nIncome = np.array([1, 3, 5, 10])\n\nnp.max(Income) # maximum\n\n\n10\n\n\nCode\nnp.min(Income) # minimum\n\n\n1\n\n\n\n\n\nWe can use the maximum() and minimum() functions to find the maximum and minimum values in a vector.\n\n\n\nCode\n\nIncome = [1, 3, 5, 10];\n\nmaximum(Income) # maximum\n\n\n10\n\n\nCode\n\nminimum(Income) # minimum\n\n\n1",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#sum-and-mean",
    "href": "R-ComparisonWithPython.html#sum-and-mean",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.7 Sum and mean",
    "text": "4.7 Sum and mean\n\nRPythonJulia\n\n\n\nWe can use the sum() and mean() functions to find the sum and mean values in a vector.\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, 10)\n\nsum(Income, na.rm = T) # sum and remove missing values\n\n\n[1] 19\n\n\nCode\nmean(Income, na.rm = T) # mean and remove missing values\n\n\n[1] 4.75\n\n\n\n\n\nWe can use the sum() and mean() functions to find the sum and mean values in a list.\n\n\n\nCode\nIncome = [1, 3, 5, 10]\n\nsum(Income) # sum\n\n\n19\n\n\nCode\nnp.mean(Income) # mean\n\n\n4.75\n\n\n\nFor numpy arrays, we can use np.sum() and np.mean() to find the sum and mean values.\n\n\n\nCode\nIncome = np.array([1, 3, 5, 10])\n\nnp.sum(Income) # sum\n\n\n19\n\n\nCode\nnp.mean(Income) # mean\n\n\n4.75\n\n\n\n\n\nWe can use the sum() and mean() functions to find the sum and mean values in a vector.\n\n\n\nCode\n\nIncome = [1, 3, 5, 10];\n\nsum(Income) # sum\n\n\n19\n\n\nCode\n\nmean(Income) # mean\n\n\n4.75",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#missing-values",
    "href": "R-ComparisonWithPython.html#missing-values",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.8 Missing values",
    "text": "4.8 Missing values\n\n\n\n\n\n\nCaution\n\n\n\n\nIn R, missing values are represented by NA.\nIn Python, missing values are represented by np.nan.\nIn Julia, missing values are represented by missing.\n\n\n\n\nRPythonJulia\n\n\n\nIn R, missing values are represented by NA.\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, NA)\n\nsum(Income, na.rm = T) # sum and remove missing values\n\n\n[1] 9\n\n\nCode\nmean(Income, na.rm = T) # mean and remove missing values\n\n\n[1] 3\n\n\n\n\n\nIn Python, missing values are represented by np.nan.\n\n\n\nCode\nIncome = [1, 3, 5, np.nan]\n\nnp.nansum(Income) # sum and remove missing values\n\n\n9.0\n\n\nCode\nnp.nanmean(Income) # mean and remove missing values\n\n\n3.0\n\n\n\n\n\nIn Julia, missing values are represented by missing. In order to take the sum or mean by removing missing values,\n\n\n\nCode\n\nIncome = [1, 3, 5, missing];\n\nsum(skipmissing(Income)) # sum and remove missing values\n\n\n9",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#element-wise-arithmetic-operations",
    "href": "R-ComparisonWithPython.html#element-wise-arithmetic-operations",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.9 Element-wise arithmetic operations",
    "text": "4.9 Element-wise arithmetic operations\n\n\n\n\n\n\nCaution\n\n\n\n\nR by default supports element-wise operations on vectors.\nPython by default does not support element-wise operations on lists. You need to use numpy arrays to do element-wise operations.\nJulia by default does not support element-wise operations on arrays. You need to use the . operator to do element-wise operations.\n\n\n\n\nRPythonJulia\n\n\n\nIf you operate on a vector with a single number, the operation will be applied to all elements in the vector\n\n\n\nCode\nIncome &lt;- c(1, 3, 5, 10)\n\nIncome + 2 # element-wise addition\n\n\n[1]  3  5  7 12\n\n\nCode\nIncome * 2 # element-wise multiplication\n\n\n[1]  2  6 10 20\n\n\n\n\n\nHowever, the base Python does not support element-wise operations on lists.\n\n\n\nCode\nIncome = [1, 3, 5, 10]\n\nIncome + 2 # element-wise addition\n\n\nTypeError: can only concatenate list (not \"int\") to list\n\n\nCode\nIncome * 2 # element-wise multiplication\n\n\n[1, 3, 5, 10, 1, 3, 5, 10]\n\n\n\nFor numpy arrays, the behavior is the same as R.\n\n\n\nCode\nIncome = np.array([1, 3, 5, 10])\n\nIncome + 2 # element-wise addition\n\n\narray([ 3,  5,  7, 12])\n\n\nCode\nIncome * 2 # element-wise multiplication\n\n\narray([ 2,  6, 10, 20])\n\n\n\n\n\nIf you operate on a vector with a single number, the operation will be applied to all elements in the vector. However, the base Julia does not support element-wise operations on arrays. In order to do element-wise operations, you need to use the . operator.\n\n\n\nCode\n\nIncome = [1, 3, 5, 10];\n\nIncome .+ 2 # element-wise addition\n\n\n4-element Vector{Int64}:\n  3\n  5\n  7\n 12\n\n\nCode\n\nIncome .* 2 # element-wise multiplication\n\n\n4-element Vector{Int64}:\n  2\n  6\n 10\n 20",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#vector-multiplication",
    "href": "R-ComparisonWithPython.html#vector-multiplication",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.10 Vector multiplication",
    "text": "4.10 Vector multiplication\n\nRPythonJulia\n\n\n\nIf the two vectors are of the same length, they can do element-wise operations, including element-wise addition and element-wise multiplication\n\n\n\nCode\nIncome1 &lt;- c(1, 3, 5, 10)\n\nIncome2 &lt;- c(2, 4, 6, 8)\n\nIncome1 + Income2 # element-wise addition\n\n\n[1]  3  7 11 18\n\n\nCode\nIncome1 * Income2 # element-wise multiplication\n\n\n[1]  2 12 30 80\n\n\n\n\n\nFor numpy arrays, we can use np.multiply() to do element-wise multiplication.\n\n\n\nCode\nIncome1 = np.array([1, 3, 5, 10])\n\nIncome2 = np.array([2, 4, 6, 8])\n\nnp.add(Income1, Income2) # element-wise addition\n\n\narray([ 3,  7, 11, 18])\n\n\nCode\nnp.multiply(Income1, Income2) # element-wise multiplication\n\n\narray([ 2, 12, 30, 80])\n\n\n\n\n\nIf the two vectors are of the same length, they can do element-wise operations, including element-wise addition and element-wise multiplication\n\n\n\nCode\n\nIncome1 = [1, 3, 5, 10];\n\nIncome2 = [2, 4, 6, 8];\n\nIncome1 .+ Income2 # element-wise addition\n\n\n4-element Vector{Int64}:\n  3\n  7\n 11\n 18\n\n\nCode\n\nIncome1 .* Income2 # element-wise multiplication\n\n\n4-element Vector{Int64}:\n  2\n 12\n 30\n 80",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#max-and-min-of-2-vectors",
    "href": "R-ComparisonWithPython.html#max-and-min-of-2-vectors",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "4.11 Max and min of 2 vectors",
    "text": "4.11 Max and min of 2 vectors\n\nRPythonJulia\n\n\n\nWe can use the pmax() and pmin() functions to find the element-wise maximum and minimum values of two vectors.\n\n\n\nCode\nIncome1 &lt;- c(1, 3, 5, 10)\n\nIncome2 &lt;- c(2, 4, 6, 8)\n\npmax(Income1, Income2) # element-wise maximum\n\n\n[1]  2  4  6 10\n\n\nCode\npmin(Income1, Income2) # element-wise minimum\n\n\n[1] 1 3 5 8\n\n\n\n\n\nWe can use the np.maximum() and np.minimum() functions to find the element-wise maximum and minimum values of two numpy arrays.\n\n\n\nCode\nIncome1 = np.array([1, 3, 5, 10])\n\nIncome2 = np.array([2, 4, 6, 8])\n\nnp.maximum(Income1, Income2) # element-wise maximum\n\n\narray([ 2,  4,  6, 10])\n\n\nCode\nnp.minimum(Income1, Income2) # element-wise minimum\n\n\narray([1, 3, 5, 8])\n\n\n\n\n\nWe can use the max() and min() functions to find the element-wise maximum and minimum values of two vectors.\n\n\n\nCode\n\nIncome1 = [1, 3, 5, 10];\n\nIncome2 = [2, 4, 6, 8];\n\nmax.(Income1, Income2) # element-wise maximum\n\n\n4-element Vector{Int64}:\n  2\n  4\n  6\n 10\n\n\nCode\n\nmin.(Income1, Income2) # element-wise minimum\n\n\n4-element Vector{Int64}:\n 1\n 3\n 5\n 8",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#creating-strings",
    "href": "R-ComparisonWithPython.html#creating-strings",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "5.1 Creating strings",
    "text": "5.1 Creating strings\n\nRPythonJulia\n\n\n\nCharacters are enclosed within a pair of quotation marks.\nSingle or double quotation marks can both work.\nIf even a character may contain numbers, it will be treated as a character, and R will not perform any mathematical operations on it.\n\n\n\nCode\nstr1 &lt;- \"1 + 1 = 2\"\n\n\n\n\n\nStrings are enclosed within a pair of quotation marks.\nSingle or double quotation marks can both work.\n\n\n\nCode\nstr1 = \"1 + 1 = 2\"\n\n\n\n\n\nIn Julia, single quotation marks (') are used for defining individual characters. Double quotation marks (\") are used for defining strings.\n\n\n\nCode\n\ncharacter1 = '1'\n\n\n'1': ASCII/Unicode U+0031 (category Nd: Number, decimal digit)\n\n\nCode\nstr1 = \"1 + 1 = 2\"\n\n\n\"1 + 1 = 2\"",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#concatenating-strings",
    "href": "R-ComparisonWithPython.html#concatenating-strings",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "5.2 Concatenating strings",
    "text": "5.2 Concatenating strings\n\nRPythonJulia\n\n\n\nWe can use the paste() function to concatenate strings.\n\n\n\nCode\nstr1 &lt;- \"1 + 1 = \"\nstr2 &lt;- \"2\"\n\npaste(str1, str2)\n\n\n[1] \"1 + 1 =  2\"\n\n\n\n\n\nWe can use the + operator to concatenate strings.\n\n\n\nCode\nstr1 = \"1 + 1 = \"\nstr2 = \"2\"\n\nstr1 + str2\n\n\n'1 + 1 = 2'\n\n\n\n\n\nWe can use the * operator to concatenate strings.\n\n\n\nCode\n\nstr1 = \"1 + 1 = \"\n\n\n\"1 + 1 = \"\n\n\nCode\n\nstr2 = \"2\"\n\n\n\"2\"\n\n\nCode\n\nstr1 * str2\n\n\n\"1 + 1 = 2\"",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#checking-the-number-of-elements-in-a-vector-length",
    "href": "R-ComparisonWithPython.html#checking-the-number-of-elements-in-a-vector-length",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "5.3 Checking the number of elements in a vector: length()",
    "text": "5.3 Checking the number of elements in a vector: length()\n\nRPythonJulia\n\n\n\nYou can measure the length of a vector using the command length()\n\n\n\nCode\nx &lt;- c('R',' is', ' the', ' best', ' language')\nlength(x)\n\n\n[1] 5\n\n\n\n\n\nYou can measure the length of a list using the command len()\n\n\n\nCode\nx = ['R',' is', ' the', ' best', ' language']\n\nlen(x)\n\n\n5\n\n\n\nFor numpy arrays, you can use the shape attribute to get the shape of the array.\n\n\n\nCode\nx = np.array(['Python',' is', ' the', ' best', ' language'])\n\nx.shape\n\n\n(5,)\n\n\n\n\n\nYou can measure the length of a vector using the command length()\n\n\n\nCode\n\nx = [\"Julia\", \" is\", \" the\", \" best\", \" language\"]\n\n\n5-element Vector{String}:\n \"Julia\"\n \" is\"\n \" the\"\n \" best\"\n \" language\"\n\n\nCode\n\nlength(x)\n\n\n5",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#special-relational-operation-in",
    "href": "R-ComparisonWithPython.html#special-relational-operation-in",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "5.4 Special relational operation: %in%",
    "text": "5.4 Special relational operation: %in%\n\nRPythonJulia\n\n\n\nA special relational operation is %in% in R, which tests whether an element exists in the object.\n\n\n\nCode\nx &lt;- c(1,3,8,7) \n\n3 %in% x\n\n\n[1] TRUE\n\n\nCode\n2 %in% x\n\n\n[1] FALSE\n\n\n\n\n\nIn Python, we can use the in operator to test whether an element exists in the object.\n\n\n\nCode\nx = [1, 3, 8, 7]\n\n3 in x\n\n\nTrue\n\n\nCode\n2 in x\n\n\nFalse\n\n\n\n\n\nIn Julia, we can use the in operator to test whether an element exists in the object.\n\n\n\nCode\n\nx = [1, 3, 8, 7];\n\n3 in x\n\n\ntrue",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#matrices-creating-matrices",
    "href": "R-ComparisonWithPython.html#matrices-creating-matrices",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.1 Matrices: creating matrices",
    "text": "6.1 Matrices: creating matrices\n\n\n\n\n\n\nCaution\n\n\n\nWhen creating R matrix using matrix(), the sequence of elements is filled by column. This by-column is named as column-major order.\nWhen creating Python matrix using np.array(), the sequence of elements is filled by row. This by-row is named as row-major order.\n\n\n\nRPythonJulia\n\n\n\nA matrix can be created using the command matrix()\n\nthe first argument is the vector to be converted into matrix\nthe second argument is the number of rows\nthe last argument is the number of cols\n\n\n\n\nCode\nmatrix(1:9, nrow = 3, ncol = 3)\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n\nA matrix can be created using the numpy package, np.array() function, where the argument is a list of lists, where each list is a row of the matrix\n\n\n\nCode\nimport numpy as np\n\nnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n\n\nA matrix can be created using the base Julia using square brackets [] and semicolon ; to separate rows.\n\n\n\nCode\n\n[1 2 3; 4 5 6; 7 8 9]\n\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#creating-matrices-combine-matrices",
    "href": "R-ComparisonWithPython.html#creating-matrices-combine-matrices",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.2 Creating matrices: combine matrices",
    "text": "6.2 Creating matrices: combine matrices\n\nRPythonJulia\n\n\nWe can use cbind() and rbind() to concatenate vectors and matrices into new matrices.\n\ncbind() does the column binding\n\n\n\nCode\na &lt;- matrix(1:6, nrow = 2, ncol = 3)\n\na\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nCode\ncbind(a, a) # column bind\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    3    5    1    3    5\n[2,]    2    4    6    2    4    6\n\n\n\nrbind() does the row binding\n\n\n\nCode\nrbind(a, a) # row bind\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n[3,]    1    3    5\n[4,]    2    4    6\n\n\n\n\n\nWe can use np.concatenate() to concatenate arrays.\n\n\n\nCode\na = np.array([[1, 2, 3], [4, 5, 6]])\n\na\n\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nCode\nnp.concatenate((a, a), axis = 1) # column bind\n\n\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]])\n\n\nCode\nnp.concatenate((a, a), axis = 0) # row bind\n\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]])\n\n\n\n\n\nWe can use the hcat() and vcat() functions to concatenate matrices.\n\n\n\nCode\n\na = [1 2 3; 4 5 6]\n\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nCode\n\nhcat(a, a) # column bind\n\n\n2×6 Matrix{Int64}:\n 1  2  3  1  2  3\n 4  5  6  4  5  6\n\n\nCode\n\nvcat(a, a) # row bind\n\n\n4×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 1  2  3\n 4  5  6",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#matrices-indexing-and-subsetting",
    "href": "R-ComparisonWithPython.html#matrices-indexing-and-subsetting",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.3 Matrices: indexing and subsetting",
    "text": "6.3 Matrices: indexing and subsetting\nMatrices have two dimensions: rows and columns. Therefore, to extract elements from a matrix, we just need to specify which row(s) and which column(s) we want.\n\nRPythonJulia\n\n\n\n\nCode\nx &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\nExtract the element in the 2nd row, 3rd column.\n\nuse square bracket with a coma inside [ , ] to indicate subsetting; the argument before coma is the row index, and the argument after the coma is the column index.\n\n2 is specified for row index, so we will extract elements from the first row\n3 is specified for column index, so we will extract elements from the the second column\nAltogether, we extract a single element in row 2, column 3.\n\n\n\n\n\nCode\nx[2,3] # the element in the 2nd row, 3rd column\n\n\n[1] 8\n\n\n\nIf we leave blank for a dimension, we extract all elements along that dimension.\n\nif we want to take out the entire first row\n\n1 is specified for the row index\ncolumn index is blank\n\n\n\n\n\nCode\nx[1,] # all elements in the first row\n\n\n[1] 1 4 7\n\n\n\n\n\n\nCode\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nx\n\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\nExtract the element in the 2nd row, 3rd column.\n\n\n\nCode\nx[1,2] # the element in the 2nd row, 3rd column\n\n\n6\n\n\n\nIf we leave blank for a dimension, we extract all elements along that dimension.\n\n\n\nCode\nx[0,:] # all elements in the first row\n\n\narray([1, 2, 3])\n\n\n\n\n\n\nCode\n\nx = [1 2 3; 4 5 6; 7 8 9];\n\n\n\nExtract the element in the 2nd row, 3rd column.\n\n\n\nCode\n\nx[2,3] # the element in the 2nd row, 3rd column\n\n\n6\n\n\n\nDifferent from R, we need to use : to extract all elements along that dimension.\n\n\n\nCode\n\nx[1,:] # all elements in the first row\n\n\n3-element Vector{Int64}:\n 1\n 2\n 3",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#matrices-check-dimensions-and-variable-types",
    "href": "R-ComparisonWithPython.html#matrices-check-dimensions-and-variable-types",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.4 Matrices: check dimensions and variable types",
    "text": "6.4 Matrices: check dimensions and variable types\n\nRPythonJulia\n\n\n\nYou can verify the size of the matrix using the command dim(); or nrow() and ncol()\n\n\n\nCode\nx &lt;- matrix(1:9, nrow = 3, ncol = 3)\n\ndim(x)\n\n\n[1] 3 3\n\n\nCode\nnrow(x)\n\n\n[1] 3\n\n\nCode\nncol(x)\n\n\n[1] 3\n\n\n\nYou can get the data type info using the command str()\n\n\n\nCode\nstr(x)\n\n\n int [1:3, 1:3] 1 2 3 4 5 6 7 8 9\n\n\n\n\n\nYou can verify the size of the matrix using the shape attribute\n\n\n\nCode\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nx.shape\n\n\n(3, 3)\n\n\n\nYou can get the data type info using the dtype attribute\n\n\n\nCode\nx.dtype\n\n\ndtype('int64')\n\n\n\n\n\nYou can verify the size of the matrix using the size() function\n\n\n\nCode\n\nx = [1 2 3; 4 5 6; 7 8 9]\n\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nCode\n\nsize(x)\n\n\n(3, 3)",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#matrices-special-operations",
    "href": "R-ComparisonWithPython.html#matrices-special-operations",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.5 Matrices: special operations",
    "text": "6.5 Matrices: special operations\n\n6.5.1 Creating a diagonal matrix\n\nRPythonJulia\n\n\n\nWe can use the diag() function to create a diagonal matrix.\n\n\n\nCode\ndiag(1:3)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n\n\n\n\nWe can use the np.diag() function to create a diagonal matrix.\n\n\n\nCode\nnp.diag([1, 2, 3])\n\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\n\n\nWe can use the diagm() function to create a diagonal matrix.\n\n\n\nCode\nusing LinearAlgebra\ndiagm(0 =&gt; [1, 2, 3])\n\n\n3×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n\n\n\n\n\n\n\n6.5.2 Creating an identity matrix\n\nRPythonJulia\n\n\n\nWe can use the diag() function to create an identity matrix.\n\n\n\nCode\ndiag(3)\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\n\n\nWe can use the np.eye() function to create an identity matrix.\n\n\n\nCode\nnp.eye(3)\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n\nWe can use the I() function to create an identity matrix.\n\n\n\nCode\n\nI(3)\n\n\n3×3 Diagonal{Bool, Vector{Bool}}:\n 1  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#matrices-operations-matrix-addition-and-multiplication",
    "href": "R-ComparisonWithPython.html#matrices-operations-matrix-addition-and-multiplication",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.6 Matrices’ operations: matrix addition and multiplication",
    "text": "6.6 Matrices’ operations: matrix addition and multiplication\n\nRPythonJulia\n\n\n\nIf the two matrices are of the same dimensions, they can do element-wise operations, including element-wise addition and element-wise multiplication\n\n\n\nCode\nset.seed(123)\n\nx = matrix(rnorm(9), nrow = 3, ncol = 3)\n\nz = matrix(rnorm(9), nrow = 3, ncol = 3)\n\nx + z   # elementwise addition\n\n\n           [,1]      [,2]       [,3]\n[1,] -1.0061376 0.4712798  2.2478293\n[2,]  0.9939043 0.2399705 -0.7672108\n[3,]  1.9185221 1.1592239 -2.6534700\n\n\n\n\nCode\nx * x \n\n\n           [,1]        [,2]      [,3]\n[1,] 0.31413295 0.004971433 0.2124437\n[2,] 0.05298168 0.016715318 1.6003799\n[3,] 2.42957161 2.941447909 0.4717668\n\n\n\nIf we want to perform the matrix multiplication as in linear algebra, we need to use %*%\n\nx and y must have conforming dimensions\n\n\n\n\nCode\nx\n\n\n           [,1]       [,2]       [,3]\n[1,] -0.5604756 0.07050839  0.4609162\n[2,] -0.2301775 0.12928774 -1.2650612\n[3,]  1.5587083 1.71506499 -0.6868529\n\n\nCode\ny = matrix(rnorm(9), nrow = 3, ncol = 3)\nx %*% y # matrix multiplication\n\n\n           [,1]       [,2]       [,3]\n[1,] -0.9186059 -0.2861301  0.6175429\n[2,]  1.1282999  0.8396152 -1.1340507\n[3,]  1.0157790 -1.5987826 -4.4424790\n\n\n\n\n\nIf the two matrices are of the same dimensions, they can do element-wise operations, including element-wise addition and element-wise multiplication\n\n\n\nCode\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\ny = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nx + y # elementwise addition\n\n\narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])\n\n\n\n\nCode\nx * y # elementwise multiplication\n\n\narray([[ 1,  4,  9],\n       [16, 25, 36],\n       [49, 64, 81]])\n\n\n\nIf we want to perform the matrix multiplication as in linear algebra, we need to use @\n\nx and y must have conforming dimensions\n\n\n\n\nCode\nx @ y # matrix multiplication\n\n\narray([[ 30,  36,  42],\n       [ 66,  81,  96],\n       [102, 126, 150]])\n\n\n\n\n\nIf the two matrices are of the same dimensions, they can do element-wise operations, including element-wise addition and element-wise multiplication. It’s recommended to use . to indicate element-wise operations\n\n\n\nCode\n\nx = [1 2 3; 4 5 6; 7 8 9]\n\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nCode\n\ny = [1 2 3; 4 5 6; 7 8 9]\n\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nCode\n\nx .+ y # elementwise addition\n\n\n3×3 Matrix{Int64}:\n  2   4   6\n  8  10  12\n 14  16  18",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#matrices-operations-inverse-and-transpose",
    "href": "R-ComparisonWithPython.html#matrices-operations-inverse-and-transpose",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "6.7 Matrices’ operations: inverse and transpose",
    "text": "6.7 Matrices’ operations: inverse and transpose\n\nRPythonJulia\n\n\n\nWe use t() to do matrix transpose\n\n\n\nCode\nx = matrix(rnorm(9), nrow = 3, ncol = 3)\nx\n\n\n           [,1]       [,2]      [,3]\n[1,]  0.1533731  0.4264642 0.8781335\n[2,] -1.1381369 -0.2950715 0.8215811\n[3,]  1.2538149  0.8951257 0.6886403\n\n\nCode\nt(x) # transpose\n\n\n          [,1]       [,2]      [,3]\n[1,] 0.1533731 -1.1381369 1.2538149\n[2,] 0.4264642 -0.2950715 0.8951257\n[3,] 0.8781335  0.8215811 0.6886403\n\n\n\nWe use solve() to get the inverse of an matrix\n\n\n\nCode\nx\n\n\n           [,1]       [,2]      [,3]\n[1,]  0.1533731  0.4264642 0.8781335\n[2,] -1.1381369 -0.2950715 0.8215811\n[3,]  1.2538149  0.8951257 0.6886403\n\n\nCode\nsolve(t(x)%*%x) # inverse; must be on a square matrix\n\n\n          [,1]      [,2]      [,3]\n[1,]  417.2893 -803.5341  299.4938\n[2,] -803.5341 1548.5735 -577.2074\n[3,]  299.4938 -577.2074  215.6665\n\n\n\n\n\nWe use T to do matrix transpose\n\n\n\nCode\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nx\n\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nCode\nx.T # transpose\n\n\narray([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])\n\n\n\nWe use np.linalg.inv() to get the inverse of an matrix\n\n\n\nCode\nnp.linalg.inv(x.T @ x) # inverse; must be on a square matrix\n\n\narray([[ 5.62949953e+14, -1.12589991e+15,  5.62949953e+14],\n       [-1.12589991e+15,  2.25179981e+15, -1.12589991e+15],\n       [ 5.62949953e+14, -1.12589991e+15,  5.62949953e+14]])\n\n\n\n\n\nWe use transpose() to do matrix transpose\n\n\n\nCode\n\nx = [1 2 3; 4 5 6; 7 8 9]\n\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nCode\n\ntranspose(x) # transpose\n\n\n3×3 transpose(::Matrix{Int64}) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\n\n\nWe use inv() to get the inverse of an matrix\n\n\n\nCode\n\ninv(transpose(x) * x) # inverse; must be on a square matrix\n\n\n3×3 Matrix{Float64}:\n  5.6295e14  -1.1259e15   5.6295e14\n -1.1259e15   2.2518e15  -1.1259e15\n  5.6295e14  -1.1259e15   5.6295e14",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#ifelse",
    "href": "R-ComparisonWithPython.html#ifelse",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "7.1 if/else",
    "text": "7.1 if/else\n\nRPythonJulia\n\n\nSometimes, you want to run your code based on different conditions. For instance, if the observation is a missing value, then use the population average to impute the missing value. This is where if/else kicks in.\nif (condition == TRUE) {\n  action 1\n} else if (condition == TRUE ){\n  action 2\n} else {\n  action 3\n}\nExample 1:\n\n\nCode\na &lt;- 15\n\nif (a &gt; 10) {\nlarger_than_10 &lt;- TRUE  \n} else {\n  larger_than_10 &lt;- FALSE\n}\n\nlarger_than_10  \n\n\n[1] TRUE\n\n\nExample 2:\n\n\nCode\nx &lt;- -5\nif(x &gt; 0){\n  print(\"x is a non-negative number\")\n} else {\n  print(\"x is a negative number\")\n}\n\n\n[1] \"x is a negative number\"\n\n\n\n\n\n\nCode\na = 15\n\nif a &gt; 10:\n    larger_than_10 = True\nelse:\n    larger_than_10 = False\n\nlarger_than_10\n\n\nTrue\n\n\nExample 2:\n\n\nCode\nx = -5\n\nif x &gt; 0:\n    print(\"x is a non-negative number\")\nelse:\n    print(\"x is a negative number\")\n\n\nx is a negative number\n\n\n\n\n\n\nCode\n\na = 15\n\n\n15\n\n\nCode\n\nif a &gt; 10\n    larger_than_10 = true\nelse\n    larger_than_10 = false\nend\n\n\ntrue\n\n\nCode\n\nlarger_than_10\n\n\ntrue\n\n\nExample 2:\n\n\nCode\n\nx = -5\n\n\n-5\n\n\nCode\n\nif x &gt; 0\n    println(\"x is a non-negative number\")\nelse\n    println(\"x is a negative number\")\nend\n\n\nx is a negative number",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#loops",
    "href": "R-ComparisonWithPython.html#loops",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "7.2 Loops",
    "text": "7.2 Loops\n\n\n\n\n\n\nCaution\n\n\n\nBoth R and Python are very inefficient in terms of loops. Therefore, codes should be written in matrix form to utlize the vectorization as much as possible.\nIn constrast, Julia is very efficient at loops. Thus code readability should be prioritized instead of vectorization.\n\n\n\nRPythonJulia\n\n\nAs the name suggests, in a loop the program repeats a set of instructions many times, until the stopping criteria is met.\nLoop is very useful for repetitive jobs.\n\n\nCode\nfor (i in 1:10){ # i is the iterator\n  # loop body: gets executed each time\n  # the value of i changes with each iteration\n}\n\n\nExample:\n\n\nCode\nfor (i in 1:5){\n  print(i)\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\n\nCode\nfor i in range(1, 6):\n    print(i)\n\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\nCode\n\nfor i in 1:5\n    println(i)\nend\n\n\n1\n2\n3\n4\n5",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "R-ComparisonWithPython.html#user-defined-functions",
    "href": "R-ComparisonWithPython.html#user-defined-functions",
    "title": "Side-to-Side Comparison between R, Python, and Julia",
    "section": "7.3 User-Defined Functions",
    "text": "7.3 User-Defined Functions\nA function takes the argument as input, run some specified actions, and then return the result to us.\nFunctions are very useful. When we would like to test different ideas, we can combine functions with loops: We can write a function which takes different parameters as input, and we can use a loop to go through all the possible combinations of parameters.\n\nRPythonJulia\n\n\nHere is how to define a function in general:\n\n\nCode\nfunction_name &lt;- function(arg1 ,arg2 = default_value){\n  # write the actions to be done with arg1 and arg2\n  # you can have any number of arguments, with or without defaults\n  return() # the last line is to return some value \n}\n\n\nExample:\n\n\nCode\nmagic &lt;- function( x, y){\n  return(x^2 + y)\n}\n\nmagic(1,3)\n\n\n[1] 4\n\n\n\n\nHere is how to define a function in general:\n\n\nCode\ndef function_name(arg1, arg2 = default_value):\n    # write the actions to be done with arg1 and arg2\n    # you can have any number of arguments, with or without defaults\n    return # the last line is to return some value\n\n\nNameError: name 'default_value' is not defined\n\n\nExample:\n\n\nCode\ndef magic(x, y):\n    return x**2 + y\n\nmagic(1, 3)\n\n\n4\n\n\n\n\nHere is how to define a function in general:\n\n\nCode\n\nfunction function_name(arg1, arg2 = default_value)\n    # write the actions to be done with arg1 and arg2\n    # you can have any number of arguments, with or without defaults\n    return # the last line is to return some value\nend\n\n\nfunction_name (generic function with 2 methods)\n\n\nExample:\n\n\nCode\n\nfunction magic(x, y)\n    return x^2 + y\nend\n\n\nmagic (generic function with 1 method)\n\n\nCode\n\nmagic(1, 3)\n\n\n4",
    "crumbs": [
      "R Tutorials",
      "Side-by-Side Comparison between R and Python"
    ]
  },
  {
    "objectID": "RTip-BLAS.html",
    "href": "RTip-BLAS.html",
    "title": "Speed Up Matrix Operations",
    "section": "",
    "text": "If your codes include lots of vector/matrix operations, especially those computation intensive tasks such as matrix inverse,1 you will probably feel that the default R is slow. Your intuition is likely correct! The reason is that the default R distributions from CRAN make use of the default BLAS/LAPACK implementation for linear algebra operations. The purpose of using such reference BLAS libraries by the development team is good: These implementations are built to be stable and cross platform compatible. However, the price that comes with stability is the sacrifice of speed. Is there a way to tweak your R settings to significantly boost the running speed? The answer is yes, and the steps are actually quite simple. We will review the steps to install highly optimized libraries and benchmark their performance.",
    "crumbs": [
      "Advanced R Tips",
      "Speed Up Matrix Operations"
    ]
  },
  {
    "objectID": "RTip-BLAS.html#how-to-switch-to-veclib-blas",
    "href": "RTip-BLAS.html#how-to-switch-to-veclib-blas",
    "title": "Speed Up Matrix Operations",
    "section": "2.1 How to Switch to VecLib BLAS",
    "text": "2.1 How to Switch to VecLib BLAS\nOn MacOS, the R’s framework path is /Library/Frameworks/R.framework/Resources/lib. To go to the folder, open terminal on your Mac, and enter the following command:\n\n\nCode\ncd /Library/Frameworks/R.framework/Resources/lib\nls -l\n\n\ntotal 16336\n-rwxrwxr-x  1 root     admin  3988192 24 Jun 11:57 libR.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libR.dylib.dSYM\n-rwxrwxr-x  1 root     admin   193440 24 Jun 11:57 libRblas.0.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRblas.0.dylib.dSYM\nlrwxr-xr-x  1 weimiao  admin       48 28 Aug 21:10 libRblas.dylib -&gt; /opt/homebrew/opt/openblas/lib/libopenblas.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRblas.dylib.dSYM\n-rwxrwxr-x  1 root     admin   154464 24 Jun 11:57 libRblas.vecLib.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRblas.vecLib.dylib.dSYM\n-rwxrwxr-x  1 root     admin  1624976 24 Jun 11:57 libRlapack.0.dylib\nlrwxr-xr-x  1 weimiao  admin       46 28 Aug 21:11 libRlapack.dylib -&gt; /opt/homebrew/opt/openblas/lib/liblapack.dylib\ndrwxrwxr-x  3 root     admin       96 23 Apr  2022 libRlapack.dylib.dSYM\n-rw-rw-r--  1 root     admin   157792 24 Jun 11:57 libgcc_s.1.1.dylib\n-rwxrwxr-x  1 root     admin  1865904 24 Jun 11:57 libgfortran.5.dylib\n-rwxrwxr-x  1 root     admin   367040 24 Jun 11:57 libquadmath.0.dylib\n\n\nwhich will change (c) the directory (d) to R’s framework folder. In this folder, you will see a few files by hitting ls:\n\nlibRblas.0.dylib is the default BLAS library.\nlibRblas.vecLib.dylib is the more efficient vecLib BLAS, which we are going to switch to.\nlibRblas.dylib is of alias file type. This means it’s kind of like a shortcut and points to a another file we set. By default, libRblas.dylib is pointed to libRblas.0.dylib, so R uses the default BLAS library. All we need to do is to relink the libRblas.0.dylib to libRblas.vecLib.dylib, such that R will use the vecLib.\n\nTo do so, type the following command in terminal:\n\n\nCode\nln -sf libRblas.vecLib.dylib libRblas.dylib\n\n\nIf vecLib has issues on your computer3 and you would like to switch back to the default BLAS/LAPACK, simply link the libRblas.dylib back to libRblas.0.dylib, by entering the following command in terminal:\n\n\nCode\nln -sf libRblas.0.dylib libRblas.dylib",
    "crumbs": [
      "Advanced R Tips",
      "Speed Up Matrix Operations"
    ]
  },
  {
    "objectID": "RTip-BLAS.html#performance-comparison",
    "href": "RTip-BLAS.html#performance-comparison",
    "title": "Speed Up Matrix Operations",
    "section": "2.2 Performance Comparison",
    "text": "2.2 Performance Comparison\nWe will compare the performance before and after switching to vecLib using the following R codes. The code generates a 1000 by 1000 matrix, and obtains the inverse of that matrix. I use microbenchmark package to run the inverse operation for 100 times, and capture the performance metrics.\n\n\nCode\nset.seed(888)\nnd = 1000\na &lt;- matrix(rnorm(nd^2), nd, nd)\nlibrary(microbenchmark)\nmb &lt;- microbenchmark(\n  solve(a),\n  times = 100,\n  unit = \"ms\"\n)\n\n\nWhen I use the default BLAS/LAPACK, the benchmark result is as follows:\n\n\nUnit: milliseconds\n     expr     min      lq     mean   median       uq      max neval\n solve(a) 383.953 388.517 400.0499 392.0261 402.7341 629.8773   100\n\n\nAfter we switch to vecLib, the benchmarks are as follows. As can be seen, the speed has increased dramatically!\n\n\nUnit: milliseconds\n     expr     min       lq     mean   median       uq      max neval\n solve(a) 27.6078 30.08605 31.93745 30.75814 31.26055 53.32608   100",
    "crumbs": [
      "Advanced R Tips",
      "Speed Up Matrix Operations"
    ]
  },
  {
    "objectID": "RTip-BLAS.html#footnotes",
    "href": "RTip-BLAS.html#footnotes",
    "title": "Speed Up Matrix Operations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor instance, in my research projects that involve structural modelling, I often need to write my own codes in matrix form to compute the value functions, policy functions, equlibrium computation, etc.↩︎\nApple’s Accelerate framework provides high-performance, energy-efficient computations on the CPU by leveraging its vector-processing capability. For more details, refer to Apple’s documentation here.↩︎\nAs warned by CRAN, “Although fast, it is not under our control and may possibly deliver inaccurate results.”↩︎",
    "crumbs": [
      "Advanced R Tips",
      "Speed Up Matrix Operations"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsII.html",
    "href": "Case-PredictiveAnalyticsII.html",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (II): Customer Targeting Using Supervised Learning",
    "section": "",
    "text": "Last week, M&S used unsupervised learning to segment customers based on their spending and income. While segmentation allows M&S to identify customer groups and tailor general marketing strategies, it doesn’t directly tell us which customers M&S should target. In this part of the case study, we take a step further by targeting individual customers using supervised learning models, which help identify who is most likely to respond positively to marketing offers.\nRecently, M&S has launched its highly anticipated Beauty Advent Calendar for 2024, a curated selection of beauty and skincare products worth over £300. This limited-edition calendar is available to customers for only £50. With the holiday season approaching, M&S wants to maximize the reach and response of its marketing campaign by promoting the advent calendar offer to the right customers.\nM&S decides to use a conventional mailing marketing strategy, where customers receive color-printed leaflets via Royal Mail to their doorsteps. Each mail costs £1.5 to produce and another £0.5 to mail to the customers. If a customer responds to the offer, M&S expects them to spend £35 on full-price clothing, homeware or beauty, and purchase the advent calendar at £50. The COGS for clothing, homeware, and beauty products is 85%. And the COGS for the advent calendar is 90%.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsII.html#break-even-analysis-break-even-response-rate",
    "href": "Case-PredictiveAnalyticsII.html#break-even-analysis-break-even-response-rate",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (II): Customer Targeting Using Supervised Learning",
    "section": "2.1 Break-Even Analysis: Break-Even Response Rate",
    "text": "2.1 Break-Even Analysis: Break-Even Response Rate\nIn order to break-even, we can calculate the break-even response rate from customers, which is the minimum response rate we need of a customer in order not to lose money from sending the marketing offer.\nOnly if a customer responds to us with the minimum response rate can we recover the costs of making a marketing offer. The higher the response rate, the more expected profit we can make from the marketing campaign.\nIf we send offers to customers whose expected response rate is lower than the break-even response rate, we make a loss by expectation.\n\nQuestion 5Answer\n\n\nCalculate the break-even response rate from customers. The break-even response rate is the minimum response rate we need from a customer in order not to lose money from sending the marketing offer.\n\n\n\n\nCode\nbreak_even_response &lt;- cost_per_offer / profit_per_customer\n\nbreak_even_response\n\n\n[1] 0.195122\n\n\n\n\n\nNext, we will use supervised learning models to predict the response rate of individual customers. We will then target customers whose predicted response rate is higher than the break-even response rate.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Case-PredictiveAnalyticsII.html#data-analytics-using-supervised-machine-learning",
    "href": "Case-PredictiveAnalyticsII.html#data-analytics-using-supervised-machine-learning",
    "title": "Improving Marketing Efficiency Using Predictive Analytics for M&S (II): Customer Targeting Using Supervised Learning",
    "section": "2.2 Data Analytics Using Supervised Machine Learning",
    "text": "2.2 Data Analytics Using Supervised Machine Learning\n\n2.2.1 Select meaningful features/predictors from data_full, named data_full_small\nWe need to select meaningful features/predictors from the dataset to train the supervised learning models. We can use the following features as predictors:\n\nSince ID is solely a customer identifier, it should be removed from the final data\nSince Dt_Customer is a character string, which cannot be directly used in the model, we should also remove it\n\n\n\nCode\n# Use select to remove the above two variables\n# Tip: a minus sign before the variable name can remove that variable\n\ndata_full_smaller &lt;- data_full %&gt;%\n    select(-ID) %&gt;%\n    select(-Dt_Customer)\n\n\n\n\n2.2.2 Construct a training set and a test set\nTasks: randomly divide data_full into a training set and a test set\n\n\nCode\n# use nrow() to count the number of rows in data_full_smaller\nn_rows_data_full &lt;- nrow(data_full_smaller)\n\n# set a seed, so that we can get the same set of results every time we rerun the model\nset.seed(8888)\n\n# use sample() to randomly draw row index from data_full\ntraining_index &lt;- sample(\n    x = 1:n_rows_data_full, # draw from 1 until 2000\n    size = 0.7 * n_rows_data_full, # size is 70% of 2000\n    replace = FALSE\n) # do not sample with replacement\n\n# think of data_full_smaller as a matrix, we can select rows based on training_index\ndata_training &lt;- data_full_smaller %&gt;%\n    slice(training_index)\n\n# minus sign means deselecting rows\ndata_test &lt;- data_full_smaller %&gt;%\n    slice(-training_index)\n\n\n\n\n2.2.3 Train a decision tree\nTasks: load the rpart and rpart.plot packages. Follow the code examples in the lecture notes and try to train a decision tree on data_training\n\n\nCode\npacman::p_load(rpart, rpart.plot)\n\n# write your codes below from what we learned in class.\n\ndecision_tree &lt;- rpart(\n    formula = Response ~ MntWines + MntFruits + MntMeatProducts + MntFishProducts +\n        MntSweetProducts + MntGoldProds + NumDealsPurchases + NumWebPurchases + NumCatalogPurchases +\n        NumStorePurchases + NumWebVisitsMonth + Complain + Year_Birth + Education +\n        Marital_Status + Income + Kidhome + Teenhome + Recency,\n    data = data_training,\n    method = \"anova\"\n) # because we need probabilities (a continuous variable) later on to decide targeting, here we sepcify \"anova\" rather than \"class\"\n\n\n\n\nCode\n# visualize the decision tree we have built\nrpart.plot(decision_tree)\n\n\n\n\n\n\n\n\n\n\n\n2.2.4 Train a random forest\nTasks: load the ranger packages. Follow the code examples in the lecture notes and try to train a random forest on data_training\n\n\nCode\npacman::p_load(ranger)\n\n# write your codes below:\n\nset.seed(888)\nrandomforest &lt;- ranger(\n    formula = Response ~ MntWines + MntFruits + MntMeatProducts + MntFishProducts +\n        MntSweetProducts + MntGoldProds + NumDealsPurchases + NumWebPurchases + NumCatalogPurchases +\n        NumStorePurchases + NumWebVisitsMonth + Complain + Year_Birth + Education +\n        Marital_Status + Income + Kidhome + Teenhome + Recency,\n    data = data_training,\n    probability = TRUE,\n    num.trees = 500\n)\n\n# make prediction on the test set, which returns a model prediction object\nprediction_from_randomforest &lt;- predict(randomforest, data_test)\n\n# mutate a new column in data_test for the predicted probability from random forest\n# the prediction_from_randomforest$predictions gives a matrix\n# so we need to the take the second column using [,2]\ndata_test &lt;- data_test %&gt;%\n    mutate(predicted_prob_randomforest = prediction_from_randomforest$predictions[, 2])\n\n\n\n\n2.2.5 Predict response rate from decision tree model\nFirst, we have already trained the decision tree model, named decision_tree, from the training set. We can predict the probability of test set customer responding to our marketing offer, using predict().\n\n\nCode\n# use predict() to make prediction on the test set\n# Note that prediction_from_decision_tree is already a vector,\n# which we can directly mutate into\nprediction_from_decision_tree &lt;- predict(decision_tree, data_test)\n\n# mutate a new column in data_test for the predicted probability\n# note that from decision tree, predict() can directly give a vector rather than matrix\n# so we no longer need to extract the second column (as we did with random forest)\ndata_test &lt;- data_test %&gt;%\n    mutate(predicted_prob_decisiontree = prediction_from_decision_tree)\n\n\nTasks: We should only send marketing offers to consumers whose expected or predicted response rate is larger than the break-even response rate. This is called targeted marketing.\n\n\nCode\n# mutate a new binary indicator for whether to target a customer based on predicted prob from decision tree model\ndata_test &lt;- data_test %&gt;%\n    mutate(is_target_decisiontree = ifelse(predicted_prob_decisiontree &gt; break_even_response, 1, 0))\n\n\nFinally, we have decided to send marketing offers to selected responsive customers. We can then compute the ROI for targeted marketing as in the blanket marketing case.\n\n\nCode\n# total marketing costs\ntotal_costs_of_mailing_decisiontree &lt;- cost_per_offer * # cost per offer\n    sum(data_test$is_target_decisiontree) # the sum of is_target is the total number of customers who receive the offer\n\n# filter out customers who receive the marketing offer\ndata_test_targeted_customers &lt;- data_test %&gt;%\n    filter(is_target_decisiontree == 1)\n\n# total profits from responding customers\n\ntotal_profit_decisiontree &lt;- sum(data_test_targeted_customers$Response) * # sum up to get how many targeted customers actually responded\n    profit_per_customer # the profit per responding customer\n\n# Compute ROI\nROI_decisiontree &lt;- (total_profit_decisiontree - total_costs_of_mailing_decisiontree) / total_costs_of_mailing_decisiontree\n\nROI_decisiontree\n\n\n[1] 0.7545045\n\n\nIf M&S uses random forest, an arguably better supervised learning model, to conduct targeted marketing, we can follow a similar logic as above, and compute the ROI from using random forest.\n\n\nCode\n# mutate a new binary indicator for whether to target a customer based on predicted prob from random forest model\ndata_test &lt;- data_test %&gt;%\n    mutate(is_target_randomforest = ifelse(predicted_prob_randomforest &gt; break_even_response, 1, 0))\n\n# total marketing costs\ntotal_costs_of_mailing_randomforest &lt;- cost_per_offer * sum(data_test$is_target_randomforest)\n\n# get the number of responding customers who are targeted: the second method\n\ndata_responding_targeted_customers &lt;- data_test %&gt;%\n    filter(is_target_randomforest == 1) %&gt;% # who are targeted\n    filter(Response == 1) # who responding among targeted customers\n\n# total profits from responding customers\ntotal_profit_randomforest &lt;- nrow(data_responding_targeted_customers) * profit_per_customer\n\n# Compute ROI\nROI_randomforest &lt;- (total_profit_randomforest - total_costs_of_mailing_randomforest) / total_costs_of_mailing_randomforest\n\nROI_randomforest\n\n\n[1] 1.113701\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that random forest model package in R uses C++ as the backend to speed up the computation. Therefore, if you use a Macbook, you should see the same ROI as above; however, if you use a Windows machine, you may see a slightly different ROI due to the different random number generation algorithms used in C++ on Windows and Mac OS.\nOn Windows, if you see 1.217836, it is correct.\n\n\nPredictive analytics model can help the company boost the marketing ROI by allowing M&S to target customers who are more likely to respond to the marketing offers than the break-even response rate. By doing so, M&S saves unnecessary marketing costs on those unresponsive customers and therefore improves its marketing efficiency.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Case-CausalInference.html",
    "href": "Case-CausalInference.html",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "",
    "text": "Instagram, one of the world’s leading social media platforms, has achieved significant success by providing users with a visually driven, interactive space for self-expression, connection, and content sharing. With over a billion active users, Instagram has become a major platform for individuals, influencers, and businesses to connect with broader audiences. Despite its popularity, Instagram faces bottlenecks in maintaining high levels of user engagement and growth, particularly as competition from other platforms increases. To address these challenges, Instagram must continuously innovate and find new ways to keep users active and engaged.\n\n\n\n\n\n\n\n\n\nScreenshot of A Nobody’s Instagram\n\n\n\n\n\nQuestion 1Answer\n\n\n\nWhat is Instagram’s business model?\nHow does Instagram make revenues?\nWho are Instagram’s customers?\nWhat are the major competitors and their relative strengths and weaknesses compared with Instagram?\nWho are the collaborators of Instagram?\nPESTLE analysis: any particular legal and regulatory issues that Instagram needs to be aware of?\n\n\n\n\nWhat is Instagram’s business model?\n\nPlatform business model. Network effect is the key to success.\n\nHow does Instagram make revenues?\n\nAdvertising: Sponsored posts, stories, and videos.\nE-commerce: Shopping features that allow users to purchase products directly from the platform.\n\nWho are Instagram’s customers?\n\nUsers: Individuals who use the platform to share photos, videos, and stories, connect with friends, and discover new content.\nAdvertisers: Brands and businesses that promote their products and services to Instagram’s user base through sponsored content and ads.\nContent Creators: Influencers, celebrities, and creators who produce engaging content to attract followers and monetize their presence on the platform.\n\nWhat are the major competitors and their relative strengths and weaknesses compared with Instagram?\n\nDirect: Other social media platforms like Facebook, Tiktok, etc.\nIndirect: News websites, discussion forums like Reddit, and alternative communication platforms that offer different ways for people to obtain information and interact online.\n\nWho are the collaborators of Instagram?\n\nBusiness Partners: Companies that integrate Instagram content into their services, such as news organizations or broadcasters.\nInfluencers and celebrities who attract and engage large audiences.\n\nPESTLE analysis: any particular legal and regulatory issues that Instagram needs to be aware of?\n\nLegal and Regulatory Issues: Changes in regulations related to data privacy, online speech, and censorship can significantly impact operations.\n\n\n\n\n\nGamification offers a promising approach to deepen engagement and retain users, creating a more interactive and rewarding experience that may align with Instagram’s focus on community building and personal expression. In recent years, gamification has emerged as a powerful tool in marketing and business, aimed at enhancing user engagement and loyalty across various industries. Gamification involves integrating game-like elements—such as points, levels, badges, and rewards—into non-game contexts to make experiences more engaging and enjoyable (Seaborn and Fels 2015). By tapping into fundamental human motivations like achievement, competition, and social interaction, gamification encourages users to interact more frequently and meaningfully with a platform or brand. From loyalty programs to interactive challenges, gamification strategies are designed to increase user activity and foster a deeper emotional connection to the brand.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#prospect-theory",
    "href": "Case-CausalInference.html#prospect-theory",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.1 Prospect theory",
    "text": "2.1 Prospect theory\nProspect theory, a behavioral economic theory, describes how people make decisions under uncertainty (Kahneman 1979). It suggests that individuals evaluate potential gains and losses relative to a reference point and tend to be risk-averse in the domain of gains but risk-seeking in the domain of losses.\n\n\n\n\n\n\n\n\n\nProspect Theory\n\n\n\n\nIn e-commerce, many platforms use prospect theory by presenting “limited time only” sales or “only 3 items left!” notifications. For instance, an online clothing retailer might display a countdown for flash sales on specific products, encouraging customers to buy now or miss out. Additionally, a feature like “items left in your cart are almost sold out” leverages loss aversion to encourage users to complete their purchases. This taps into the user’s fear of missing out (FOMO), driving them to act quickly to avoid potential regret over a lost deal.\n\nQuestion 2Answer\n\n\nBased on the Prospect Theory, how could Instagram leverage the concept of loss aversion to increase user engagement?\n\n\nInstagram could introduce a time-limited reward system where users earn exclusive virtual items or features by engaging with the platform within a specific timeframe. For example, users could receive a special badge or virtual currency for posting a certain number of stories or liking a certain number of posts within a week. If users fail to meet the activity goal, they lose the opportunity to earn the reward, creating a sense of loss aversion that motivates them to participate more actively to avoid missing out.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#social-comparison-theory",
    "href": "Case-CausalInference.html#social-comparison-theory",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "2.2 Social comparison theory",
    "text": "2.2 Social comparison theory\nSocial comparison theory posits that individuals gauge their social and personal worth by comparing themselves to others (Ye et al. 2022). People engage in social comparison to assess their abilities, opinions, and status relative to others. By creating a leaderboard, users could see their standings based on metrics like follower engagement, post interactions, or content quality. Ranking users against one another could motivate them to post more frequently, improve content quality, or engage more with other users’ posts. The leaderboard would provide real-time feedback, fostering both upward and downward comparisons. Users higher up the leaderboard would feel motivated to maintain their status, while those lower might be motivated to increase their activity to climb the ranks.\n\n\n\n\n\n\n\n\n\nSocial Comparison Theory\n\n\n\n\nFitness apps often incorporate social comparison to keep users motivated. For example, a fitness app might display a leaderboard that ranks users by steps taken, calories burned, or workout streaks. Users can compare their progress with friends or a larger community. This often motivates users to stay active and improve their standing. By allowing users to see both how they compare to others and celebrate their achievements, such apps encourage regular engagement and goal completion. Companies like Strava, which adds social aspects to running and cycling, have successfully used this approach to drive sustained engagement.\n\nQuestion 3Answer\n\n\nThink about how Instagram could leverage the Social Comparison Theory to increase user engagement. Provide a specific example of a feature or mechanism that Instagram could implement to encourage social comparison among users.\n\n\nInstagram could introduce a gamified leaderboard system that ranks users based on their daily activity levels, such as the number of posts, likes, comments, or stories shared. Users could see their rankings relative to other users, encouraging competition and social comparison. For example, Instagram could display a daily leaderboard that shows the top users based on their activity metrics, motivating others to increase their engagement to climb the rankings. This feature would leverage social comparison theory by creating a sense of competition and achievement among users, driving them to engage more actively with the platform.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#step-1-decide-on-the-unit-of-randomization",
    "href": "Case-CausalInference.html#step-1-decide-on-the-unit-of-randomization",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.1 Step 1: Decide on the Unit of Randomization",
    "text": "3.1 Step 1: Decide on the Unit of Randomization\n\nQuestion 4Answer\n\n\n\nWhat would be the best unit of randomization?\n\n\n\n\nThe ideal randomization level would be the user level.\nDevice level would be too granular and can easily cause crossover effects.\nNeed to force users to log in using the same account to make sure there is no crossover effect. This explains why websites and apps always ask users to log in before using the service.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#step-2-mitigate-spillover-and-crossover-effects",
    "href": "Case-CausalInference.html#step-2-mitigate-spillover-and-crossover-effects",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.2 Step 2: Mitigate Spillover and Crossover Effects",
    "text": "3.2 Step 2: Mitigate Spillover and Crossover Effects\n\nQuestion 5Answer\n\n\n\nWhat are the potential problems for spillover and crossover?\n\n\n\n\nA user may use multiple devices, causing crossover effects; that is, the same user may be exposed to different treatments on their phones, laptops, and tablets.\n\nThis can be mitigated by forcing users to log in using the same account on all devices.\n\nSpillover effects may occur when a user talks to family members or friends about the treatment they received, potentially influencing their behavior as well as the user’s. Meanwhile, even if the user does not directly talk to others about the treatment, they may still influence others’ behavior through their actions on the platform due to the network effect.\n\nThis can be mitigated by ensuring that users are not aware of the treatment they received and by keeping the treatment confidential.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#step-3-decide-on-randomization-allocation-scheme",
    "href": "Case-CausalInference.html#step-3-decide-on-randomization-allocation-scheme",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.3 Step 3: Decide on Randomization Allocation Scheme",
    "text": "3.3 Step 3: Decide on Randomization Allocation Scheme\n\nQuestion 6Answer\n\n\n\nHow should we determine the randomization scheme?\n\n\n\n\nSince A/B/N testing can be costly and risky, normally we would not use all the users.\n\nMethod 1: On testing launch date, we can randomly assign users to different treatment groups based on their user ID. For instance, we can assign 10% of users to the treatment group and 90% to the control group. Then, we can take the last digit of the user ID and assign the user to the treatment group if the last digit is 0, and to the control group if the last digit is 1-9.\nMethod 2: We can also randomly assign users to different treatment groups based on random sampling. See the codes below.\n\nAfter randomization is assigned, the treatment should remain the same for each user during the experiment period.\n\n\n\nCode\n# Method 1: Randomization using user ID\n\ndata_user &lt;- data_user %&gt;%\n    mutate(treated = ifelse(ID %% 10 == 0, 1, 0 ))\n\n\n\n\nCode\n# Method 2: Randomization using R\n\n# how to randomize the treatment if there is 1 control group and 1 treatment group\n\nset.seed(888)\n\n# assign 10% of users to the treatment group\ntreatment_probability &lt;- 0.1\n\ntreated_index &lt;- sample(1:nrow(data_user),\n    nrow(data_user) * treatment_probability,\n    replace = F\n)\n\ndata_user &lt;- data_user %&gt;%\n    mutate(treated = ifelse(ID %in% treated_index,\n        1,\n        0\n    ))",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#step-4-collect-data",
    "href": "Case-CausalInference.html#step-4-collect-data",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.4 Step 4: Collect Data",
    "text": "3.4 Step 4: Collect Data\n\nQuestion 7Answer\n\n\n\nWhat is the sample size we need?\nWhat data should we collect?\n\n\n\n\nWe can do a power analysis using pwr package in R, or simply some websites, e.g., this link.\nWe need to collect the following two types of data. The data serve 2 purposes: (1) randomization check (2) estimation of treatment effects\n\nDemographic data, this helps us to conduct the randomization check.\nBehavioral data, this helps us to estimate the treatment effects.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Case-CausalInference.html#step-5-data-analytics",
    "href": "Case-CausalInference.html#step-5-data-analytics",
    "title": "Improve User Engagement for Instagram Using A/B/N Testing",
    "section": "3.5 Step 5: Data analytics",
    "text": "3.5 Step 5: Data analytics\n\nQuestion 8Answer\n\n\n\nOnce data are collected, how can we test our hypothesis?\n\n\n\n\nFirst, we need to do a randomization check to ensure that the treatment group and control group users have similar characteristics. For any significant differences, we need to run a regression model to control for these differences.\n\n\n\nCode\npacman::p_load(dplyr)\ndata_instagram &lt;- read.csv(\"https://www.dropbox.com/scl/fi/wf7al7k8go8tg9rkf033r/instagram_ab_test.csv?rlkey=5u43dxj705iepx1bir5h7snkg&dl=1\")\n\n\n# examine if there is any difference across the treatment and control groups\nt.test(age ~ treatment,\n    data = data_instagram %&gt;%\n    filter(treatment %in% c('control','A'))\n)\n\n\n\n    Welch Two Sample t-test\n\ndata:  age by treatment\nt = -0.90268, df = 636.35, p-value = 0.367\nalternative hypothesis: true difference in means between group A and group control is not equal to 0\n95 percent confidence interval:\n -1.1134248  0.4121476\nsample estimates:\n      mean in group A mean in group control \n             24.91722              25.26786 \n\n\nCode\nt.test(age ~ treatment,\n    data = data_instagram %&gt;%\n    filter(treatment %in% c('control','B'))\n)\n\n\n\n    Welch Two Sample t-test\n\ndata:  age by treatment\nt = -0.61826, df = 668.76, p-value = 0.5366\nalternative hypothesis: true difference in means between group B and group control is not equal to 0\n95 percent confidence interval:\n -0.9547753  0.4974924\nsample estimates:\n      mean in group B mean in group control \n             25.03922              25.26786 \n\n\n\nNext, we can analyze the treatment effects by comparing the key activity metrics between the treatment and control groups. We can use pairwise t-tests if it’s A/B testing, or linear regression models if it’s A/B/N testing.\n\n\n\nCode\ndata_instagram_avg &lt;- data_instagram %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(avg_post_total_activity = mean(post_total_activity)) %&gt;%\n    ungroup()\n\n# compare the treatment effects for proposal A versus control\ndata_instagram_avg$avg_post_total_activity[1] - data_instagram_avg$avg_post_total_activity[3]\n\n\n[1] 13.3913\n\n\nCode\n# compare the treatment effects for proposal B versus control\ndata_instagram_avg$avg_post_total_activity[2] - data_instagram_avg$avg_post_total_activity[3]\n\n\n[1] 33.70592\n\n\nCode\n# is the difference statistically significant?\nt.test(post_total_activity ~ treatment,\n    data = data_instagram %&gt;%\n    filter(treatment %in% c('control','A'))\n)\n\n\n\n    Welch Two Sample t-test\n\ndata:  post_total_activity by treatment\nt = 24.481, df = 586.29, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group A and group control is not equal to 0\n95 percent confidence interval:\n 12.31696 14.46564\nsample estimates:\n      mean in group A mean in group control \n             56.18212              42.79082 \n\n\nCode\nt.test(post_total_activity ~ treatment,\n    data = data_instagram %&gt;%\n    filter(treatment %in% c('control','B'))\n)\n\n\n\n    Welch Two Sample t-test\n\ndata:  post_total_activity by treatment\nt = 57.328, df = 550.65, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group B and group control is not equal to 0\n95 percent confidence interval:\n 32.55102 34.86081\nsample estimates:\n      mean in group B mean in group control \n             76.49673              42.79082 \n\n\n\nWe will see that, we can also run a linear regression to obtain the average treatment effects for A/B/N testings.\n\n\n\nCode\npacman::p_load(modelsummary, fixest)\n# run a linear regression model to estimate the treatment effects\n\n# create dummy variables for treatment groups\n\ndata_instagram &lt;- data_instagram %&gt;%\nmutate(treatment_factor = as.factor(treatment)) %&gt;%\nmutate(treatment_factor = relevel(treatment_factor, ref = \"control\"))\n\nfeols(fml = post_total_activity ~ treatment_factor,\ndata = data_instagram) %&gt;%\nmodelsummary(stars = TRUE)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)      \n                  42.791***\n                \n                \n                                   \n                  (0.379)  \n                \n                \n                  treatment_factorA\n                  13.391***\n                \n                \n                                   \n                  (0.575)  \n                \n                \n                  treatment_factorB\n                  33.706***\n                \n                \n                                   \n                  (0.573)  \n                \n                \n                  Num.Obs.         \n                  1000     \n                \n                \n                  R2               \n                  0.777    \n                \n                \n                  R2 Adj.          \n                  0.776    \n                \n                \n                  AIC              \n                  6872.4   \n                \n                \n                  BIC              \n                  6887.1   \n                \n                \n                  RMSE             \n                  7.50     \n                \n                \n                  Std.Errors       \n                  IID      \n                \n        \n      \n    \n\n\n\n\n\n\nBased on the analyses, it seems that both proposal A and proposal B have a significant positive impact on user engagement. However, we need to consider the costs and feasibility of implementing these features on Instagram. By conducting A/B/N testing, we can evaluate the effectiveness of different gamification strategies and make data-driven decisions to optimize user engagement on the platform.",
    "crumbs": [
      "Lectures",
      "[Week 7] Linear Regression Models",
      "Case Study: Improve User Engagement for Instagram Using A/B/N Testing"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html",
    "href": "Week9-Lecture2.html",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "",
    "text": "Concept of regression discontinuity design\nEstimation of causal effects using regression discontinuity design\nApplication of regression discontinuity design in the business field\n\n\n\n\n\nRCTs are the gold standard of causal inference: In an RCT, the treatment is randomized and hence uncorrelated with any confounding factors, i.e., \\(cov(X,\\epsilon)=0\\)\nIn practice, however, it can be challenging to implement a perfect RCT.\n\nCrossover and spillover effects;\nCostly in terms of time and money\n\nTherefore, we may want to exploit causal effects from existing secondary data. Besides the instrumental variable method, we can also investigate natural experiments.\n\n\n\n\n\n\n\n\n\n\nNatural Experiment\n\n\n\nA natural experiment is an event in which individuals are exposed to the experimental conditions that are determined by nature or exogenous factors beyond researchers’ control. The process governing the exposures arguably resembles randomized experiments.\n\n\n\n\nRCT\n\nAssignment of treatment is randomized by us\nTreatment is under control by us\nPrimary data\n\n\nNatural Experiment\n\nAssignment of treatment is randomized by nature\nTreatment is not controlled by us\nSecondary data",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#class-objectives",
    "href": "Week9-Lecture2.html#class-objectives",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "",
    "text": "Concept of regression discontinuity design\nEstimation of causal effects using regression discontinuity design\nApplication of regression discontinuity design in the business field",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#from-rcts-to-secondary-data",
    "href": "Week9-Lecture2.html#from-rcts-to-secondary-data",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "",
    "text": "RCTs are the gold standard of causal inference: In an RCT, the treatment is randomized and hence uncorrelated with any confounding factors, i.e., \\(cov(X,\\epsilon)=0\\)\nIn practice, however, it can be challenging to implement a perfect RCT.\n\nCrossover and spillover effects;\nCostly in terms of time and money\n\nTherefore, we may want to exploit causal effects from existing secondary data. Besides the instrumental variable method, we can also investigate natural experiments.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#comparison-rct-natural-experiment",
    "href": "Week9-Lecture2.html#comparison-rct-natural-experiment",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "",
    "text": "Natural Experiment\n\n\n\nA natural experiment is an event in which individuals are exposed to the experimental conditions that are determined by nature or exogenous factors beyond researchers’ control. The process governing the exposures arguably resembles randomized experiments.\n\n\n\n\nRCT\n\nAssignment of treatment is randomized by us\nTreatment is under control by us\nPrimary data\n\n\nNatural Experiment\n\nAssignment of treatment is randomized by nature\nTreatment is not controlled by us\nSecondary data",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#what-is-an-rdd",
    "href": "Week9-Lecture2.html#what-is-an-rdd",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "2.1 What is an RDD",
    "text": "2.1 What is an RDD\n\nA regression discontinuity design (RDD) is a natural-experimental design that aims to determine the causal effects of interventions by identifying a cutoff around which an intervention is as if randomized across individuals.\n\n\n\n\n\n\n\n\n\n\nVisual illustration of RDD",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#motivating-example",
    "href": "Week9-Lecture2.html#motivating-example",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "2.2 Motivating Example",
    "text": "2.2 Motivating Example\nBusiness objective: What is the causal effect of receiving a Master’s degree with Distinction versus Non-Distinction on students’ future salary?\n\nCan we run the following simple linear regression and obtain the causal effects?\n\nNo, because the regression suffers from endogeneity issue, such as omitted variable bias. Individual’s ability is correlated with their average grades and also affects individual salary, which is omitted from the regression.\n\n\n\\[\nSalary_i = \\alpha + \\beta Distinction_i + \\epsilon_i\n\\]\n\nCan we use RCT?\n\nNo, RCTs would be unethical to run for this research question.\n\nCan we use instrumental variables?\n\nTheoretically yes, but in practice, it is difficult to find valid instrumental variables.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#a-natural-experiment-in-the-uk",
    "href": "Week9-Lecture2.html#a-natural-experiment-in-the-uk",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "2.3 A Natural Experiment in the UK",
    "text": "2.3 A Natural Experiment in the UK\n\nIn the UK Education system, students receiving 70% or above final average grades will receive Distinction while students below 70% will receive Merit.\nThe above setting gives us a nice natural experiment:\n\nStudents may improve their average grades significantly, such as moving from 60% to 69% by working harder, but they cannot perfectly control their average grades around the cutoff, say, from 69.9% to 70%.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#visual-illustration-of-rdd-an-example-of-distinction-on-salary",
    "href": "Week9-Lecture2.html#visual-illustration-of-rdd-an-example-of-distinction-on-salary",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "2.4 Visual Illustration of RDD: An Example of Distinction on Salary",
    "text": "2.4 Visual Illustration of RDD: An Example of Distinction on Salary",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#why-rdd-gives-causal-effects",
    "href": "Week9-Lecture2.html#why-rdd-gives-causal-effects",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "2.5 Why RDD Gives Causal Effects?",
    "text": "2.5 Why RDD Gives Causal Effects?\n\nFor students just above 70%, to measure the treatment effects of receiving Distinction, we would need their counterfactual salaries if they had not received Distinction.\nAt the same time, because the “running variable” cannot be perfectly controlled by the individuals around the cutoff point, it’s as if the treatment was randomized near the cutoff. Thus, individuals near the cutoff should be very similar, such that there should be no systematic differences across the treatment and control group.\n\nSimilar to RCT, we overcome the fundamental problem of causal inference using students just below 70 as the control group.\n\nAll else being equal, a sudden change in the outcome variable at the cutoff can only be attributed to the treatment effect.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#conditions-for-using-an-rdd",
    "href": "Week9-Lecture2.html#conditions-for-using-an-rdd",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "2.6 Conditions for Using an RDD",
    "text": "2.6 Conditions for Using an RDD\n\nAn RDD design arises when treatment is assigned based on whether an underlying continuous variable crosses a cutoff.\n\nThe continuous variable is often referred to as the running variable.\n\nAND the characteristic cannot be perfectly manipulated by individuals\n\nWe should only focus on individuals close to the cutoff point.\n\n\nExercise: eBay endorses sellers with 10,000 orders as Gold Seller. Can we use RDD to identify the causal effect of receiving Gold Seller endorsement on seller sales?\nNo, because sellers can have perfect control over their sales around the cutoff point.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#step-1-select-sample-of-analysis",
    "href": "Week9-Lecture2.html#step-1-select-sample-of-analysis",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.1 Step 1: Select Sample of Analysis",
    "text": "3.1 Step 1: Select Sample of Analysis\n\nDetermine the bandwidth above and below the cutoff and select the subset of individuals within the bandwidth\n\ne.g., if we choose a bandwidth of 0.5, we need to filter out students with average scores between 69.5 and 70.5\n\n\n\nWe face a trade-off when selecting the bandwidth: If we choose a smaller bandwidth around the cut-off\n\nPros: Individuals should be more similar around the cutoff, thus it is more likely the control group and treatment group are “as-if randomized”, thus higher internal validity.\nCons: We have a smaller subset of subjects which may not be representative of remaining individuals, thus lower external validity; We have a smaller sample size due to fewer individuals selected\n\nIn practice, there is no specific rule how to determine the bandwidth. We need to run a set of different bandwidths as robustness checks.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#step-2-examine-continuity-of-observed-characteristics",
    "href": "Week9-Lecture2.html#step-2-examine-continuity-of-observed-characteristics",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.2 Step 2: Examine Continuity of Observed Characteristics",
    "text": "3.2 Step 2: Examine Continuity of Observed Characteristics\n\nExamine if other characteristics of the treatment group and control group are continuous at the cut-off point.\n\nThe idea is similar to “randomization check” in an RCT.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#step-3-data-analysis",
    "href": "Week9-Lecture2.html#step-3-data-analysis",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.3 Step 3: Data Analysis",
    "text": "3.3 Step 3: Data Analysis\n\nRegress the outcome variable on the treatment indicator to obtain the causal effect.\n\n\\[\nY_i = \\beta_0 + \\beta_1 Treated + \\beta_2 running\\_variable + \\epsilon_i\n\\]\n\n\\(Treated\\) is a binary variable for whether or not the running variable is above the cutoff.\nWe may also want to control the running variable in the regression to mitigate its confounding effects.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#the-causal-effect-of-distinction-on-salary",
    "href": "Week9-Lecture2.html#the-causal-effect-of-distinction-on-salary",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.4 The Causal Effect of Distinction on Salary",
    "text": "3.4 The Causal Effect of Distinction on Salary\n\nWe collect a dataset of 1000 graduates with their MSc final grade and salary.\n\n\n\nCode\npacman::p_load(dplyr, fixest, modelsummary, ggplot2)\ndata_rdd &lt;- read.csv(\"https://www.dropbox.com/scl/fi/z4rgm15cmp19m3i65il3a/data_rdd.csv?rlkey=wnb5ypssg79whq2x4iiov6vte&dl=1\") %&gt;%\n    mutate(Distinction = ifelse(score &gt;= 70, 1, 0))\n\ndata_rdd %&gt;%\n    slice(1:5)",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#linear-regression-analyses",
    "href": "Week9-Lecture2.html#linear-regression-analyses",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.5 Linear Regression Analyses",
    "text": "3.5 Linear Regression Analyses\n\nRun a linear regression: salary ~ Distinction\n\n\n\nCode\nfeols(\n    fml  = salary ~ Distinction,\n    data = data_rdd\n) %&gt;%\n    modelsummary(\n        stars = T,\n        gof_map = c(\"nobs\", \"r.squared\")\n    )\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  63306.835***\n                \n                \n                             \n                  (57.722)    \n                \n                \n                  Distinction\n                  5533.565*** \n                \n                \n                             \n                  (94.638)    \n                \n                \n                  Num.Obs.   \n                  1000        \n                \n                \n                  R2         \n                  0.774       \n                \n        \n      \n    \n\n\n\n\nThe result suggests that Distinction can increase the salary by 5.5k, which is likely over-estimated due to omitted variable bias.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#rdd-analysis",
    "href": "Week9-Lecture2.html#rdd-analysis",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.6 RDD Analysis",
    "text": "3.6 RDD Analysis\n\nStep 1: Select a bandwidth around the cutoff, between 68% to 72%\nStep 2: Examine discontinuity of other variables (randomization check).\nStep 3: Run a linear regression on the subsample.\n\n\n\nCode\nresult_RDD &lt;- feols(\n    fml = salary ~ Distinction,\n    data = data_rdd %&gt;%\n        filter(score &gt; 68 & score &lt; 72)\n)",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#rdd-results",
    "href": "Week9-Lecture2.html#rdd-results",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.7 RDD Results",
    "text": "3.7 RDD Results\n\n\nCode\nresult_RDD %&gt;%\n    modelsummary(\n        stars = T,\n        gof_map = c(\"nobs\", \"r.squared\")\n    )\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  65201.101***\n                \n                \n                             \n                  (81.291)    \n                \n                \n                  Distinction\n                  2898.285*** \n                \n                \n                             \n                  (110.725)   \n                \n                \n                  Num.Obs.   \n                  282         \n                \n                \n                  R2         \n                  0.710       \n                \n        \n      \n    \n\n\n\n\nThe result suggests that Distinction can increase the salary by 2.898k, which is likely a more accurate estimate of the causal effect than the OLS estimate.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#visualization-of-rdd",
    "href": "Week9-Lecture2.html#visualization-of-rdd",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.8 Visualization of RDD",
    "text": "3.8 Visualization of RDD\n\n\n\n\nCode\npacman::p_load(ggplot2, ggthemes)\ndata_rdd %&gt;%\n    ggplot(aes(x = score, y = salary)) +\n    geom_point() +\n    geom_vline(xintercept = 70, linetype = \"dashed\") +\n    labs(\n        title = \"RDD: Distinction on Salary\",\n        x = \"Final Score\",\n        y = \"Salary\"\n    ) +\n    geom_smooth(\n        data = subset(data_rdd, score &lt; 70),\n        method = \"lm\", se = FALSE,\n        color = \"red\"\n    ) +\n    geom_smooth(\n        data = subset(data_rdd, score &gt;= 70),\n        method = \"lm\", se = FALSE,\n        color = \"blue\"\n    ) +\n    theme_minimal()\n\n\n\n\n\nCode\npacman::p_load(ggplot2, ggthemes)\ndata_rdd %&gt;%\n    ggplot(aes(x = score, y = salary)) +\n    geom_point() +\n    geom_vline(xintercept = 70, linetype = \"dashed\") +\n    labs(\n        title = \"RDD: Distinction on Salary\",\n        x = \"Final Score\",\n        y = \"Salary\"\n    ) +\n    geom_smooth(\n        data = subset(data_rdd, score &lt; 70),\n        method = \"lm\", se = FALSE,\n        color = \"red\"\n    ) +\n    geom_smooth(\n        data = subset(data_rdd, score &gt;= 70),\n        method = \"lm\", se = FALSE,\n        color = \"blue\"\n    ) +\n    theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#application-of-rdd-in-marketing-and-business-context",
    "href": "Week9-Lecture2.html#application-of-rdd-in-marketing-and-business-context",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.9 Application of RDD in Marketing and Business Context",
    "text": "3.9 Application of RDD in Marketing and Business Context\n\nScores vs. stars: A regression discontinuity study of online consumer reviews: 4.49 rating vs. 4.5 rating result in a significant difference in sales.\nThe causal effect of Uber’s surge pricing on demand and driver labor supply: On the Uber platform, surge multiplier is rounded to the nearest 0.1, which creates a natural experiment for RDD.",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week9-Lecture2.html#after-class-reading",
    "href": "Week9-Lecture2.html#after-class-reading",
    "title": "Class 18 Natural Experiment I: Regression Discontinuity Design",
    "section": "3.10 After-class Reading",
    "text": "3.10 After-class Reading\n\n(recommended) Quasi-experiment (Econometrics with R)",
    "crumbs": [
      "Lectures",
      "[Week 9] Natural Experiment",
      "Lecture 2: Natural Experiment I: Regression Discontinuity Design"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html",
    "href": "Week8-Lecture1.html",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Understand the reasoning why linear regression can almost never provide causal effects from non-experimental data.\n\nDirect and Indirect Effects\nCausal Inference from Regression Models\nUnderstand the difference between RCT and non-experimental data.\n\nUnderstand the concept of endogeneity and its causes.\n\nOmitted Variable Bias\nReverse Causality\nMeasurement Error (Optional)\n\n\n\n\n\n\nTask: M&S wants to understand the causal impact of customer \\(Income\\) on customer \\(Spending\\), i.e., the Marginal Propensity to Consume (MPS).1\n\n\nPlease run the two regressions on your Quarto document and export the regression table:\n\nRegression 1: \\(Spending\\) ~ \\(Income\\)\nRegression 2: \\(Spending\\) ~ \\(Income\\) + \\(Kidhome\\)\n\n\n\n\n\n\n\nCode\npacman::p_load(fixest,modelsummary)\n\nregression1 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income ) \n\nregression2 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income + Kidhome) \n\n\n\nmodelsummary(list(regression1,regression2),\n             stars = TRUE,\n             gof_map = c('nobs','r.squared'))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -556.823***\n                  -299.119***\n                \n                \n                             \n                  (21.654)   \n                  (28.069)   \n                \n                \n                  Income     \n                  0.022***   \n                  0.019***   \n                \n                \n                             \n                  (0.000)    \n                  (0.000)    \n                \n                \n                  Kidhome    \n                             \n                  -230.610***\n                \n                \n                             \n                             \n                  (16.945)   \n                \n                \n                  Num.Obs.   \n                  2000       \n                  2000       \n                \n                \n                  R2         \n                  0.629      \n                  0.661      \n                \n        \n      \n    \n\n\n\n\nQuestion: if we want to evaluate income’s causal effect on spending, which value (0.022***, 0.019***) should we use?\n\n\n\n\nUsing our common sense, let’s think about how income can causally affect total spending:\n\n\n\n\n\n\n\n\n\n\n\n\nCausal effect\n\nDirect effect keeping other variables ﬁxed\n\n\n\n\nTotal Effect\n\nDirect effect + indirect effects through other variables\n\n\n\n\n\n\n\n\nTo obtain causal effects from non-experimental data, we refer to obtaining the direct effects of a focal \\(X\\) variable on the outcome variable \\(Y\\).\nHowever, if we do not include Kidhome in the regression, the regression coefficient 0.022 measures the total effects of income, including\n\ndirect effects of income on total spending, 0.019\nindirect effects of income on other intermediate variables, which in turn affect income. These intermediate variables are called confounding variables or confounders.\n\nTherefore, it is important to include all other confounding variables, which affect income and total spending at the same time, to control for the indirect effects via other variables, in order to tease out the clean direct effect of income on total spending.\n\n\n\n\n\nFor causal inference tasks, we need to use business senses to decide which confounding variables to control. We face the good control and bad control problems.2\n\n\n\n\n“Some variables are bad controls and should not be included in a regression model, even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notional experiment at hand. That is, bad controls might just as well be dependent variables too. Good controls are variables that we can think of having been fixed at the time the regressor of interest was determined.”\n\n\nSometimes, control variables may be statistically insignificant, they should NOT be removed from the regression because they still serve the purpose of control variables.\nA high correlation between independent variables is generally not an issue in practice. However, if some variables are mechanically correlated, then we should not put them altogether in the regression to avoid perfect collinearity problems.\nFor correct statistical inference, we should construct the correct standard errors\n\nRobust standard errors for cross-sectional data. This is to account for the heteroskedasticity of errors. vcov = \"hetero\" in feols().\nClustered robust standard errors for panel data or longitudinal with group structures. This is to account for the correlation of errors within the same group. cluster = ~ ID in feols().\n\n\nQuestion: what is the best you can do with data_full to estimate the causal effect of income on spending?\n\n\n\nNow that we have included Kidhome to tease out the effect of kids, what problems do we still have that prevent us from getting causal effect of income on total spending?\n\nDue to data availability, we are never able to include all confounding variables in the regression. Therefore, strictly speaking, we can never obtain causal effects from non-experimental data by merely controlling confounding variables in a linear regression.\nMathematically speaking, because we can never control all confounding factors, the error term is always correlated with income to some extent, violating the exogeneity assumption of a linear regression model \\(E[\\epsilon|X] = 0\\).\n\n\n\n\n\nWhy RCTs are the gold standard for causal inference? Why we can obtain causal inference from primary data collected from RCTs?\n\nIf we randomize people into different income groups, we can then collect the total_spending for each individual in each Income group.\nWe can run a linear regression to examine the impact of Income on total_spending.\n\n\n\\[\nSpending = \\beta_0 + \\beta_1Income + \\epsilon\n\\]\n\nIn the above regression, are there still any confounding effects?\n\nNo, there are no confounders remaining, because Income is randomized, so Income should be uncorrelated with anything. Thus no confounders remain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn non-experiment setting without randomization, Income can be correlated with other unobserved confounding factors\n\n\n\n\n\n\n\n\n\n\n\n\nIn experiment setting with randomization, Income is randomized so should be uncorrelated with any other unobserved factors.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#class-objective",
    "href": "Week8-Lecture1.html#class-objective",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Understand the reasoning why linear regression can almost never provide causal effects from non-experimental data.\n\nDirect and Indirect Effects\nCausal Inference from Regression Models\nUnderstand the difference between RCT and non-experimental data.\n\nUnderstand the concept of endogeneity and its causes.\n\nOmitted Variable Bias\nReverse Causality\nMeasurement Error (Optional)",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#causal-effect-from-non-experimental-secondary-data",
    "href": "Week8-Lecture1.html#causal-effect-from-non-experimental-secondary-data",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Task: M&S wants to understand the causal impact of customer \\(Income\\) on customer \\(Spending\\), i.e., the Marginal Propensity to Consume (MPS).1\n\n\nPlease run the two regressions on your Quarto document and export the regression table:\n\nRegression 1: \\(Spending\\) ~ \\(Income\\)\nRegression 2: \\(Spending\\) ~ \\(Income\\) + \\(Kidhome\\)",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#regression-results",
    "href": "Week8-Lecture1.html#regression-results",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Code\npacman::p_load(fixest,modelsummary)\n\nregression1 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income ) \n\nregression2 &lt;- feols(data = data_full,\n     fml = total_spending ~ Income + Kidhome) \n\n\n\nmodelsummary(list(regression1,regression2),\n             stars = TRUE,\n             gof_map = c('nobs','r.squared'))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -556.823***\n                  -299.119***\n                \n                \n                             \n                  (21.654)   \n                  (28.069)   \n                \n                \n                  Income     \n                  0.022***   \n                  0.019***   \n                \n                \n                             \n                  (0.000)    \n                  (0.000)    \n                \n                \n                  Kidhome    \n                             \n                  -230.610***\n                \n                \n                             \n                             \n                  (16.945)   \n                \n                \n                  Num.Obs.   \n                  2000       \n                  2000       \n                \n                \n                  R2         \n                  0.629      \n                  0.661      \n                \n        \n      \n    \n\n\n\n\nQuestion: if we want to evaluate income’s causal effect on spending, which value (0.022***, 0.019***) should we use?",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#direct-and-indirect-effects",
    "href": "Week8-Lecture1.html#direct-and-indirect-effects",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Using our common sense, let’s think about how income can causally affect total spending:\n\n\n\n\n\n\n\n\n\n\n\n\nCausal effect\n\nDirect effect keeping other variables ﬁxed\n\n\n\n\nTotal Effect\n\nDirect effect + indirect effects through other variables",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#causal-inference-from-regression-models",
    "href": "Week8-Lecture1.html#causal-inference-from-regression-models",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "To obtain causal effects from non-experimental data, we refer to obtaining the direct effects of a focal \\(X\\) variable on the outcome variable \\(Y\\).\nHowever, if we do not include Kidhome in the regression, the regression coefficient 0.022 measures the total effects of income, including\n\ndirect effects of income on total spending, 0.019\nindirect effects of income on other intermediate variables, which in turn affect income. These intermediate variables are called confounding variables or confounders.\n\nTherefore, it is important to include all other confounding variables, which affect income and total spending at the same time, to control for the indirect effects via other variables, in order to tease out the clean direct effect of income on total spending.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#practical-tips-for-running-regression-models-for-causal-inference",
    "href": "Week8-Lecture1.html#practical-tips-for-running-regression-models-for-causal-inference",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "For causal inference tasks, we need to use business senses to decide which confounding variables to control. We face the good control and bad control problems.2\n\n\n\n\n“Some variables are bad controls and should not be included in a regression model, even when their inclusion might be expected to change the short regression coefficients. Bad controls are variables that are themselves outcome variables in the notional experiment at hand. That is, bad controls might just as well be dependent variables too. Good controls are variables that we can think of having been fixed at the time the regressor of interest was determined.”\n\n\nSometimes, control variables may be statistically insignificant, they should NOT be removed from the regression because they still serve the purpose of control variables.\nA high correlation between independent variables is generally not an issue in practice. However, if some variables are mechanically correlated, then we should not put them altogether in the regression to avoid perfect collinearity problems.\nFor correct statistical inference, we should construct the correct standard errors\n\nRobust standard errors for cross-sectional data. This is to account for the heteroskedasticity of errors. vcov = \"hetero\" in feols().\nClustered robust standard errors for panel data or longitudinal with group structures. This is to account for the correlation of errors within the same group. cluster = ~ ID in feols().\n\n\nQuestion: what is the best you can do with data_full to estimate the causal effect of income on spending?",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#causal-inference-from-regressions",
    "href": "Week8-Lecture1.html#causal-inference-from-regressions",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Now that we have included Kidhome to tease out the effect of kids, what problems do we still have that prevent us from getting causal effect of income on total spending?\n\nDue to data availability, we are never able to include all confounding variables in the regression. Therefore, strictly speaking, we can never obtain causal effects from non-experimental data by merely controlling confounding variables in a linear regression.\nMathematically speaking, because we can never control all confounding factors, the error term is always correlated with income to some extent, violating the exogeneity assumption of a linear regression model \\(E[\\epsilon|X] = 0\\).",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#revisit-rct-the-gold-standard-of-causal-inference",
    "href": "Week8-Lecture1.html#revisit-rct-the-gold-standard-of-causal-inference",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "Why RCTs are the gold standard for causal inference? Why we can obtain causal inference from primary data collected from RCTs?\n\nIf we randomize people into different income groups, we can then collect the total_spending for each individual in each Income group.\nWe can run a linear regression to examine the impact of Income on total_spending.\n\n\n\\[\nSpending = \\beta_0 + \\beta_1Income + \\epsilon\n\\]\n\nIn the above regression, are there still any confounding effects?\n\nNo, there are no confounders remaining, because Income is randomized, so Income should be uncorrelated with anything. Thus no confounders remain.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#comparison-of-rct-versus-secondary-data",
    "href": "Week8-Lecture1.html#comparison-of-rct-versus-secondary-data",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "",
    "text": "In non-experiment setting without randomization, Income can be correlated with other unobserved confounding factors\n\n\n\n\n\n\n\n\n\n\n\n\nIn experiment setting with randomization, Income is randomized so should be uncorrelated with any other unobserved factors.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#endogeneity",
    "href": "Week8-Lecture1.html#endogeneity",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "2.1 Endogeneity",
    "text": "2.1 Endogeneity\n\n2.1.1 Endogeneity\nEndogeneity refers to an econometric issue with OLS linear regression, in which a focal explanatory variable is correlated with the error term, such that the Conditional Independence Assumption (CIA) for OLS linear regression, \\(E[\\epsilon|X] = 0\\), is violated.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#cause-i-omitted-variable-bias",
    "href": "Week8-Lecture1.html#cause-i-omitted-variable-bias",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "2.2 Cause I: Omitted Variable Bias",
    "text": "2.2 Cause I: Omitted Variable Bias\n\n2.2.1 Omitted Variable Bias (OVB)\nAn omitted variable is a determinant of the outcome variable \\(y_i\\) that is correlated with the focal explanatory variable \\(x_i\\), but is not included in the regression, either due to data unavailability or ignorance of data scientists.\n\n\nTwo conditions for omitted variable bias\n\nThe omitted variable affects the dependent variable.\nThe omitted variable is correlated with the focal explanatory variable.3",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#example-i-of-ovb",
    "href": "Week8-Lecture1.html#example-i-of-ovb",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "2.3 Example I of OVB",
    "text": "2.3 Example I of OVB\n\nIf we would like to understand the causal effect of Education on a person’s salary.\n\n\\[\nSalary_t = \\beta_0 + \\beta_1 Education_t + \\epsilon_t\n\\]\n\nCan we get causal effect from this regression? What would be the issue here?\n\n\nThe issue here is that Education is correlated with other unobserved factors, such as IQ, personality, family background, etc. These unobserved factors may also affect salary. Therefore, the error term \\(\\epsilon\\) is correlated with Education, violating the exogeneity assumption of OLS regression.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#example-ii-of-ovb",
    "href": "Week8-Lecture1.html#example-ii-of-ovb",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "2.4 Example II of OVB",
    "text": "2.4 Example II of OVB\n\nWhen predicting unit sales from prices, the common practice in the industry is to regress the sales in each period on the price in each period (marketing mix modeling).\n\n\\[\nSales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n\\]\n\nHowever, is this regression correct?\n\n\nThe issue here is that the price is correlated with other unobserved factors, such as brand image, product quality, advertising, etc. These unobserved factors may also affect sales. Therefore, the error term \\(\\epsilon\\) is correlated with Price, violating the exogeneity assumption of OLS regression.\n\n\nVery often, if we regress sales only on price, we get a positive coefficient for price.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#cause-ii-reverse-causality-simultaneity",
    "href": "Week8-Lecture1.html#cause-ii-reverse-causality-simultaneity",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "3.1 Cause II: Reverse Causality (Simultaneity)",
    "text": "3.1 Cause II: Reverse Causality (Simultaneity)\n\n3.1.1 Reverse Causality\nReverse causality refers to the phenomenon that the independent variable \\(X_i\\) affects the dependent variable \\(y_i\\) and the dependent variable \\(y_i\\) also affects the independent variable \\(X_i\\) at the same time.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#example-i-of-reverse-causality-simultaneity",
    "href": "Week8-Lecture1.html#example-i-of-reverse-causality-simultaneity",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "3.2 Example I of Reverse Causality (Simultaneity)",
    "text": "3.2 Example I of Reverse Causality (Simultaneity)\n\nBesides potential omitted variable biases, there may also exist reverse causality problems with marketing mix modelling.\n\n\\[\nSales_t = \\beta_0 + \\beta_1 Price_t + \\epsilon_t\n\\]\n\nPrice affects demand, and demand affects sellers’ price setting decisions.\n\nHigher price leads to lower sales. (X =&gt; Y)\nIf sellers expect higher demand, sellers may increase the price to increase profits. (Y =&gt; X)",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#example-ii-of-reverse-causality-simultaneity",
    "href": "Week8-Lecture1.html#example-ii-of-reverse-causality-simultaneity",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "3.3 Example II of Reverse Causality (Simultaneity)",
    "text": "3.3 Example II of Reverse Causality (Simultaneity)\n\nUberEat interview question: If we have historical data on number of restaurants on UberEat in each month, and the total number of orders in each month, can we run an OLS regression to get the causal impact of network effect?\n\n\\[\nNumOrders_t = \\beta_0 + \\beta_1 NumRestaurants_t + \\epsilon_t\n\\]\n\nIf not, how can we measure the causal effects for UberEat?\n\nWe need to run A/B testings and randomize how many restaurants a customer can see on their apps.\n\nThis question is not just limited to UberEat; it is in fact related to any platform business with network effect!\n\nAmazon; Airbnb; Uber Ridesharing; etc.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#cause-iii-measurement-error-optional",
    "href": "Week8-Lecture1.html#cause-iii-measurement-error-optional",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "4.1 Cause III: Measurement Error (Optional)",
    "text": "4.1 Cause III: Measurement Error (Optional)\nSuppose that a perfect measure of an independent variable is impossible. That is, instead of observing \\(x^{real}\\), what is actually observed is \\(x^{observed} = x^{real} + \\nu\\) where \\(\\nu\\) is the measurement error with random “noise”. In this case, a model given by \\[\ny_i=\\alpha+\\beta x^{observed}_i+\\varepsilon_i\n\\]\nwould not give us the coefficients from the regression we actually want to run \\[\ny_i=\\alpha+\\beta x^{real}_i+\\varepsilon_i\n\\]\nThis endogeneity issue is called measurement error.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#when-to-worry-about-measurement-errors",
    "href": "Week8-Lecture1.html#when-to-worry-about-measurement-errors",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "4.2 When to Worry About Measurement Errors?",
    "text": "4.2 When to Worry About Measurement Errors?\n\nThis problem needs to be addressed when you expect a high measurement error in the independent variable, especially when using proxy variables. For example,\n\ngrades as a proxy for Ability\nESGRating as a proxy for firms’ ESGPerformance\naudit fee as a proxy for audit quality\n\nMeanwhile, if the measurement error is in the dependent variable, and the expectation of the measurement error is zero, then the OLS estimator is still unbiased.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture1.html#footnotes",
    "href": "Week8-Lecture1.html#footnotes",
    "title": "Class 15 Endogeneity and Its Causes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn economics, MPC refers to the proportion of an additional unit of income that is spent on consumption.↩︎\nAngrist, Joshua D., and Jörn-Steffen Pischke. Mostly harmless econometrics: An empiricist’s companion. Princeton university press, 2009.↩︎\nIf the omitted variable is uncorrelated with X, then we do not have OVB problem, but the error term will have a larger noise and coefficients will have larger standard errors. Therefore, it’s better to control these variables if possible.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 1: Endogeneity"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html",
    "href": "Week8-Lecture2.html",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "The requirements of a valid instrumental variable and how to find good instruments\nIntuition of why instrumental variables solve endogeneity problems\nApply two-stage least square method to estimate the causal effects using instrumental variables\n\n\n\n\n\nFrom non-experimental secondary data, it is impossible to control all confounding factors, which means we can never obtain causal effects from OLS regressions.\nCan we still obtain causal inference from secondary data?\n\n\n\n\n\n\nAn instrumental variable is a set of variables \\(Z\\) that satisfies the following requirements:\n\n\\(z\\) is exogeneous and uncorrelated with \\(\\epsilon\\); that is, \\(cov(Z,\\epsilon) = 0\\)\n\\(z\\) only affects \\(Y\\) through \\(X\\), but not directly affect \\(Y\\)\n\\(z\\) affects \\(x\\) to some extent, that is, \\(cov(Z,x) \\neq 0\\)\n\n\n\nPoint 1 is called exogeneity requirement: the instrumental variable should be beyond individual’s control, such that the instrumental variables are uncorrelated with any individual’s unobserved confounding factors.\n\nPotential IVs: government policy; natural disasters; randomized experiment; birthdays; etc.\n\nPoint 2 is called exclusion restriction: the instrumental variable should only affect \\(Y\\) through \\(X\\), but not directly affect \\(Y\\).\nPoint 3 is called relevance requirement: though beyond an individual’s control, the instrumental variable should still affect the individual’s \\(X\\), causing some exogenous changes in \\(X\\) that is beyond individual control.\n\nIf the correlation between \\(z\\) and \\(x\\) is too small, we have a weak IV problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn of Military Service to Lifetime Income1\n\\[\nIncome = \\beta_0 + \\beta_1MilitaryService + \\epsilon\n\\]\n\nOLS suffers from endogeneity problems. What are the potential endogeneity issues?\nA lottery was used to determine if soldiers with certain birthdays are drafted to the frontline.\n\n\n\n\n\nThe date of birth (\\(z\\)) or zodiacs can be an instrumental variable for military service (\\(x\\)) in this case.\n\nRelevance requirement: Affects years of military service: \\(cov(z,x) \\neq 0\\)\nExogeneity requirement: Randomly drawn and thus uncorrelated with any confounders: \\(cov(z,\\epsilon) = 0\\)\nExclusion restriction: \\(z\\) only affects \\(Y\\) through \\(X\\), but not directly affect \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan you come up with IV candidates for the following causation questions?\n\nNumber of restaurants on UberEat =&gt; Number of orders on UberEat\n\ntemporary close down of restaurants due to government inspections\n\nRetail price =&gt; Sales\n\nwholesale price\ncosts of raw materials\nCOGS\nHausman instruments: the prices of the same product in other markets",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#class-objectives",
    "href": "Week8-Lecture2.html#class-objectives",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "The requirements of a valid instrumental variable and how to find good instruments\nIntuition of why instrumental variables solve endogeneity problems\nApply two-stage least square method to estimate the causal effects using instrumental variables",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#causal-inference-from-ols",
    "href": "Week8-Lecture2.html#causal-inference-from-ols",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "From non-experimental secondary data, it is impossible to control all confounding factors, which means we can never obtain causal effects from OLS regressions.\nCan we still obtain causal inference from secondary data?",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#what-is-an-instrumental-variable",
    "href": "Week8-Lecture2.html#what-is-an-instrumental-variable",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "An instrumental variable is a set of variables \\(Z\\) that satisfies the following requirements:\n\n\\(z\\) is exogeneous and uncorrelated with \\(\\epsilon\\); that is, \\(cov(Z,\\epsilon) = 0\\)\n\\(z\\) only affects \\(Y\\) through \\(X\\), but not directly affect \\(Y\\)\n\\(z\\) affects \\(x\\) to some extent, that is, \\(cov(Z,x) \\neq 0\\)\n\n\n\nPoint 1 is called exogeneity requirement: the instrumental variable should be beyond individual’s control, such that the instrumental variables are uncorrelated with any individual’s unobserved confounding factors.\n\nPotential IVs: government policy; natural disasters; randomized experiment; birthdays; etc.\n\nPoint 2 is called exclusion restriction: the instrumental variable should only affect \\(Y\\) through \\(X\\), but not directly affect \\(Y\\).\nPoint 3 is called relevance requirement: though beyond an individual’s control, the instrumental variable should still affect the individual’s \\(X\\), causing some exogenous changes in \\(X\\) that is beyond individual control.\n\nIf the correlation between \\(z\\) and \\(x\\) is too small, we have a weak IV problem.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#a-classic-example-of-instrumental-variable",
    "href": "Week8-Lecture2.html#a-classic-example-of-instrumental-variable",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "Return of Military Service to Lifetime Income1\n\\[\nIncome = \\beta_0 + \\beta_1MilitaryService + \\epsilon\n\\]\n\nOLS suffers from endogeneity problems. What are the potential endogeneity issues?\nA lottery was used to determine if soldiers with certain birthdays are drafted to the frontline.",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#a-classic-example-of-instrumental-variable-1",
    "href": "Week8-Lecture2.html#a-classic-example-of-instrumental-variable-1",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "The date of birth (\\(z\\)) or zodiacs can be an instrumental variable for military service (\\(x\\)) in this case.\n\nRelevance requirement: Affects years of military service: \\(cov(z,x) \\neq 0\\)\nExogeneity requirement: Randomly drawn and thus uncorrelated with any confounders: \\(cov(z,\\epsilon) = 0\\)\nExclusion restriction: \\(z\\) only affects \\(Y\\) through \\(X\\), but not directly affect \\(Y\\).",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#more-examples-of-ivs",
    "href": "Week8-Lecture2.html#more-examples-of-ivs",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "",
    "text": "Can you come up with IV candidates for the following causation questions?\n\nNumber of restaurants on UberEat =&gt; Number of orders on UberEat\n\ntemporary close down of restaurants due to government inspections\n\nRetail price =&gt; Sales\n\nwholesale price\ncosts of raw materials\nCOGS\nHausman instruments: the prices of the same product in other markets",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#solving-endogeneity-using-iv",
    "href": "Week8-Lecture2.html#solving-endogeneity-using-iv",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "2.1 Solving Endogeneity Using IV",
    "text": "2.1 Solving Endogeneity Using IV\n\nGiven an endogenous OLS regression,\n\n\\[\n    y_{i}=X_{i} \\beta+\\varepsilon_{i}, \\quad \\operatorname{cov}\\left(X_{i}, \\varepsilon_{i}\\right) \\neq 0\n\\]\n\nFind instrumental variables \\(Z_i\\) that do not (directly) inﬂuence \\(y_i\\) , but are correlated with \\(X_i\\)",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#two-stage-least-squares-stage-1",
    "href": "Week8-Lecture2.html#two-stage-least-squares-stage-1",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "2.2 Two-Stage Least Squares: Stage 1",
    "text": "2.2 Two-Stage Least Squares: Stage 1\n\nRun a regression with X ~ Z. The predicted \\(\\hat X\\) is predicted by Z, which should be uncorrelated with the error term \\(\\epsilon\\).\n\n\\(\\hat{X}\\) (the part of changes in \\(X\\) due to \\(Z\\)) is exogenous, because \\(Z\\) is exogenous\nAll endogenous parts are now left over in the error term in the first-stage regression \\(\\epsilon_{i}\\)\n\n\n\\[\nX_{i}=Z_{i}\\eta+\\epsilon_{i}\n\\]",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#two-stage-least-squares-stage-2",
    "href": "Week8-Lecture2.html#two-stage-least-squares-stage-2",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "2.3 Two-Stage Least Squares: Stage 2",
    "text": "2.3 Two-Stage Least Squares: Stage 2\n\nRun a regression with \\(Y\\) ~ \\(\\hat{X}\\): now \\(\\hat{X}\\) is uncorrelated with the error term and thus we can get causal inference from the second stage regression.\n\n\\[\ny_{i}=\\hat{X} \\beta+\\varepsilon_{i}, \\quad \\operatorname{cov}\\left(\\hat{X}_{i}, \\varepsilon_{i}\\right) = 0\n\\]",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#after-class-readings",
    "href": "Week8-Lecture2.html#after-class-readings",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "2.4 After-Class Readings",
    "text": "2.4 After-Class Readings\n\nNext week, we are going to discuss a case study using IV and 2SLS. Please read the case study before the next class.\n(optional) Econometrics with R: Instrumental Variables Regression",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Week8-Lecture2.html#footnotes",
    "href": "Week8-Lecture2.html#footnotes",
    "title": "Class 16 Instrumental Variables and Two-Stage Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAngrist, Joshua D., Stacey H. Chen, and Jae Song. “Long-term consequences of Vietnam-era conscription: New estimates using social security data.” American Economic Review 101, no. 3 (2011): 334-38.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 8] Endogeneity and Instrumental Variables",
      "Lecture 2: Instrumental Variables and Two-Stage Least Squares"
    ]
  },
  {
    "objectID": "Case-ProfitabilityAnalysis.html#marketing-decision-a-static-view",
    "href": "Case-ProfitabilityAnalysis.html#marketing-decision-a-static-view",
    "title": "Profitability Analysis for Apple Inc",
    "section": "2.1 Marketing Decision: A Static View",
    "text": "2.1 Marketing Decision: A Static View\nThe marketing analytics team has proposed a plan of an influencer marketing campaign. Influencer marketing is a type of social media marketing that entails endorsements and product placement by influencers, individuals and organizations with a reputed expert degree of knowledge or social influence in their industry. Influencers are individuals who have the ability to influence others’ purchasing habits other quantifiable activities by uploading original—often sponsored—content to social media platforms such as TikTok, Instagram, YouTube, Snapchat, or other social media platforms.\nThe team proposes to collaborate with the top tech influencers on Tiktok, Instagram, and Youtube to promote the new iPhone 16. The total one-off budget for endorsement fee is £100 million.\nAnd from historical data, the team estimates that such an influencer campaign can boost the total sales within the next financial year by 2.5%.\n\nQuestion 3Answer\n\n\nBased on the information at hand, should Tom approve the influencer marketing plan?\n\n\nTo decide whether Tom should approve the marketing plan, we need to conduct break-even analyses.\nThe first step is to compute the break-even quantity, as shown in the following code.\n\n\nCode\n# numerator is the marketing expense\n# denominator is the \"extra profit\", or the contribution margin, from selling one more unit\nBEQ &lt;- endorsement_fee / contribution_margin\nBEQ\n\n\n[1] 0.2361442\n\n\nThe next step is to compare BEQ with the estimated incremental sales from the campaign.\n\n\nCode\n# check if incremental sales is greater than BEQ\nincremental_sales &lt;- quantity * endorsement_sales_increase\n\nprint(paste(\"Incremental sales: \", incremental_sales, \" million units\"))\n\n\n[1] \"Incremental sales:  0.25  million units\"\n\n\nCode\nif (incremental_sales &gt; BEQ) {\n    print(\"It is profitable to continue with the influencer marketing campaign.\")\n} else {\n    print(\"It is not profitable to continue with the influencer marketing campaign.\")\n}\n\n\n[1] \"It is profitable to continue with the influencer marketing campaign.\"\n\n\nBEQ is 0.2361442 million units, which means in order not to lose any money, the influencer marketing campaign needs to bring in additional 0.2361442 million units;\nIn reality, the company can actually sell 0.25 million units, so it’s profitable to continue with the influencer marketing campaign.\nTherefore, based on the above reasoning, Tom should approve the influencer marketing campaign.\n\n\n\n\n\n\nSales\n\n\n\nIn this module (and in practice), when we talk about sales, we mean the quantity sales, the number of units sold. For instance, in the case study, the original sales without influencer marketing is 10 million units.\nThe total money made is often called revenue or revenue sales. For instance, in the case study, the original revenue is 6000 million pounds.",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Case Study: Profitability Analysis for Apple Inc."
    ]
  },
  {
    "objectID": "Case-ProfitabilityAnalysis.html#marketing-decision-a-dynamic-view",
    "href": "Case-ProfitabilityAnalysis.html#marketing-decision-a-dynamic-view",
    "title": "Profitability Analysis for Apple Inc",
    "section": "2.2 Marketing Decision: A Dynamic View",
    "text": "2.2 Marketing Decision: A Dynamic View\nIn the afternoon, during a board meeting, the CFO reported that the company was facing increasing uncertainty regarding future cash flows due to more strict EU regulations on Apple. Specifically, since mid September, alternative app distribution platforms have been allowed on Apple’s devices, which may lead to a significant decrease in Apple’s App Store revenue. Meanwhile, the good news is that the Bank of England has announced a 0.5% decrease in the base interest rate, which will reduce the cost of financing for Apple Inc.\nThe current cost of financing, weighted average cost of capital (WACC),2 is 10% annually. Therefore, any marketing event is recommended to take time value of money into consideration.\nRight after the meeting, Tom asked his team for a decomposition of the predicted annual incremental sales, 2.5%, into a more granular monthly level analysis.\nThe team came back with the predicted monthly incremental sales: with influencer marketing, the first month sales will increase by 0.3% and 0.2% in the following 11 months.\n\nQuestion 4Answer\n\n\nBased on the information at hand, should Tom approve the influencer marketing plan based on Net Present Value method?\n\n\nStep 1: Compute the sequence of monthly cash flows\n\nFirst, we compute the incremental sales percentage for each month, relative to the 10 million. This is a 12-element vector, each element representing the incremental sales percentage.\n\n\n\nCode\n# incremental sales percentage for the first month 0.3%\nincremental.sales.percentage_1stmonth &lt;- 0.003\n\n# incremental sales percentage for the next 11 months, which is 0.2% each month\n# We use rep() to repeat the same value 11 times\nincremental.sales.percentage_next11months &lt;- rep(0.002, 11)\n\n# incremental profit for the next 12 months\n# combine the two vectors using c()\nvector_incremental.sales.percentage_12months &lt;- c(incremental.sales.percentage_1stmonth, incremental.sales.percentage_next11months)\n\n# print the vector\n\nprint(paste(\"Incremental sales percentage for 12 months are \"))\n\n\n[1] \"Incremental sales percentage for 12 months are \"\n\n\nCode\nprint(vector_incremental.sales.percentage_12months)\n\n\n [1] 0.003 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n\n\n\nNext, we multiply the incremental sales percentage with quantity, to get the incremental sales in terms of units each month.\n\n\n\nCode\n# multiply the incremental sales percentage with quantity to get the incremental sales in units (million)\nvector_incremental.sales.units_12months &lt;- vector_incremental.sales.percentage_12months * quantity\n\nprint(paste(\"Incremental sales in units for 12 months are \"))\n\n\n[1] \"Incremental sales in units for 12 months are \"\n\n\nCode\nprint(vector_incremental.sales.units_12months)\n\n\n [1] 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n\n\n\nLastly, we multiply the incremental quantity sales with the contribution margin per unit, to get the total contribution margins (incremental profits) for each month, i.e., the CFs for each month.\n\nFor example, the first month’s incremental sales is 0.03 million units, and the contribution margin is 423.47 pounds per unit. Therefore, the incremental profit for the first month is 12.7041 million pounds.\n\n\n\n\nCode\nvector_CF &lt;- vector_incremental.sales.units_12months * contribution_margin\n\nvector_CF\n\n\n [1] 12.7041  8.4694  8.4694  8.4694  8.4694  8.4694  8.4694  8.4694  8.4694\n[10]  8.4694  8.4694  8.4694\n\n\nStep 2. Compute the sequence of discount factors\n\n\nCode\n# divide annual wacc to get monthly wacc\nmonthly_WACC &lt;- 0.1 / 12\n\n# discount factor for 1 month is 1/(1+k)\ndiscount_factor &lt;- 1 / (1 + monthly_WACC)\n\n# Generate a geometric sequence vector of discounted CFs for 12 months\n\nvector_discount_factor &lt;- discount_factor^c(1:12)\n\nprint(paste(\"Discount factors for 12 months are \"))\n\n\n[1] \"Discount factors for 12 months are \"\n\n\nCode\nvector_discount_factor\n\n\n [1] 0.9917355 0.9835394 0.9754110 0.9673497 0.9593551 0.9514265 0.9435635\n [8] 0.9357654 0.9280319 0.9203622 0.9127559 0.9052124\n\n\nStep 3. Compute the NPV\n\nMultiply CF vector with discount factor vector, to get the discounted CF vector for 12 months.\n\n\n\nCode\nvector_discounted.CF &lt;- vector_CF * vector_discount_factor\n\nvector_discounted.CF\n\n\n [1] 12.599107  8.329988  8.261146  8.192872  8.125162  8.058012  7.991417\n [8]  7.925372  7.859873  7.794915  7.730495  7.666606\n\n\n\nuse function sum() to get the sum of all elements in a vector. That is, the sum of discounted cash flows in all 12 months.\n\n\n\nCode\nprint(paste(\"The sum of discounted cash flows for 12 months is \"))\n\n\n[1] \"The sum of discounted cash flows for 12 months is \"\n\n\nCode\nsum(vector_discounted.CF)\n\n\n[1] 100.535\n\n\n\nWe need to subtract the endorsement fee, which is the marketing expense, to get the net present value\n\n\n\nCode\nNPV &lt;- sum(vector_discounted.CF) - endorsement_fee\n\nprint(paste(\"The Net Present Value is \"))\n\n\n[1] \"The Net Present Value is \"\n\n\nCode\nNPV\n\n\n[1] 0.5349641\n\n\n\n\nCode\nif (NPV &gt; 0) {\n    print(\"It is profitable to continue with the influencer marketing campaign, because the NPV is positive.\")\n} else {\n    print(\"It is not profitable to continue with the influencer marketing campaign, because the NPV is negative.\")\n}\n\n\n[1] \"It is profitable to continue with the influencer marketing campaign, because the NPV is positive.\"",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Case Study: Profitability Analysis for Apple Inc."
    ]
  },
  {
    "objectID": "Case-ProfitabilityAnalysis.html#footnotes",
    "href": "Case-ProfitabilityAnalysis.html#footnotes",
    "title": "Profitability Analysis for Apple Inc",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDr Meow’s personal favorite! Highly recommended after a long day of studies. Go for 30% sugar, less ice—trust me, it’s perfection! 🧋↩︎\nWACC is the average rate of return a company expects to compensate all its different investors. It reflects the cost of capital for the company, which is usually a blend of the cost of equity and the cost of debt.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 1] Module Introduction and Profitability Analysis",
      "Case Study: Profitability Analysis for Apple Inc."
    ]
  },
  {
    "objectID": "Week5-Lecture2.html",
    "href": "Week5-Lecture2.html",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "",
    "text": "Recently, M&S has launched its highly anticipated Beauty Advent Calendar for 2024, a curated selection of beauty and skincare products worth over £300. This limited-edition calendar is available to customers for only £50. With the holiday season approaching, M&S wants to maximize the reach and response of its marketing campaign by promoting the advent calendar offer to the right customers.\nM&S decides to use a conventional mailing marketing strategy, where customers receive color-printed leaflets via Royal Mail to their doorsteps. Each mail costs £1.5 to produce and another £0.5 to mail to the customers. If a customer responds to the offer, M&S expects them to spend £35 on full-price clothing, homeware or beauty, and purchase the advent calendar at £50. The COGS for clothing, homeware, and beauty products is 85%. And the COGS for the advent calendar is 90%.\n\n\n\n\nCost: Each mail costs £1.5 to produce and another £0.5 to mail to the customers.\nThe cost is the marketing offer we send, cost_per_offer\n\nBased on the information provided, calculate the following values:\n\ncost_per_offer: the cost of sending an marketing offer\n\n\nBenefit: If customer responds to the offer, the management expects customers to buy our products and generate profits for M&S.\nThe benefit is the profit margin if a customer responds, profit_per_customer\n\n\nprofit_per_customer: the profit from a customer if a customer responds to the marketing offer\n\n\n\n\n\nBlanket marketing: Send marketing offers to all 2000 customers. Compute the ROI for blanket marketing.\nWe already know the cost of sending an offer is cost_per_offer. We can calculate the total marketing costs by multiplying the cost per offer by the number of customers in the dataset.\nBased on the Response variable in the dataset, calculate the total number of customers who responded to the marketing offer. And then calculate the total profit from the marketing campaign.\n\nTip: you can use data_full$Response to extract the Response variable as a vector in the dataset. Based on this vector, you can calculate the total number of responding customers and the total profit from the marketing campaign.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#background",
    "href": "Week5-Lecture2.html#background",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "",
    "text": "Recently, M&S has launched its highly anticipated Beauty Advent Calendar for 2024, a curated selection of beauty and skincare products worth over £300. This limited-edition calendar is available to customers for only £50. With the holiday season approaching, M&S wants to maximize the reach and response of its marketing campaign by promoting the advent calendar offer to the right customers.\nM&S decides to use a conventional mailing marketing strategy, where customers receive color-printed leaflets via Royal Mail to their doorsteps. Each mail costs £1.5 to produce and another £0.5 to mail to the customers. If a customer responds to the offer, M&S expects them to spend £35 on full-price clothing, homeware or beauty, and purchase the advent calendar at £50. The COGS for clothing, homeware, and beauty products is 85%. And the COGS for the advent calendar is 90%.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#cost-benefit-analyses",
    "href": "Week5-Lecture2.html#cost-benefit-analyses",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "",
    "text": "Cost: Each mail costs £1.5 to produce and another £0.5 to mail to the customers.\nThe cost is the marketing offer we send, cost_per_offer\n\nBased on the information provided, calculate the following values:\n\ncost_per_offer: the cost of sending an marketing offer\n\n\nBenefit: If customer responds to the offer, the management expects customers to buy our products and generate profits for M&S.\nThe benefit is the profit margin if a customer responds, profit_per_customer\n\n\nprofit_per_customer: the profit from a customer if a customer responds to the marketing offer",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#roi-for-blanket-marketing",
    "href": "Week5-Lecture2.html#roi-for-blanket-marketing",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "",
    "text": "Blanket marketing: Send marketing offers to all 2000 customers. Compute the ROI for blanket marketing.\nWe already know the cost of sending an offer is cost_per_offer. We can calculate the total marketing costs by multiplying the cost per offer by the number of customers in the dataset.\nBased on the Response variable in the dataset, calculate the total number of customers who responded to the marketing offer. And then calculate the total profit from the marketing campaign.\n\nTip: you can use data_full$Response to extract the Response variable as a vector in the dataset. Based on this vector, you can calculate the total number of responding customers and the total profit from the marketing campaign.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#break-even-analysis-break-even-response-rate",
    "href": "Week5-Lecture2.html#break-even-analysis-break-even-response-rate",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "2.1 Break-Even Analysis: Break-Even Response Rate",
    "text": "2.1 Break-Even Analysis: Break-Even Response Rate\n\nIn order to break-even, we can calculate the break-even response rate from customers, which is the minimum response rate we need of a customer in order not to lose money from sending the marketing offer1\n\n\nOnly if a customer responds to us with at least the break-even response rate can we recover the costs of making an marketing offer.\nIf we send offers to customers whose expected response rate is lower than the break-even response rate, we make a loss by expectation.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#workflow-using-supervised-learning",
    "href": "Week5-Lecture2.html#workflow-using-supervised-learning",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "2.2 Workflow using Supervised Learning",
    "text": "2.2 Workflow using Supervised Learning\n\nData collection and cleaning\n\nSend marketing offers to a random sample of customers and collect their responses (done by M&S)\nSplit the data into a training set and a test set\n\nData analytics\n\nTrain predictive models on the training set\nPredict customer response rate on the test set\n\nBusiness recommendations\n\nTarget customers based on predicted response rate\nCompute and compare ROIs for each targeting method: (1) blanket marketing; (2) decision tree; (3) random forest\n\n\nLet’s work on the Quarto document together!",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#customer-life-cycle",
    "href": "Week5-Lecture2.html#customer-life-cycle",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "3.1 Customer Life Cycle",
    "text": "3.1 Customer Life Cycle\n\nAcquisition\n\nUse predictive analytics to target responsive customers to reduce marketing costs\n\nDevelopment\n\nUse predictive analytics to recommend products to customers (personalized recommendation system); for each customer, promote the item with the highest purchase probability\n\nRetention\n\nUse predictive analytics to find valuable customers who are likely to churn and conduct targeted churn management",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week5-Lecture2.html#footnotes",
    "href": "Week5-Lecture2.html#footnotes",
    "title": "Class 10 (Case Study) Customer Targeting Using Supervised Learning for M&S",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe idea break-even is similar to the break-even quantity we learned in Week 1, the minimum incremental quantity we need to sell in order not to lose any money.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 2: Case Study: Customer Targeting Using Supervised Learning for M&S"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html",
    "href": "Week2-Lecture1.html",
    "title": "Class 3 Customer Lifetime Value",
    "section": "",
    "text": "Understand the concept of customer-centric marketing and customer lifecycle\nUnderstand the concept of customer acquisition cost (CAC) and how to compute it with R\nUnderstand the concept of Customer Lifetime Value (CLV) and how to compute it with R\n\n\n\n\n\nIn the past, companies focused on product-centric marketing, while ignoring the long-term relationship with customers.\n\nProduct-centric marketing: focus on the promotion of a specific product or service\n\nCustomer-centric marketing is a strategy that places the individual customer at the center of marketing design and delivery.\n\n\n\n\n\nThe customer life cycle is a term used to describe the progression of steps a customer goes through when considering, purchasing, using, and maintaining loyalty to a firm.\n\nAcquisition: Persuade a prospect customer to purchase for the first time\nDevelopment: Increase the customer’s value by upselling higher-margin products or cross-selling complementary products and services\nRetention: keep the customer loyal to the brand\n\n\n\n\n\n\n\n\n\n\n\nCustomer Life Cycle",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#class-objectives",
    "href": "Week2-Lecture1.html#class-objectives",
    "title": "Class 3 Customer Lifetime Value",
    "section": "",
    "text": "Understand the concept of customer-centric marketing and customer lifecycle\nUnderstand the concept of customer acquisition cost (CAC) and how to compute it with R\nUnderstand the concept of Customer Lifetime Value (CLV) and how to compute it with R",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#from-product-centric-to-customer-centric-marketing",
    "href": "Week2-Lecture1.html#from-product-centric-to-customer-centric-marketing",
    "title": "Class 3 Customer Lifetime Value",
    "section": "",
    "text": "In the past, companies focused on product-centric marketing, while ignoring the long-term relationship with customers.\n\nProduct-centric marketing: focus on the promotion of a specific product or service\n\nCustomer-centric marketing is a strategy that places the individual customer at the center of marketing design and delivery.",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#customer-life-cycle-1",
    "href": "Week2-Lecture1.html#customer-life-cycle-1",
    "title": "Class 3 Customer Lifetime Value",
    "section": "",
    "text": "The customer life cycle is a term used to describe the progression of steps a customer goes through when considering, purchasing, using, and maintaining loyalty to a firm.\n\nAcquisition: Persuade a prospect customer to purchase for the first time\nDevelopment: Increase the customer’s value by upselling higher-margin products or cross-selling complementary products and services\nRetention: keep the customer loyal to the brand\n\n\n\n\n\n\n\n\n\n\n\nCustomer Life Cycle",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#customer-acquisition-cost-cac",
    "href": "Week2-Lecture1.html#customer-acquisition-cost-cac",
    "title": "Class 3 Customer Lifetime Value",
    "section": "2.1 Customer Acquisition Cost (CAC)",
    "text": "2.1 Customer Acquisition Cost (CAC)\n\n\n\n\n\n\nDefinition\n\n\n\nCustomer Acquisition Cost (CAC) refers to the total expenses incurred by a business to acquire a new customer. This includes costs related to marketing, sales, and any other efforts made to attract and convert potential customers into actual buyers.\n\n\n\nWhy should we care about CAC?\n\nAcquiring new customers is not always beneficial if the costs of acquiring them exceed the revenue they generate.\nFor example, no company would want to spend £500 to acquire a new customer worth £300",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#how-to-acquire-new-customers",
    "href": "Week2-Lecture1.html#how-to-acquire-new-customers",
    "title": "Class 3 Customer Lifetime Value",
    "section": "2.2 How to Acquire New Customers",
    "text": "2.2 How to Acquire New Customers\n\nFree sampling/trials",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#how-to-acquire-new-customers-1",
    "href": "Week2-Lecture1.html#how-to-acquire-new-customers-1",
    "title": "Class 3 Customer Lifetime Value",
    "section": "2.3 How to Acquire New Customers",
    "text": "2.3 How to Acquire New Customers\n\nReferral Programs",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#customer-acquisition-cost-calculation",
    "href": "Week2-Lecture1.html#customer-acquisition-cost-calculation",
    "title": "Class 3 Customer Lifetime Value",
    "section": "2.4 Customer Acquisition Cost: Calculation",
    "text": "2.4 Customer Acquisition Cost: Calculation\n\nWhen the marketing cost can be attributed to individual customers, the CAC can be calculated as the cost of making a marketing offer divided by the response rate of the customer.\n\nCAC = (# of offers needed to acquire 1 customer) * (cost of making a marketing offer)\nCAC = (cost of making 1 marketing offer) / (customer response rate)\n\nAfter we study machine learning later in this module, we will be able to predict response rate for each individual customer and compute individual-specific CAC.",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#customer-acquisition-cost-an-example",
    "href": "Week2-Lecture1.html#customer-acquisition-cost-an-example",
    "title": "Class 3 Customer Lifetime Value",
    "section": "2.5 Customer Acquisition Cost: An Example",
    "text": "2.5 Customer Acquisition Cost: An Example\nA new Bubble Tea shop MeowMeow Bubble Tea in Canary Wharf is contemplating whether or not to attract new customers by sending ads leaflets to nearby residents.\nThe cost of sending a leaflet, which includes production and labor costs, is £0.5.\n\nSending out leaflets randomly to all nearby residents\n\nexpected response rate of 1%\n\nUsing names purchased from a marketing agency\n\neach name costs £0.2\nexpected response rate of 4% by analyzing the buying behavior and demographics of current customers\n\n\nCompute the CAC for each choice.\n\n\nCode\ncost_per_random_offer &lt;- 0.5\nresponse_rate_random_offer &lt;- 0.01\n# Following the formula in the previous slide\nCAC_random_offer &lt;- cost_per_random_offer / response_rate_random_offer\nCAC_random_offer\n\n\n[1] 50\n\n\n\n\nCode\ncost_per_targeted_offer &lt;- 0.5 + 0.2 \nresponse_rate_targeted_offer &lt;- 0.04\nCAC_targeted_offer &lt;- cost_per_targeted_offer/response_rate_targeted_offer\nCAC_targeted_offer\n\n\n[1] 17.5",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#customer-lifetime-value-clv-1",
    "href": "Week2-Lecture1.html#customer-lifetime-value-clv-1",
    "title": "Class 3 Customer Lifetime Value",
    "section": "3.1 Customer Lifetime Value (CLV)",
    "text": "3.1 Customer Lifetime Value (CLV)\n\n\n\n\n\n\nDefinition\n\n\n\nCustomer lifetime value (CLV or LTV) is the total worth to a business of a customer over the whole period of their relationship.\n\n\n\nThe underlying idea of CLV is essentially NPV, but at the customer level–Think of acquiring a new customer as an investment in an “asset” that can generate future cash flows.\nCLV is a key metric for customer-centric marketing. It helps companies to decide how much to spend on acquiring new customers and retaining existing customers.",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#clv-calculation",
    "href": "Week2-Lecture1.html#clv-calculation",
    "title": "Class 3 Customer Lifetime Value",
    "section": "3.2 CLV: Calculation",
    "text": "3.2 CLV: Calculation\n\\[\n\\mathrm{CLV} = - CAC + \\sum_{t=1}^{N} \\frac{g_t * r^{(t-1)}}{(1+k)^{t}},\nwhere \\space g_t = M_t - c_t\n\\]\n\n\\(r\\) is the average retention rate for one period; \\(r^{(t-1)}\\) is the cumulative retention rate in period \\(t\\)\n\\(N\\) is the number of periods over which the relationship is calculated\n\\(M_{t}\\) is the profit margin the customer generates from buying products and services in period \\(t\\)\n\\(c_{t}\\) is the expected cost of variable marketing costs or other expenses to the customer in period \\(t\\)\n\\(g_t\\) is the net profit the customer generates in period \\(t\\) (sometimes denoted as CF)\n\\(k\\) is the discount rate for discounting future cash flows. The discount factor \\(d = 1/(1+k)\\).",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#retention-rate",
    "href": "Week2-Lecture1.html#retention-rate",
    "title": "Class 3 Customer Lifetime Value",
    "section": "3.3 Retention Rate",
    "text": "3.3 Retention Rate\n\nThe churn rate, also known as the rate of attrition or rate of customer churn, is the rate (probability) at which customers stop doing business with the company. Sometimes we also use the term retention rate: retention rate = 1 - churn rate\n\nThe aggregate churn rate can be calculated as the number of customers lost during a certain time period divided by the number of customers at the beginning of that time period.\nThe individual churn rate: machine learning models to predict the churn rate of an individual customer (Week 5).\n\nAssumptions in the CLV formula\n\nThe retention rate is constant over time.\nThe first period retention rate is 100% (all customers stay with us after the first period).",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week2-Lecture1.html#number-of-years-of-customer-relationship",
    "href": "Week2-Lecture1.html#number-of-years-of-customer-relationship",
    "title": "Class 3 Customer Lifetime Value",
    "section": "3.4 Number of Years of Customer Relationship",
    "text": "3.4 Number of Years of Customer Relationship\n\nIf we assume infinite customer economic life, we can simplify the formula into the following using the property of geometric sequence.\n\n\\[\nC L V_{N} = \\sum_{t=1}^{N} \\frac{g r^{(t-1)}}{(1+k)^{t}} =&gt; C L V_{N}=\\mathrm{g} \\cdot \\frac{1-\\left(\\frac{r}{1+k}\\right)^{N}}{1+k-r}  =&gt;\nC L V_{\\infty}=\\frac{g}{(1+k-r)}\n\\]\n\nHowever, most of the time, we are more comfortable to assume finite customer economic life; we need to decide on a cutoff date for CLV calculation\n\nRule A: until the year when the \\(g = M-c\\) becomes negative\nRule B: industry’s average customer lifespan",
    "crumbs": [
      "Lectures",
      "[Week 2] Customer Lifetime Value",
      "Lecture 1: Customer Lifetime Value"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html",
    "href": "Week5-Lecture1.html",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "",
    "text": "A supervised learning model is used when we have one or more explanatory variables AND a response variable and we would like to learn the underlying true relationship between the explanatory variables and the response variable as accurately as possible.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe use the following notations for supervised learning tasks: \\[\nY = f(X;\\theta) + \\epsilon\n\\]\n\n\\(Y\\) is the response/outcome/target variable to be predicted\n\\(X = (X_1,X_2,...,X_p)\\) are a set of explanatory variables/features/predictors\n\\(f(X;\\theta) + \\epsilon\\) is the true relationship between \\(X\\) and \\(Y\\), or DGP, which is never known to us1; \\(\\epsilon\\) is the randomness term or error term\n\\(\\theta\\) represents the set of parameters to be learned from the data\n\n\n\n\nDepending on the type of the response variable, supervised learning tasks can be divided into two groups:\n\nClassification tasks if the outcome is categorical\n\nWhether a customer responds to marketing offers (e.g., 1 for response, 0 for no response)\nWhether a customer churns (e.g., 1 for churn, 0 for no churn)\nWhich product a customer purchases (e.g., 1 for product A, 2 for product B, etc.)\n\nRegression tasks if the outcome is continuous\n\nCustomer total spending in each period (e.g., $100, $200, etc.)\nDemand forecasting such as the daily sales of a product (e.g., 100 units, 120 units, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nEstimate or predict an output based on one or more inputs.\nFind structure and relationships from inputs. No “supervising” output.\n\n\nVariables\nExplanatory and Response variables\nExplanatory variables only\n\n\nGoal\n(1) predict new values or (2) understand existing relationship between explanatory and response variables\nplace observations from a dataset into a specific cluster\n\n\nTypes of algorithms\n(1) Regression and (2) Classification\nClustering",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#supervised-learning-1",
    "href": "Week5-Lecture1.html#supervised-learning-1",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "",
    "text": "A supervised learning model is used when we have one or more explanatory variables AND a response variable and we would like to learn the underlying true relationship between the explanatory variables and the response variable as accurately as possible.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#data-generating-process-dgp",
    "href": "Week5-Lecture1.html#data-generating-process-dgp",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "",
    "text": "We use the following notations for supervised learning tasks: \\[\nY = f(X;\\theta) + \\epsilon\n\\]\n\n\\(Y\\) is the response/outcome/target variable to be predicted\n\\(X = (X_1,X_2,...,X_p)\\) are a set of explanatory variables/features/predictors\n\\(f(X;\\theta) + \\epsilon\\) is the true relationship between \\(X\\) and \\(Y\\), or DGP, which is never known to us1; \\(\\epsilon\\) is the randomness term or error term\n\\(\\theta\\) represents the set of parameters to be learned from the data",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#types-of-supervised-learning-algorithms",
    "href": "Week5-Lecture1.html#types-of-supervised-learning-algorithms",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "",
    "text": "Depending on the type of the response variable, supervised learning tasks can be divided into two groups:\n\nClassification tasks if the outcome is categorical\n\nWhether a customer responds to marketing offers (e.g., 1 for response, 0 for no response)\nWhether a customer churns (e.g., 1 for churn, 0 for no churn)\nWhich product a customer purchases (e.g., 1 for product A, 2 for product B, etc.)\n\nRegression tasks if the outcome is continuous\n\nCustomer total spending in each period (e.g., $100, $200, etc.)\nDemand forecasting such as the daily sales of a product (e.g., 100 units, 120 units, etc.)",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#difference-between-supervised-and-unsupervised-learning",
    "href": "Week5-Lecture1.html#difference-between-supervised-and-unsupervised-learning",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "",
    "text": "Supervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nEstimate or predict an output based on one or more inputs.\nFind structure and relationships from inputs. No “supervising” output.\n\n\nVariables\nExplanatory and Response variables\nExplanatory variables only\n\n\nGoal\n(1) predict new values or (2) understand existing relationship between explanatory and response variables\nplace observations from a dataset into a specific cluster\n\n\nTypes of algorithms\n(1) Regression and (2) Classification\nClustering",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#accuracy-interpretability-tradeoff",
    "href": "Week5-Lecture1.html#accuracy-interpretability-tradeoff",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "2.1 Accuracy-Interpretability Tradeoff",
    "text": "2.1 Accuracy-Interpretability Tradeoff\n\nSimpler models are easier to interpret but gives lower accuracy\nComplicated models can give better prediction accuracy but results are hard to interpret",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#comparison-of-classic-supervised-learning-models",
    "href": "Week5-Lecture1.html#comparison-of-classic-supervised-learning-models",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "2.2 Comparison of Classic Supervised Learning Models",
    "text": "2.2 Comparison of Classic Supervised Learning Models\n\nLinear regression class models (easy to interpret, low accuracy)\n\nLinear regression coefficients have economic interpretations but prediction accuracy is low\n\nTree-based Models (balance between interpretability and accuracy)\n\nDecision tree, random forest, and gradient boosting models\n\nNeural-network based models (hard to interpret, high accuracy)\n\nDeep learning only give estimated weights that have no direct business interpretations",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#bias-error-and-variance-error",
    "href": "Week5-Lecture1.html#bias-error-and-variance-error",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "2.3 Bias Error and Variance Error",
    "text": "2.3 Bias Error and Variance Error\n\nAfter we have trained a machine learning model, we can test the model performance by looking at the errors of predictions.\n\nbias is the prediction error of the model on the historical data\nvariance is the prediction error of the model on new data",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#bias-variance-tradeoff",
    "href": "Week5-Lecture1.html#bias-variance-tradeoff",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "2.4 Bias-Variance Tradeoff",
    "text": "2.4 Bias-Variance Tradeoff\n\nIf a predictive model fits historical data too well, then it may not be flexible enough and thus have a higher chance of failing to make predictions for new data accurately. This problem is called overfitting.\nOverfitting leads to low bias but high variance. But this is not good for us because with supervised learning models, we want to have higher prediction accuracy for the future. Hence we face a bias-variance tradeoff or bias-variance dilemma.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#how-to-mitigate-overfitting",
    "href": "Week5-Lecture1.html#how-to-mitigate-overfitting",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "3.1 How to Mitigate Overfitting",
    "text": "3.1 How to Mitigate Overfitting\n\nTo mitigate the overfitting problem, when training predictive models, we need to use the cross-validation technique by splitting the full historical data into a training set and a test set.\n\nA training set (70% - 80% of labelled data): we train the ML model based on the training set.\nA test set (20% - 30% of labelled data): Using the trained ML model from the training data, we can make predictions for the test data. However, we do observe the actual outcomes for the test set, so that we can evaluate the prediction accuracy by comparing the predicted outcomes versus the actual outcomes.\n\n\n\n\n\n\n\n\n\n\n\n\nFor more complicated models with hyper-parameters such as deep learning models, we may even need to split our data into 3 sets (training, validation, and test sets).",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#underfitting",
    "href": "Week5-Lecture1.html#underfitting",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "3.2 Underfitting",
    "text": "3.2 Underfitting\n\nUnderfitting occurs when a predictive model cannot sufficiently capture the DGP even on historical data.\nUnderfitting leads to high bias as well as high variance. Thus, underfitting is the worst case, which should be avoided by all means.\nTo mitigate the underfitting problem, we need to select more suitable models.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#introduction-to-decision-tree",
    "href": "Week5-Lecture1.html#introduction-to-decision-tree",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.1 Introduction to Decision Tree",
    "text": "4.1 Introduction to Decision Tree\n\nA decision tree is a tree-like structure, which can be used for both classification and regression tasks.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#business-objective-predict-customer-response-to-marketing-offers",
    "href": "Week5-Lecture1.html#business-objective-predict-customer-response-to-marketing-offers",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.2 Business Objective: Predict Customer Response to Marketing Offers",
    "text": "4.2 Business Objective: Predict Customer Response to Marketing Offers\n\nM&S made marketing offers to customers in the data, and the variable Response represents whether or not customers responded to our offer in the previous similar marketing campaign.\nBusiness objective: Based on our historical data data_full, we want to train a decision tree model to predict the outcome variable Response based on Recency and totalspending.\nData collection and cleaning:\n\n\n\nCode\npacman::p_load(dplyr, modelsummary)\n\ndata_full &lt;- read.csv(\"https://www.dropbox.com/scl/fi/2q7ppqtyca0pd3j486osl/data_full.csv?rlkey=gsyk51q27vd1skek4qpn5ikgm&dl=1\") %&gt;%\n    mutate(totalspending = MntWines + MntFruits +\n    MntMeatProducts + MntFishProducts +\n    MntSweetProducts + MntGoldProds)",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#implementation-of-decision-tree-in-r",
    "href": "Week5-Lecture1.html#implementation-of-decision-tree-in-r",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.3 Implementation of Decision Tree in R",
    "text": "4.3 Implementation of Decision Tree in R\n\nPackage rpart provides efficient implementation of decision trees in R; Package rpart.plot provides visualizations of decision trees\n\nformula: Response ~ Recency + totalspending means that we want to predict the outcome variable Response based on the explanatory variables Recency and totalspending. In R, we use ~ to separate the outcome variable and the explanatory variables for all supervised learning tasks.\ndata: the dataset to train the model\nmethod: “class” for classification tasks, “anova” for regression tasks\n\n\n\n\nCode\n# Load the necessary packages\npacman::p_load(rpart,rpart.plot)\n\n# Below example shows how to train a decision tree\ntree1 &lt;- rpart(\n  formula = Response ~ Recency + totalspending, # formula\n  data    = data_full,\n  method  = \"class\" # classification task; or 'anova' for regression\n  )\n\n# visualize the tree\nrpart.plot(tree1)",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#how-decision-tree-works-step-1",
    "href": "Week5-Lecture1.html#how-decision-tree-works-step-1",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.4 How Decision Tree Works: Step 1",
    "text": "4.4 How Decision Tree Works: Step 1\n\n\n\n\n\n\n\n\n\nStep 1. Decision tree (DT) will try to split customers into 2 groups based on each unique value of each variable, and see which split can lead to customers being most different in terms of outcome Response.\n\n\nCode\ndata_full$totalspending |&gt; unique() |&gt; sort() |&gt; head(10)\n\n\n [1]  5  6  8  9 10 11 12 13 14 15\n\n\nCode\ndata_full$Recency |&gt; unique() |&gt; sort() |&gt; head(10)\n\n\n [1] 0 1 2 3 4 5 6 7 8 9",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#how-decision-tree-works-step-1-1",
    "href": "Week5-Lecture1.html#how-decision-tree-works-step-1-1",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.5 How Decision Tree Works: Step 1",
    "text": "4.5 How Decision Tree Works: Step 1\n\n\n\n\n\n\n\n\n\nStep 1. Decision tree (DT) will try to split customers into 2 groups based on each unique value of each variable, and see which split can lead to customers being most different in terms of outcome Response.\n\nAfter this step, DT finds that total spending is the best variable and 1396 is the best cutoff.\nDT therefore splits customers into 2 groups based on 1396.\nIn each node, the 3 numbers are (1) predicted outcome (2) predicted probability of outcome being 1, and (3) share of customers in the node",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#how-decision-tree-works-step-2",
    "href": "Week5-Lecture1.html#how-decision-tree-works-step-2",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.6 How Decision Tree Works: Step 2",
    "text": "4.6 How Decision Tree Works: Step 2\n\n\n\n\n\n\n\n\n\nStep 2. For customers in the left branch (totalspending &lt; 1396), DT will continue to split based on each unique value of each variable, and see which split can result in the customers to be most different in terms of Response.\n\nHowever, DT couldn’t find a cutoff that sufficiently differentiate customers, so DT stops in the left branch.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#how-decision-tree-works-step-3",
    "href": "Week5-Lecture1.html#how-decision-tree-works-step-3",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.7 How Decision Tree Works: Step 3 …",
    "text": "4.7 How Decision Tree Works: Step 3 …\n\n\n\n\n\n\n\n\n\nStep 3. For customers in the right branch (totalspending &gt;= 1396), DT will continue to split based on each unique value of each variable, and see which split can result in the customers to be most different in terms of Response.\n\nAfter this step, DT finds Recency is the best variable and 72 is the best cutoff. DT further splits customers into 2 groups.\n\nStep 4. This process continues until DT determines that there is no need to further split customers.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#how-decision-tree-works-step-4",
    "href": "Week5-Lecture1.html#how-decision-tree-works-step-4",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.8 How Decision Tree Works: Step 4",
    "text": "4.8 How Decision Tree Works: Step 4\n\nOnce the tree is fully grown, we can use the tree to make predictions on new customers.\nFor a new customer, we can follow the tree from the root node to the leaf node, and the predicted outcome is the outcome of the leaf node.\nIn R, we can use the predict() function to make predictions on new customers, which returns the predicted outcome of the new customers. Note that, the test data should have the same variable names as the training data.\n\n\n\nCode\n# Make predictions on the mtcars\nprediction_tree1 &lt;- predict(tree1,\n                         data = data_test)",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#advantages-of-decision-trees",
    "href": "Week5-Lecture1.html#advantages-of-decision-trees",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "4.9 Advantages of Decision Trees",
    "text": "4.9 Advantages of Decision Trees\n\nThey are very interpretable.\nMaking predictions is fast.\nIt’s easy to understand what variables are important in making the prediction. The internal nodes (splits) are those variables that most largely reduce the SSE (criteria for split).",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#disadvantages-of-decision-trees",
    "href": "Week5-Lecture1.html#disadvantages-of-decision-trees",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "5.1 Disadvantages of Decision Trees",
    "text": "5.1 Disadvantages of Decision Trees\n\nSingle regression trees tend to overfit, resulting in unstable predictions.\nDue to the high variance, single regression trees tend to have poor predictive accuracy.",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#random-forest-1",
    "href": "Week5-Lecture1.html#random-forest-1",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "5.2 Random Forest",
    "text": "5.2 Random Forest\n\nTo overcome the overfitting tendency of a single decision tree, random forest has been developed by (Breiman 2001).\n\nInstead of using all customers, each tree is grown to a subsample of customers instead of all customers (e.g., 70% of training data)\nInstead of using all features for splitting, each tree is grown to a subset of features instead of all features (e.g., 3 out of 5 features)",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#visualization-of-random-forest",
    "href": "Week5-Lecture1.html#visualization-of-random-forest",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "5.3 Visualization of Random Forest",
    "text": "5.3 Visualization of Random Forest\n\n\n\n\n\n\n\n\n\nFor a new customer,\n\nEach tree gives a prediction of the outcome\nRandom forest takes the average (for regression tasks) or majority vote (for classification tasks) of all trees’ predictions as the final prediction",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#implementation-of-random-forest-in-r",
    "href": "Week5-Lecture1.html#implementation-of-random-forest-in-r",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "5.4 Implementation of Random Forest in R",
    "text": "5.4 Implementation of Random Forest in R\n\nPackage ranger provides implementation of random forest in R.\nranger() is the function in the package to train a random forest; refer to its help function for more details.\nThe following code shows how to train a random forest consisting of 500 decision trees, where the outcome variable is mpg, and the predictors are 5 car attribute variables.\n\n\n\nCode\npacman::p_load(ranger)\nrandomforest1 &lt;- ranger(\n    formula   = Response ~ totalspending + Recency, # formula\n    data      = data_full, # dataset to train the model\n    num.trees = 500, # 500 decision trees\n    seed = 888, # make sure of replication\n    probability  = TRUE # to return predicted probabilities\n  )",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#make-predictions-from-random-forest",
    "href": "Week5-Lecture1.html#make-predictions-from-random-forest",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "5.5 Make Predictions from Random Forest",
    "text": "5.5 Make Predictions from Random Forest\n\nAfter we train the predictive model, we can use predict() function to make predictions\n\nThe 1st argument is the trained model object\nThe 2nd argument is the dataset to make predictions on\n\n\n\n\nCode\n# Make predictions on the mtcars\nprediction_rf &lt;- predict(randomforest1,\n                         data = data_full)\n\n# Because prediction_rf is a list object\n# Need to use $ to extract the predicted value as a numeric vector\nprediction_rf$predictions",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#after-class-reading",
    "href": "Week5-Lecture1.html#after-class-reading",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "5.6 After-Class Reading",
    "text": "5.6 After-Class Reading\n\n(recommended) Decision tree in R\n(recommended) Random forest in R",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week5-Lecture1.html#footnotes",
    "href": "Week5-Lecture1.html#footnotes",
    "title": "Class 9 Supervised Machine Learning and Tree-Based Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“All model are wrong, but some are useful” – George Box. As business analysts, we need to use the “wrong models” correctly.↩︎",
    "crumbs": [
      "Lectures",
      "[Week 5] Supervised Learning for Customer Targeting",
      "Lecture 1: Supervised Learning and Tree-Based Models"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html",
    "href": "Week3-Lecture2.html",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "",
    "text": "In R, missing values are represented by the symbol NA (i.e., not available).\nMost statistical models cannot handle missing values, so we need to deal with them in R.\nIf there are just a few missing values: remove them from analysis.\nIf there are many missing values: need to replace them with appropriate values:\n\nmean/median/imputation\n\n\n\n\n\n\nOutliers are data points that are significantly different from other data points in the dataset, such as unusually large and small values.\nWinsorization is a common method to deal with outliers. It replaces the extreme values with the nearest non-extreme value, usually the 99th or 1th percentile (or other thresholds as appropriate).",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html#missing-values",
    "href": "Week3-Lecture2.html#missing-values",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "",
    "text": "In R, missing values are represented by the symbol NA (i.e., not available).\nMost statistical models cannot handle missing values, so we need to deal with them in R.\nIf there are just a few missing values: remove them from analysis.\nIf there are many missing values: need to replace them with appropriate values:\n\nmean/median/imputation",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html#outliers",
    "href": "Week3-Lecture2.html#outliers",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "",
    "text": "Outliers are data points that are significantly different from other data points in the dataset, such as unusually large and small values.\nWinsorization is a common method to deal with outliers. It replaces the extreme values with the nearest non-extreme value, usually the 99th or 1th percentile (or other thresholds as appropriate).",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html#two-major-tasks-of-descriptive-analytics",
    "href": "Week3-Lecture2.html#two-major-tasks-of-descriptive-analytics",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "2.1 Two Major Tasks of Descriptive Analytics",
    "text": "2.1 Two Major Tasks of Descriptive Analytics\n\nYou can think of descriptive analytics as creating a dashboard to display the key information you would like to know for your business. For instance:\n\n\nDescribe data depending on your business purposes\n\n“How much do our customers spend each month on average?”\n“What percentage of our customers are unprofitable?”\n“What is the difference between the retention rates across different demographic groups?”\n\nConduct statistical tests (such as t-tests) for hypothesis testing.\n\nIs there any significant difference in the average spending between different age/gender groups?\nBased on our test mailing, can we conclude that ad-copy A works better than ad-copy B?",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html#example-of-descriptive-analytics-dashboard",
    "href": "Week3-Lecture2.html#example-of-descriptive-analytics-dashboard",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "2.2 Example of Descriptive Analytics Dashboard",
    "text": "2.2 Example of Descriptive Analytics Dashboard",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html#summary-statistics",
    "href": "Week3-Lecture2.html#summary-statistics",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "2.3 Summary Statistics",
    "text": "2.3 Summary Statistics\n\nSummary statistics are used to summarize a set of observations, in order to communicate the largest amount of information as simply as possible.\nThere are two main types of summary statistics used in evaluation:\n\nmeasures of central tendency: number of observations, mean, min, 25 percentile, median, 75 percentile, max, etc.\nmeasures of dispersion: range and standard deviation.\n\nIt’s important to include summary statistics table in your dissertation before any statistical analysis!",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture2.html#summary-statistics-with-r",
    "href": "Week3-Lecture2.html#summary-statistics-with-r",
    "title": "Class 6 Descriptive Analytics for M&S",
    "section": "2.4 Summary Statistics with R",
    "text": "2.4 Summary Statistics with R\n\nIn R, a power package to report summary statistics is called modelsummary.\ndatasummary_skim() is a shortcut to conduct basic summary statistics\nFor more features, refer to the package tutorial here\n\n\n\nCode\npacman::p_load(modelsummary)\ndata_full %&gt;%\n  datasummary_skim(type = \"numeric\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID                 \n                  2000\n                  0\n                  5599.2 \n                  3242.0 \n                  0.0   \n                  5492.0 \n                  11191.0 \n                  \n                \n                \n                  MntWines           \n                  738 \n                  0\n                  306.1  \n                  338.3  \n                  0.0   \n                  176.5  \n                  1493.0  \n                  \n                \n                \n                  MntFruits          \n                  157 \n                  0\n                  26.4   \n                  39.9   \n                  0.0   \n                  8.0    \n                  199.0   \n                  \n                \n                \n                  MntMeatProducts    \n                  532 \n                  0\n                  167.9  \n                  225.3  \n                  0.0   \n                  68.0   \n                  1725.0  \n                  \n                \n                \n                  MntFishProducts    \n                  179 \n                  0\n                  37.6   \n                  54.6   \n                  0.0   \n                  12.0   \n                  259.0   \n                  \n                \n                \n                  MntSweetProducts   \n                  175 \n                  0\n                  27.5   \n                  41.8   \n                  0.0   \n                  8.0    \n                  263.0   \n                  \n                \n                \n                  MntGoldProds       \n                  207 \n                  0\n                  43.8   \n                  51.7   \n                  0.0   \n                  24.0   \n                  362.0   \n                  \n                \n                \n                  NumDealsPurchases  \n                  15  \n                  0\n                  2.3    \n                  2.0    \n                  0.0   \n                  2.0    \n                  15.0    \n                  \n                \n                \n                  NumWebPurchases    \n                  15  \n                  0\n                  4.1    \n                  2.8    \n                  0.0   \n                  4.0    \n                  27.0    \n                  \n                \n                \n                  NumCatalogPurchases\n                  14  \n                  0\n                  2.7    \n                  3.0    \n                  0.0   \n                  2.0    \n                  28.0    \n                  \n                \n                \n                  NumStorePurchases  \n                  14  \n                  0\n                  5.8    \n                  3.3    \n                  0.0   \n                  5.0    \n                  13.0    \n                  \n                \n                \n                  NumWebVisitsMonth  \n                  15  \n                  0\n                  5.3    \n                  2.5    \n                  0.0   \n                  6.0    \n                  20.0    \n                  \n                \n                \n                  Complain           \n                  2   \n                  0\n                  0.0    \n                  0.1    \n                  0.0   \n                  0.0    \n                  1.0     \n                  \n                \n                \n                  Response           \n                  2   \n                  0\n                  0.2    \n                  0.4    \n                  0.0   \n                  0.0    \n                  1.0     \n                  \n                \n                \n                  Year_Birth         \n                  59  \n                  0\n                  1968.8 \n                  12.0   \n                  1893.0\n                  1970.0 \n                  1996.0  \n                  \n                \n                \n                  Income             \n                  1783\n                  1\n                  52139.7\n                  21492.4\n                  1730.0\n                  51518.0\n                  162397.0\n                  \n                \n                \n                  Kidhome            \n                  3   \n                  0\n                  0.4    \n                  0.5    \n                  0.0   \n                  0.0    \n                  2.0     \n                  \n                \n                \n                  Teenhome           \n                  3   \n                  0\n                  0.5    \n                  0.5    \n                  0.0   \n                  0.0    \n                  2.0     \n                  \n                \n                \n                  Recency            \n                  100 \n                  0\n                  49.2   \n                  29.0   \n                  0.0   \n                  50.0   \n                  99.0    \n                  \n                \n        \n      \n    \n\n\n\nCode\ndata_full %&gt;%\n  datasummary_skim(type = \"categorical\")\n\n\nWarning: These variables were omitted because they include more than 50 levels:\nDt_Customer.\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                  \n                N\n                %\n              \n        \n        \n        \n                \n                  Education     \n                  2n Cycle  \n                  185\n                  9.2 \n                \n                \n                                \n                  Basic     \n                  43 \n                  2.1 \n                \n                \n                                \n                  Graduation\n                  992\n                  49.6\n                \n                \n                                \n                  Master    \n                  327\n                  16.4\n                \n                \n                                \n                  PhD       \n                  453\n                  22.6\n                \n                \n                  Marital_Status\n                  Alone     \n                  3  \n                  0.1 \n                \n                \n                                \n                  Divorced  \n                  206\n                  10.3\n                \n                \n                                \n                  Married   \n                  767\n                  38.4\n                \n                \n                                \n                  Single    \n                  436\n                  21.8\n                \n                \n                                \n                  Together  \n                  521\n                  26.0\n                \n                \n                                \n                  Widow     \n                  67 \n                  3.4",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 2: Caset Study: Descriptive Analytics for M&S"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html",
    "href": "Week3-Lecture1.html",
    "title": "Class 5 Data Wrangling with R",
    "section": "",
    "text": "Understand the major steps to conduct data analytics. We will use improve the marketing efficiency for the M&S case study as an example.\nData collection: Learn how to collect first-hand survey data and how to load second-hand data into R\nData cleaning: Learn how to use the dplyr package to clean data\nData analysis: Learn how to conduct descriptive analytics for the M&S case study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur project for M&S in Weeks 3-5: Help M&S to improve marketing efficiency by improving its ROI on its targeted marketing offers. The project will involve data collection, data cleaning, and data analysis, including both descriptive and prescriptive analytics, to identify the most profitable customers and develop a personalized marketing targeting strategy.\n\n\n\n\nBurberry provides relevant product recommendations on Burberry.com to facilitate in-session product exploration and to create a more personalised user experience. This project is to develop a new product recommendation system that tailors suggestions to individual users based on their product selections and preferences.\nThe AXA project will explore fraud detection approaches using unsupervised ML including models such as isolation forests. The candidate will develop an understanding of the business problem and our data, formulating hypotheses and testing them. They will build, evaluate, and interpret their ML models.\nAt Waitrose, it’s crucial to balance product availability with minimizing waste by understanding sales rates. Factors like product shelf life, varying sales velocities, promotions, and unexpected trends make it challenging to find a one-size-fits-all solution. Current manual forecasting introduces inaccuracies and process delays. We aim to develop a machine learning algorithm to generate daily product forecasts, integrate with our stock management system, and enable automated, accurate forecasting.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#class-objectives",
    "href": "Week3-Lecture1.html#class-objectives",
    "title": "Class 5 Data Wrangling with R",
    "section": "",
    "text": "Understand the major steps to conduct data analytics. We will use improve the marketing efficiency for the M&S case study as an example.\nData collection: Learn how to collect first-hand survey data and how to load second-hand data into R\nData cleaning: Learn how to use the dplyr package to clean data\nData analysis: Learn how to conduct descriptive analytics for the M&S case study",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#business-objective-our-business-question-in-weeks-3---5",
    "href": "Week3-Lecture1.html#business-objective-our-business-question-in-weeks-3---5",
    "title": "Class 5 Data Wrangling with R",
    "section": "",
    "text": "Our project for M&S in Weeks 3-5: Help M&S to improve marketing efficiency by improving its ROI on its targeted marketing offers. The project will involve data collection, data cleaning, and data analysis, including both descriptive and prescriptive analytics, to identify the most profitable customers and develop a personalized marketing targeting strategy.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#business-objective-example-dissertation-projects-in-term-3",
    "href": "Week3-Lecture1.html#business-objective-example-dissertation-projects-in-term-3",
    "title": "Class 5 Data Wrangling with R",
    "section": "",
    "text": "Burberry provides relevant product recommendations on Burberry.com to facilitate in-session product exploration and to create a more personalised user experience. This project is to develop a new product recommendation system that tailors suggestions to individual users based on their product selections and preferences.\nThe AXA project will explore fraud detection approaches using unsupervised ML including models such as isolation forests. The candidate will develop an understanding of the business problem and our data, formulating hypotheses and testing them. They will build, evaluate, and interpret their ML models.\nAt Waitrose, it’s crucial to balance product availability with minimizing waste by understanding sales rates. Factors like product shelf life, varying sales velocities, promotions, and unexpected trends make it challenging to find a one-size-fits-all solution. Current manual forecasting introduces inaccuracies and process delays. We aim to develop a machine learning algorithm to generate daily product forecasts, integrate with our stock management system, and enable automated, accurate forecasting.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#types-of-data-by-source",
    "href": "Week3-Lecture1.html#types-of-data-by-source",
    "title": "Class 5 Data Wrangling with R",
    "section": "2.1 Types of Data by Source",
    "text": "2.1 Types of Data by Source\n\nPrimary Data: Data that are generated by the data analyst through surveys, interviews, experiments, specially designed for understanding and solving the research problem at hand.\nSecondary Data: Existing data generated by the company’s or consumer’s past activities, as part of organizational record keeping.\n\n\n\n\n\n\n\n\n\nBasis for Comparison\nPrimary Data\nSecondary Data\n\n\n\n\nMeaning\nPrimary data refers to the first-hand data gathered by the analyst.\nSecondary data means data collected by someone else earlier (usually by the company).\n\n\nData\nNew data\nHistorical data in the past\n\n\nSource\nSurveys, observations, experiments, questionnaire, personal interviews, etc.\nCompany databases, government publications, websites, books, journal articles, internal records, etc.\n\n\nCost\nExpensive; Very involved and costly\nEconomical; Quick and easy\n\n\nCollection time\nLong\nShort\n\n\nSpecific\nAlways specific to the researcher’s needs.\nMay or may not be specific to the researcher’s needs.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#types-of-data-by-structure",
    "href": "Week3-Lecture1.html#types-of-data-by-structure",
    "title": "Class 5 Data Wrangling with R",
    "section": "2.2 Types of Data by Structure",
    "text": "2.2 Types of Data by Structure\n\nWe often consider 2 dimensions of a dataset: unit and time.\n\nCross-sectional data: data collected at a single point in time.\nLongitudinal data: data collected over time but may not contain the same individuals.\nPanel data: data collected over time and contain the same individuals.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#primary-data-marketing-surveys",
    "href": "Week3-Lecture1.html#primary-data-marketing-surveys",
    "title": "Class 5 Data Wrangling with R",
    "section": "2.3 Primary Data: Marketing Surveys",
    "text": "2.3 Primary Data: Marketing Surveys\n\nA marketing survey is often the easiest and most cost-effective way to collect primary data. We often collect the following variables:\n\npurchase intention: how likely a customer will buy a product, helps to predict sales\nwillingness to pay: how much a customer is willing to pay for a product, helps to set the optimal price\nshopping basket: what products a customer usually buys, helps to cross-sell\nshare of wallet: how much a customer spends on a product category, helps to identify the high-potential customers for market penetration\ndemographics: gender, age, income, education, etc., helps to segment customers\n\nLet’s see an example in Mentimeter of how to design a marketing survey!\nYou can design surveys to collect data for your term 1 projects or term 3 dissertation.\n\nThe quick start guide on how to conduct market research surveys",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#limitation-of-marketing-surveys",
    "href": "Week3-Lecture1.html#limitation-of-marketing-surveys",
    "title": "Class 5 Data Wrangling with R",
    "section": "2.4 Limitation of Marketing Surveys",
    "text": "2.4 Limitation of Marketing Surveys\n\nHawthorne Effect and Response Bias: Participants may answer in ways they think are socially desirable or expected, rather than their true feelings or behavior.\nSampling Bias: The sample may not be representative of the customer population.\nFatigue: Long surveys may lead to respondent fatigue, causing rushed or careless answers toward the end. Do not ask too many questions!",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#data-frame-basics",
    "href": "Week3-Lecture1.html#data-frame-basics",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.1 Data Frame Basics",
    "text": "3.1 Data Frame Basics\n\nData Frame is the R object that we will deal with most of the time in the MSc program. You can think of data.frame as an Excel spreadsheet.\n\nEach row stands for an observation; usually a record for a customer\nEach column stands for a variable; each column should have a unique name.\nEach column must contain the same data type, but the different columns can store different data types.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#install-and-load-the-dplyr-package",
    "href": "Week3-Lecture1.html#install-and-load-the-dplyr-package",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.2 Install and Load the dplyr package",
    "text": "3.2 Install and Load the dplyr package\n\nIn R, we use the dplyr package for data cleaning and manipulation.1\n\n\n\nCode\npacman::p_load(dplyr)\n\n\n\nLoad a csv format dataset called data_full using read.csv()\n\n\n\nCode\ndata_full &lt;- read.csv(\"https://www.dropbox.com/scl/fi/2q7ppqtyca0pd3j486osl/data_full.csv?rlkey=gsyk51q27vd1skek4qpn5ikgm&dl=1\")\n\n\n\nTo browse the whole dataset, we can simply click the dataset in the environment",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#first-look-at-the-dataset",
    "href": "Week3-Lecture1.html#first-look-at-the-dataset",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.3 First Look at the Dataset",
    "text": "3.3 First Look at the Dataset\n\nWhat variables do the data have? The data types of each variable?\n\n\n\nCode\nstr(data_full)\n\n\n'data.frame':   2000 obs. of  22 variables:\n $ ID                 : int  5524 2174 4141 6182 5324 7446 965 6177 4855 5899 ...\n $ MntWines           : int  635 11 426 11 173 520 235 76 14 28 ...\n $ MntFruits          : int  88 1 49 4 43 42 65 10 0 0 ...\n $ MntMeatProducts    : int  546 6 127 20 118 98 164 56 24 6 ...\n $ MntFishProducts    : int  172 2 111 10 46 0 50 3 3 1 ...\n $ MntSweetProducts   : int  88 1 21 3 27 42 49 1 3 1 ...\n $ MntGoldProds       : int  88 6 42 5 15 14 27 23 2 13 ...\n $ NumDealsPurchases  : int  3 2 1 2 5 2 4 2 1 1 ...\n $ NumWebPurchases    : int  8 1 8 2 5 6 7 4 3 1 ...\n $ NumCatalogPurchases: int  10 1 2 0 3 4 3 0 0 0 ...\n $ NumStorePurchases  : int  4 2 10 4 6 10 7 4 2 0 ...\n $ NumWebVisitsMonth  : int  7 5 4 6 5 6 6 8 9 20 ...\n $ Complain           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Response           : int  1 0 0 0 0 0 0 0 1 0 ...\n $ Year_Birth         : int  1957 1954 1965 1984 1981 1967 1971 1985 1974 1950 ...\n $ Education          : chr  \"Graduation\" \"Graduation\" \"Graduation\" \"Graduation\" ...\n $ Marital_Status     : chr  \"Single\" \"Single\" \"Together\" \"Together\" ...\n $ Income             : int  58138 46344 71613 26646 58293 62513 55635 33454 30351 5648 ...\n $ Kidhome            : int  0 1 0 1 1 0 0 1 1 1 ...\n $ Teenhome           : int  0 1 0 0 0 1 1 0 0 1 ...\n $ Dt_Customer        : chr  \"04/09/2012\" \"08/03/2014\" \"21/08/2013\" \"10/02/2014\" ...\n $ Recency            : int  58 38 26 26 94 16 34 32 19 68 ...",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#common-data-wrangling-operations",
    "href": "Week3-Lecture1.html#common-data-wrangling-operations",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.4 Common Data Wrangling Operations",
    "text": "3.4 Common Data Wrangling Operations\n\nFilter rows (filter)\nSort rows (arrange)\nSelect columns (select)\nGenerate new columns (mutate)\nGroup aggregation (group_by)\nMerge datasets (join)",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#subset-rows-based-on-conditions-filter",
    "href": "Week3-Lecture1.html#subset-rows-based-on-conditions-filter",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.5 Subset Rows Based on Conditions: filter",
    "text": "3.5 Subset Rows Based on Conditions: filter\n\nWe can use filter() to select rows that meet certain logical criteria.\n\n\n\n\n\n\n\n\n\n\n\nImportant: To store the generated new subset of data in RStudio, we need to assign it to a new object.\n\nExample: From data_full, find customers who are single\n\n\nCode\n# keep only single customers\nfilter(data_full, Marital_Status == \"Single\" )",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#the-pipe-operator",
    "href": "Week3-Lecture1.html#the-pipe-operator",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.6 The Pipe Operator",
    "text": "3.6 The Pipe Operator\n\n3.6.1 Pipe Operator\n%&gt;% passes the object in front as the first argument of the subsequent function.2",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#example-of-the-pipe-operator",
    "href": "Week3-Lecture1.html#example-of-the-pipe-operator",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.7 Example of the Pipe Operator",
    "text": "3.7 Example of the Pipe Operator\n\n\nCode\n# without using pipe\nfilter(data_full, Marital_Status == 'Single')\n\n# with pipe \ndata_full %&gt;% filter(Marital_Status == 'Single')",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#why-do-we-need-pipe-operator-for-data-wrangling",
    "href": "Week3-Lecture1.html#why-do-we-need-pipe-operator-for-data-wrangling",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.8 Why Do We Need Pipe Operator for Data Wrangling?",
    "text": "3.8 Why Do We Need Pipe Operator for Data Wrangling?\n\nExercise: find out single customers who have a PhD without using pipe.\n\n\n\nCode\n# based on data_full, find out customers who are single\ndata_full_single &lt;- filter(data_full, Marital_Status == \"Single\")\n\n# based on data_full_single, find out customers who are single and have PhD\ndata_full_single_PhD &lt;- filter(data_full_single, Education == \"PhD\")\n\n\n\nExercise: find out single customers who have a PhD using pipe.\n\n\n\nCode\ndata_full_single_PhD &lt;- data_full %&gt;%\n  filter(Marital_Status == 'Single') %&gt;%\n  filter(Education == 'PhD')\n  \n  ## You can even continue with more filter steps with more pipe operators\n\n\n\nThe pipe works like a conveyor belt in a factory, passing the intermediate outputs from the previous data wrangling step to the next step for further processing until you finish your data wrangling task.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#sort-rows-arrange",
    "href": "Week3-Lecture1.html#sort-rows-arrange",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.9 Sort Rows: arrange",
    "text": "3.9 Sort Rows: arrange\n\narrange() orders the rows by the values of selected columns.\n\nascending order by default; for descending order, put a minus sign in front of the variable.\nallows multiple sorting variables separated by comma.\n\nExample: sort customers based on income in descending order.\n\n\n\nCode\ndata_full %&gt;% \n  arrange(-Income) %&gt;%\n  head(10)\n\n\n\n  \n\n\n\n\nExercise: sort customers based on income in descending order and age in ascending order.\n\n\n\nCode\ndata_full %&gt;% \n  mutate(Age = 2023 - Year_Birth) %&gt;%\n  arrange(-Income, Age) %&gt;%\n  head(10) # only show the first 10 rows using head(10)",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#generate-new-variables-mutate",
    "href": "Week3-Lecture1.html#generate-new-variables-mutate",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.10 Generate New Variables: mutate",
    "text": "3.10 Generate New Variables: mutate\n\nmutate() generates new variables in the dataset while preserving existing variables\nExample: create a new variable named Age from Year_Birth.\n\n\n\nCode\ndata_full %&gt;%\n  mutate(Age = 2023 - Year_Birth) %&gt;%\n  head(10)\n\n\n\n  \n\n\n\n\nExercise: create a new variable named totalkids, which is the sum of Kidhome and Teenhome.\n\n\n\nCode\ndata_full %&gt;%\n  mutate(totalkids = Kidhome + Teenhome) %&gt;%\n  head(10)",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.11 Aggregation by Groups: group_by",
    "text": "3.11 Aggregation by Groups: group_by\n\ngroup_by() allows us to aggregate data by group and compute statistics for each group\n\n\n\nCode\n# group by marital status\ndata_full %&gt;%\n    group_by(Marital_Status) \n\n\n\nInternally, the dataset is already grouped based on the specified variable(s).",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by-summarise",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by-summarise",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.12 Aggregation by Groups: group_by() %>% summarise()",
    "text": "3.12 Aggregation by Groups: group_by() %&gt;% summarise()\n\nAfter aggregating data, we can use summarise() to compute group-specific statistics for us.\n\nSimilar to mutate() in generating new variables\nDifferent from mutate() in that the new variable is computed based on groups.\n\n\n\n\nCode\n# compute the average income for each marital status group\ndata_full %&gt;%\n  group_by(Marital_Status) %&gt;% \n  summarise(avg_income = mean(Income,na.rm = T)) %&gt;%\n  ungroup()\n\n\n\nWhat if you replace summarise() with mutate()?",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#aggregation-by-groups-group_by-multiple-groups",
    "href": "Week3-Lecture1.html#aggregation-by-groups-group_by-multiple-groups",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.13 Aggregation by Groups: group_by() Multiple Groups",
    "text": "3.13 Aggregation by Groups: group_by() Multiple Groups\n\nWe can have multiple group variables for group_by , such as computing average income for each marital status, education combination\n\n\n\nCode\n# compute the average income for each marital, education group\ndata_full %&gt;%\n  group_by(Marital_Status,Education) %&gt;% \n  summarise(avg_income = mean(Income,na.rm = T)) %&gt;% \n  ungroup() %&gt;%\n  head(5)",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#after-class",
    "href": "Week3-Lecture1.html#after-class",
    "title": "Class 5 Data Wrangling with R",
    "section": "3.14 After-Class",
    "text": "3.14 After-Class\n\n(essential) Cheatsheet for dplyr. This cheatsheet provides a quick reference for the most commonly used functions in the dplyr package. It’s very important to familiarize yourself with these functions as you will use them a lot in your future projects.\n(optional) Complete the after-class exercise for Week 3. If you still have time, you can also complete the data camp exercise on the dplyr package. The link is here.",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  },
  {
    "objectID": "Week3-Lecture1.html#footnotes",
    "href": "Week3-Lecture1.html#footnotes",
    "title": "Class 5 Data Wrangling with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npacman is a package management tool that makes our lives easier by loading multiple packages at once.↩︎\nStarting from R 4.0 version, base R introduces the native pipe operator |&gt;↩︎",
    "crumbs": [
      "Lectures",
      "[Week 3] Data Wrangling and Descriptive Analytics",
      "Lecture 1: Data Wrangling with R"
    ]
  }
]