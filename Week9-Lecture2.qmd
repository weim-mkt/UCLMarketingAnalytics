---
author: Dr Wei Miao
institute: UCL School of Management
date: "`r (lubridate::ymd('20231004')+lubridate::dweeks(8))`"
date-format: long
title: "Class 18 Difference-in-Differences Design"
df-print: kable
colorlinks: true
code-line-numbers: true
format:
  html: 
    toc: true
    number-sections: true
    page-layout: full
    toc-depth: 2
    code-line-numbers: true
    code-copy: hover
  beamer: 
    toc: false
    toc-title: ""
    slide-level: 2
    section-titles: true
    theme: Frankfurt
    colortheme: beaver
    fonttheme: structurebold
    navigation: horizontal
    tbl-colwidths: auto
    fontsize: 9pt
knitr:
  opts_chunk:
    echo: true
    warning: true
    message: true
    error: true
execute: 
  freeze: auto
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

# Difference-in-Differences Design

## What Can Go Wrong with RDiT

-   The underlying assumption for RDiT is that, the pre-treatment outcomes before the natural experiment are good counterfactual outcomes for the post-treatment outcomes if the natural experiment had not happened.

$$
Y_{it}=\alpha+\beta_{1} Post_{it}   + \mu_{i t}
$$

-   However, we cannot guarantee that there is no seasonality. Therefore, we need to find alternative methods that require less strict assumptions than RDiT.

## Difference-in-Differences Design

-   If we can find a control group of individuals who are unaffected by the natural experiment, we can then use difference-in-differences design.

-   **Difference-in-differences design** (**DiD**, **DD**, **Diff-in-Diff**) is a statistical technique used in economics that attempts to mimic an experimental research design using observational data, by comparing the changes in the outcomes of the treatment group with the changes in the outcomes of the control group.

## Visualization of DiD Design

::: {.content-visible when-format="beamer"}
![](images/DiDGraph.png){fig-align="center" width="267"}
:::

::: {.content-visible when-format="html"}
![](images/DiDGraph.png){fig-align="center"}
:::

## DiD Estimator: Alternative Illustration

-   We can use a DiD regression to quantify the treatment effects:
    -   $Post_{t}$ controls for the seasonality for all customers
    -   $Treated_{i}$ controls for the pre-existing difference between the treatment group and control group.

$$
Outcome_{i, t}=\beta_0+ \beta_{1} Post_{t}+\beta_{2} Treated_{i}+\beta_{3}  Treated_{i} \times Post_{t}  + \mu_{i, t}
$$

-   Therefore, after teasing out (1) seasonality $\beta_1$ and (2) pre-existing across-group differences $\beta_2$ , $\beta_3$ measures the actual treatment effect,

## Parallel Pre-trend Assumption

-   The requirement for a valid DiD analysis is that there is **no differential trend** between the treatment and control group before the treatment happens, or we must need **parallel pre-trend**.

::: {.content-visible when-format="beamer"}
![](images/DiDPretrend.png){fig-align="center" width="300"}
:::

::: {.content-visible when-format="html"}
![](images/DiDPretrend.png){fig-align="center"}
:::

# Implementation of DiD Using R

## The Causal Effect of a New Loyalty Program

-   Tesco introduces a new loyalty program in Manchester in June while customers in London still use the old loyalty program.
    -   $y$: standardized customer spending
    -   $x1$: standardized customer income.
    -   $id$: Identifier of the customer.
    -   $period$: January to October
    -   $post$: equals 1 for June and onwards.
    -   $treat$: equals 1 for Manchester customers.

```{r}
pacman::p_load(fixest,modelsummary,dplyr)
data("base_did")
```

::: callout-tip
When randomization is difficult, you can use DiD design to replace randomized experiments.
:::

## Dataset

```{r}
head(base_did,10)
```

## Estimation of DiD

-   We need to run a linear regression with 3 variables: treat, post, and the interaction term

$$
Outcome_{i, t}=\beta_0+ \beta_{1} Post_{t}+\beta_{2} Treated_{i}+\beta_{3}  Treated_{i} \times Post_{t}  + \mu_{i, t}
$$

```{r}
est_did = feols(
  fml = y ~ treat + post + treat:post, 
                data = base_did)
```

## Report the DiD Results

```{r}
#| echo: false
modelsummary(est_did,
             stars = TRUE,
             gof_map = c('nobs','r.squared'))
```

-   After the new loyalty program was introduced, compared with the control group, the treatment group customers increase their total spending by 4.993 units.

## Testing the Parallel Pre-trend Assumption

-   The loyalty program was introduced in June, we need to make sure that, the differences across the treatment group and control group are parallel before June. We can visually examine the assumption:
    -   Step 1: use `dplyr` group aggregation to compute the average outcome for each group in each month
    -   Step 2: use `ggplot2` to visualize the trend.

::: {.content-hidden when-format="beamer"}
```{r}
#| message: false
#| warning: false
pacman::p_load(dplyr,ggplot2, ggthemes)

group_did <- base_did%>%
  group_by(treat,period) %>%
  summarise(avg_y = mean(y, na.rm = T))%>%
  ungroup()

ggplot() + 
  geom_line(data = group_did,
            aes(x = period,
                y = avg_y,
                color = factor(treat))) + 
  theme_stata()

```
:::

## When the Parallel Pre-Trend is Violated

-   If the parallel pre-trend assumption is violated, we can use a variety of methods to rectify the issue

    -   [propensity score matching](https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html)

    -   [synthetic difference-in-differences](https://synth-inference.github.io/synthdid/)

    -   causal forest (week 10)
