---
author: Dr Wei Miao
date: "`r (lubridate::ymd('20231004')+lubridate::dweeks(3))`"
date-format: long
institute: UCL School of Management
title: "Class 7 Supervised Learning Basics"
df-print: kable
colorlinks: true
code-line-numbers: true
format:
  html: 
    toc: true
    number-sections: true
    page-layout: full
    toc-depth: 2
    code-line-numbers: true
    code-copy: hover
    suppress-bibliography: true
  beamer: 
    toc: false
    toc-title: ""
    slide-level: 2
    section-titles: true
    theme: Frankfurt
    colortheme: beaver
    fonttheme: structurebold
    navigation: horizontal
    tbl-colwidths: auto
    fontsize: 9pt
    suppress-bibliography: true
knitr:
  opts_chunk:
    echo: true
    warning: true
    message: true
    error: true
execute: 
  freeze: auto
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

# Supervised Learning

## Motivation: Why Supervised Learning for Business?

::: {.content-visible when-format="html"}
![](images/Supervised%20learning%20and%20unsupersied%20learning.png){fig-align="center" width="1000"}
:::

::: {.content-visible when-format="beamer"}
![](images/Supervised%20learning%20and%20unsupersied%20learning.png){fig-align="center" width="300"}
:::

## Supervised Learning

-   A **supervised learning model** is used when we have one or more **explanatory variables** AND **a response variable** and we would like to learn the relationship (data generating process, or DGP) between the **explanatory** variables and the **response** variable as accurately as possible.

::: {.content-visible when-format="beamer"}
![](images/supervisedlearning.png){fig-align="center" width="200"}
:::

::: {.content-visible when-format="html"}
![](images/supervisedlearning.png){fig-align="center" width="400"}
:::

## Data Generating Process

$$
Y = f(X;\theta) + \epsilon
$$

-   $f$ is the function that characterizes the true relationship between $X$ and $Y$, or DGP, which is never known to us[^1]
-   $Y$ the **response or outcome** variable to be predicted, e.g., if a customer responded to our marketing offer.
-   $X = (X_1,X_2,...,X_p)$ are a set of **explanatory variables,** sometimes called features or predictors, e.g.,
    -   \(1\) customers' past purchase history (e.g., spending in each category, frequency of purchase, recency of purchase)
    -   \(2\) demographic variables (e.g., gender, income, age, kids, etc.)
-   $\theta$ represents the set of **function** **parameters** to be trained
-   $\epsilon$ is the **error term**

[^1]: "All model are wrong, but some are useful" -- George Box. As business analysts, we need to use the "wrong models" correctly.

## Types of Supervised Learning Algorithms

Depending on the type of the **response variable**, supervised learning tasks can be divided into two groups:

-   **Classification tasks** if the outcome is **categorical**
    -   Whether a customer responds to marketing offers
    -   Whether a customer churns
    -   Which product a customer purchases
-   **Regression tasks** if the outcome is **continuous**
    -   Customer total spending in each period
    -   Demand forecasting such as the daily sales of a product

## Difference between Supervised and Unsupervised Learning

::: {.content-visible when-format="beamer"}
[![](images/SupervisedVSUnsupervised.png){fig-alt="source: Statology" fig-align="center" width="400"}](https://www.statology.org/supervised-vs-unsupervised-learning/)
:::

::: {.content-visible when-format="html"}
[![](images/SupervisedVSUnsupervised.png){fig-alt="source: Statology" fig-align="center"}](https://www.statology.org/supervised-vs-unsupervised-learning/)
:::

# Fundamental Tradeoffs

## Accuracy-Interpretability Tradeoff

-   Simpler models are easier to interpret but gives lower accuracy
-   Complicated models can give better prediction accuracy but results are hard to interpret

::: {.content-visible when-format="beamer"}
[![](images/AccuracyVersusInterpretability.png){fig-alt="Pichler, Maximilian & Hartig, Florian. (2022). Machine Learning and Deep Learning -- A review for Ecologists." fig-align="center" width="250"}](https://www.researchgate.net/publication/359890656_Machine_Learning_and_Deep_Learning_--_A_review_for_Ecologists)
:::

::: {.content-hidden when-format="beamer"}
[![](images/AccuracyVersusInterpretability.png){fig-alt="Pichler, Maximilian & Hartig, Florian. (2022). Machine Learning and Deep Learning -- A review for Ecologists." fig-align="center" width="1000"}](https://www.researchgate.net/publication/359890656_Machine_Learning_and_Deep_Learning_--_A_review_for_Ecologists)
:::

## Comparision of Classic Supervised Learning Models

-   Linear regression class models (easy to interpret, low accuracy)

    -   Linear regression coefficients have economic interpretations but prediction accuracy is low

-   **Tree-based Models (balance between interpretability and accuracy)**

    -   Decision tree, random forest

-   Neural-network based models (hard to interpret, high accuracy)

    -   Deep learning only give estimated weights that have no direct business interpretations

## Bias-Variance Tradeoff

-   After we have trained a machine learning model, **bias** is the prediction error of the model on the historical data; **variance** is the prediction error of the model on unseen, new data.

-   If a predictive model **fits** **historical data too well**, then it may not be flexible enough to accommodate future data and thus have a higher chance of failing to make predictions for new data accurately. This problem is called **overfitting**.

-   Overfitting leads to **low bias** but **high variance**. Hence the name bias-variance tradeoff or bias-variance dilemma.

::: {.content-visible when-format="beamer"}
![](images/overfitting.png){fig-align="center" width="150"}
:::

::: {.content-hidden when-format="beamer"}
![](images/overfitting.png){fig-align="center" width="300"}
:::

# Overfitting and Underfitting

## How to Mitigate Overfitting

-   To mitigate the overfitting problem, when training predictive models, we need to use the **cross-validation** technique by splitting the full **historical data** into a **training set** and a **test set**.
    -   **A training** **set** (70% - 80% of labelled data)**:** we train the predictive model based on the training set.
    -   **A test** **set** (20% - 30% of labelled data)**:** we pretend that we don't know the outcomes for the test set and make predictions from the predictive model. However, in fact, we do observe the actual outcomes for the test set, so that we can evaluate the prediction accuracy by comparing the predicted outcomes versus the actual outcomes.

::: {.content-visible when-format="beamer"}
![](images/trainingtest.png){fig-align="center" width="150"}
:::

::: {.content-visible when-format="html"}
![](images/trainingtest.png){fig-align="center" width="500"}
:::

-   For more complicated models with hyper-parameters such as deep learning models, we may even need to split our data into 3 sets (training, validation, and test sets).

## Underfitting

-   **Underfitting** occurs when a predictive model cannot sufficiently capture the DGP on both historical data and new data.

-   Underfitting leads to **high bias** as well as **high variance**. Thus, underfitting is the worst case, which should be avoided by all means.

-   To mitigate the underfitting problem, we need to select more suitable models.

::: {.content-visible when-format="beamer"}
![](images/OverfittingUnderfitting.png){fig-align="center" width="300"}
:::

::: {.content-visible when-format="html"}
![](images/OverfittingUnderfitting.png){fig-align="center" width="1000"}
:::
