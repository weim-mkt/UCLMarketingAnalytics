---
author: Dr Wei Miao
date: "`r (lubridate::ymd('20231004')+lubridate::dweeks(5))`"
date-format: long
institute: UCL School of Management
title: "Class 12 OLS Regression Basics"
df-print: kable
colorlinks: true
code-line-numbers: true
format:
  html: 
    toc: true
    number-sections: true
    page-layout: full
    toc-depth: 2
    code-line-numbers: true
    code-copy: hover
  beamer: 
    toc: false
    toc-title: ""
    slide-level: 2
    section-titles: true
    theme: Frankfurt
    colortheme: beaver
    fonttheme: structurebold
    navigation: horizontal
    tbl-colwidths: auto
    fontsize: 9pt
    suppress-bibliography: true
knitr:
  opts_chunk:
    echo: true
    warning: true
    message: true
    error: true
execute: 
  freeze: auto
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

# Basics of Linear Regression

```{r}
#| echo: false
data_full <- readRDS("/Users/weimiao/Dropbox/UCL/Teaching/MSIN0094 Marketing Analytics/UCL 2021 - 2022/slides/Week 4/data_full.rds")
pacman::p_load(dplyr,ggplot2,ggthemes)
```

## Linear Regression Models

-   A simple linear regression is a model as follows. $$
    Y_i = \beta_0 + x_1 \beta_1 + x_2\beta_2+ \ldots + x_k\beta_k + \epsilon_i
    $$

-   $y_i$: Outcome variable/dependent variable/regressand/response variable/LHS variable

-   $\beta$: Regression coefficients/estimates/parameters; $\beta_0$: intercept

-   $x_k$: Control variable/independent variable/regressor/explanatory variable/RHS variable

    -   Lower case such as $x_1$ usually indicates a single variable while upper case such as $X_{ik}$ indicates a set of several variables

-   $\epsilon_i$: Error term, which captures the deviation of Y from the prediction

    -   Expected mean should be 0, i.e., $E[\epsilon|X]=0$

    -   If we take the expectation of Y, we should have $$
        E[Y|X] = \beta_0 + x_1 \beta_1 + x_2\beta_2+ \ldots + x_k\beta_k 
        $$

## Why the Name "Regression"?

-   The term "regression" was coined by Francis Galton to describe a biological phenomenon: The heights of descendants of tall ancestors tend to regress down towards a normal average.

-   The term "regression" was later extended by statisticians Udny Yule and Karl Pearson to a more general statistical context (Pearson, 1903).

-   In supervised learning models, "regression" has a different meaning: when outcome is continuous, the task is called regression task.[^1]

[^1]: ML models are developed by computer science; causal inference models are developed by economists.

# Estimation of Coefficients

## How to Run Regression in R

-   In R, there are tons of packages that can run OLS regression.

-   In this module, we will be using the `fixest` package, because it's able to estimate high-dimensional fixed effects.

```{r}
#| echo: true
pacman::p_load(modelsummary,fixest)

OLS_result <- feols( 
   fml = total_spending ~ Income, # Y ~ X
   data = data_full, # dataset from Tesco
   ) 

```

## Report Regression Results

```{r}
modelsummary(OLS_result,
    stars = TRUE  # export statistical significance
  )
```

## Parameter Estimation: Univariate Regression Case

-   Let's take a **univariate regression**[^2] as an example

[^2]: Regressions with a single regressor is called univariate regressions.

$$
    y = a + b x_1  + \epsilon
$$

-   For each guess of a and b, we can compute the error for customer $i$, $$
    e_i = y_{i}-a-b x_{1i}
    $$

-   We can compute the **sum of squared residuals (SSR)** across all customers

$$
        SSR =\sum_{i=1}^{n}\left(y_{i}-a-b x_{1i}\right)^{2}
$$

-   **Objective of estimation**: Search for the unique set of $a$ and $b$ that can minimize the SSR.

-   This estimation method that minimizes SSR is called **Ordinary Least Square (OLS).**

## Visualization: Estimation of Univariate Regression

-   If in the Tesco dataset, if we regress **total spending** (Y) on **income** (X)

::: {.content-visible when-format="beamer"}
```{r}
#| out-width: "50%"
#| fig-align: "center"
#| echo: false
library(ggplot2)

lm_obj <- lm(data = data_full,
             total_spending ~ Income)

get_ssr <- function(a,b){
  return(sum(data_full$total_spending - a - b * data_full$Income)^2)
}

ggplot(data = data_full, 
       aes(x = Income, y = total_spending)) + 
  geom_point() + 
  theme_stata() + 
  geom_abline(slope = 0.004, intercept = 0, color = "red") + 
  # geom_abline(slope = 0.004, intercept = 200, color = "blue") + 
  geom_abline(slope = 0.06, intercept = - 552, color = "purple") + 
  geom_abline(slope = lm_obj$coefficients[2], 
              intercept = lm_obj$coefficients[1], color = "green") 
  
```
:::

::: {.content-visible when-format="html"}
```{r}
#| fig-align: "center"
#| echo: false
library(ggplot2)

lm_obj <- lm(data = data_full,
             total_spending ~ Income)

get_ssr <- function(a,b){
  return(sum(data_full$total_spending - a - b * data_full$Income)^2)
}

ggplot(data = data_full, 
       aes(x = Income, y = total_spending)) + 
  geom_point() + 
  theme_stata() + 
  geom_abline(slope = 0.004, intercept = 0, color = "red") + 
  # geom_abline(slope = 0.004, intercept = 200, color = "blue") + 
  geom_abline(slope = 0.06, intercept = - 552, color = "purple") + 
  geom_abline(slope = lm_obj$coefficients[2], 
              intercept = lm_obj$coefficients[1], color = "green") 
  
```
:::

| Model                  | Color  | Sum of Squared Error    |
|------------------------|--------|-------------------------|
| $Y = -552 + 0.06 * X$  | Purple | `r get_ssr(-552,0.06)`  |
| $Y = 0 + 0.004 * X$    | Red    | `r get_ssr(0,0.004)`    |
| $Y = -552 + 0.021 * X$ | Green  | `r get_ssr(-552,0.021)` |

## Multivariate Regression

-   The OLS estimation also applies to multivariate regression with multiple regressors.

$$
y_i = b_0 + b_1 x_{1} + ... + b_k x_{k}+\epsilon_i
$$

-   **Objective of estimation**: Search for the **unique** set of $b$ that can minimize the **sum of squared residuals**.

$$
    SSR= \sum_{i=1}^{n}\left(y_{i}-b_0 - b_1 x_{1} - ... - b_k x_{k} \right)^{2}
$$

# Interpretation of Coefficients

## Coefficients Interpretation

-   Now on your Quarto document, let's run a new regression, where the DV is $total\_spending$, and X includes $Income$ and $Kidhome$.

\footnotesize

```{r}
#| echo: false
feols(data = data_full,
     fml = total_spending ~ Income + Kidhome) %>%
  modelsummary(stars = T)
```

\normalsize

-   **Controlling for** Kidhome, one unit increase in `Income` increases `totalspending` by Â£0.019.

## Standard Errors and P-Values

-   Because the regression is estimated on a random sample of the population, so if we rerun the regression on different samples, we would get a different set of regression coefficients each time.

-   In theory, the regression coefficients estimates follows a **t-distribution**: the mean is the true $\beta$. The **standard error** of the estimates is the estimated standard deviation of the error.

-   We can whether the coefficients are statistically different from 0 using **hypothesis testing**.

    -   Null hypothesis: the true regression coefficient $\beta$ is 0

-   `Income`/`Kidhome` is statistically significant at the 1% level.

## R-Squared

-   R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by all included variables in a regression.

-   Interpretation: 65.8% of the variation in `totalspending` can be explained by `Income` and `Kidhome`.

-   As the number of variables increases, the $R^2$ will naturally increase, so sometimes we may need to penalize the number of variables using the so-called **adjusted R-squared**.

    ::: callout-important
    R-Squared is only important for supervised learning prediction tasks, because it measures the predictive power of the X. However, In causal inference tasks, $R^2$ does not matter much.
    :::
