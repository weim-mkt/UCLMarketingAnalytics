---
author: Dr Wei Miao
date: "`r (lubridate::ymd('20221006')+lubridate::dweeks(6))|> format('%a, %b %d %Y')`"
institute: UCL School of Management
title: "Class 13 OLS Regression Advanced"
df-print: kable
colorlinks: true
code-line-numbers: true
format:
  html: 
    toc: true
    embed-resources: true
    number-sections: true
    page-layout: full
    toc-depth: 2
    code-line-numbers: true
    code-copy: hover
  beamer: 
    toc: false
    toc-title: ""
    slide-level: 2
    section-titles: true
    theme: Frankfurt
    colortheme: beaver
    fonttheme: structurebold
    navigation: horizontal
    tbl-colwidths: auto
    fontsize: 9pt
    suppress-bibliography: true
knitr:
  opts_chunk:
    echo: true
    warning: true
    message: true
    error: false
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

# Categorical Variables

## Categorical variables

::: {.content-visible when-format="beamer"}
```{r}
#| echo: false
pacman::p_load(dplyr,ggplot2,ggthemes)
# Load both datasets
data_purchase <- read.csv(file = "https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1", 
                      header = T)

data_demo <- read.csv("https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1",
                      header = T)

# Left join demographic data into purchase data
data_full <- data_purchase %>%
  left_join(data_demo, by = "ID")

# Handle Missing Values of Income
data_full <- data_full %>%
  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T))) %>%
  mutate(total_spending = MntFishProducts + MntFruits + MntGoldProds + MntMeatProducts + MntSweetProducts + MntWines)
```
:::

::: {.content-visible when-format="html"}
```{r}
#| echo: true
pacman::p_load(dplyr,ggplot2,ggthemes)
# Load both datasets
data_purchase <- read.csv(file = "https://www.dropbox.com/s/126e9vkq80y9ti9/purchase.csv?dl=1", 
                      header = T)

data_demo <- read.csv("https://www.dropbox.com/s/hbrgktcz98y0igs/demographics.csv?dl=1",
                      header = T)

# Left join demographic data into purchase data
data_full <- data_purchase %>%
  left_join(data_demo, by = "ID")

# Handle Missing Values of Income
data_full <- data_full %>%
  mutate(Income = replace(Income, is.na(Income), mean(Income,na.rm =T))) %>%
  mutate(total_spending = MntFishProducts + MntFruits + MntGoldProds + MntMeatProducts + MntSweetProducts + MntWines)
```
:::

-   So far, the independent variables we have used are `Income` and `Kidhome`, which are **continuous variables**.

-   Some variables are intrinsically not countable; we need to treat them as **categorical variables**

    -   e.g., gender, education group, city.

## Handling Categorical Variables using `factor()`

-   In R, we need to use a function `factor()` to inform R that this variable is a categorical variable, such that statistical models will treat them differently from continuous variables.
    -   Refer to this [link](https://www.datacamp.com/tutorial/factors-in-r) for more examples in datacamp.
-   We can use `factor(Education)` to indicate that, `Education` is a categorical variable.

```{r}
data_full <- data_full %>%
  mutate(Education_factor = factor(Education))
```

-   We can use `levels()` to check how many categories are there in the factor variable.

```{r}
# check levels of a factor
levels(data_full$Education_factor)
```

::: {.content-visible when-format="beamer"}
## Handling Categorical Variables using `factor()`

-   We can also change the baseline group to another group using `relevel()`.

```{r}
data_full <- data_full %>%
  mutate(Education_factor_2 = relevel(Education_factor, 
                                      ref = "Basic") )

levels(data_full$Education_factor_2)
```
:::

::: {.content-visible when-format="html"}
```{r}
data_full <- data_full %>%
  mutate(Education_factor_2 = relevel(Education_factor, ref = "Basic") )

levels(data_full$Education_factor_2)
```
:::

## Running Regression with Factor Variables

\tiny

```{r}
pacman::p_load(fixest,modelsummary)
feols_categorical <- feols(data = data_full,
  fml = total_spending ~ Income + Kidhome + Education_factor_2)
modelsummary(feols_categorical,
             stars = T)
```

## Interpretation of Coefficients for Categorical Variables

-   Internally, R uses **one-hot encoding** to encode factor variables with **K** levels into **K-1** binary variables.
    -   Because we have the intercept, we can only have K-1 binary variables.
    -   The intercept stands for the effects of the baseline group.
    -   In the regression result table, `Basic` group is suppressed if we use `Education_factor_2`, because this group is chosen as the **baseline group**.
-   The interpretation template of coefficients for factor variables: Ceteris paribus, compared with the \[baseline group\], the \[outcome variable\] of \[group XXX\] is higher/lower by \[coefficient\], and the coefficient is statistically \[significant/insignificant\].
    -   Ceteris paribus, compared with the basic education group, the total spending of PhD group is lower by 132.252 dollors. The coefficient is statistically significant at the 5% level.

\

> After-class exercise: change the baseline group to Master, rerun the regression, and interpret the coefficients.

# Non-linear Effects

## Quadratic Terms

-   If we believe the relationship between the outcome variable and explanatory variable is a quadratic function, we can include **an additional quadratic term** in the regression to model such non-linear relationship.

$$
Spending = \beta_0 + \beta_1Income + \beta_2Income^2  + \epsilon
$$

::: {.content-visible when-format="beamer"}
```{r}
#| echo: false
#| out-width: 50%
#| fig-align: 'center'

ggplot(data = data_full,
       aes(x = Income, y = total_spending)) + 
  geom_point()+theme_stata()
```
:::

::: {.content-visible when-format="html"}
```{r}
#| echo: true

ggplot(data = data_full,
       aes(x = Income, y = total_spending)) + 
  geom_point()+theme_stata()
```
:::

## Quadratic Terms

-   If after estimation, the coefficient for $Income^2$, $\beta_2$, is negative, then we have an down open parabola.

![](images/parabola.jpeg){fig-align="center" width="300"}

-   That is, as income increases, total spending first increases and then decreases, i.e., a non-linear effect.

## Quadratic Terms in Linear Regression

-   Let's run two regressions, with and without the quadratic term.

```{r}

data_full <- data_full %>%
  mutate(Income_quadartic = Income^2 )

# model 1: without quadratic term
feols_noquadratic <- feols(data = data_full,
  fml = total_spending ~ Income )

# model 2: with quadratic term
feols_quadratic <- feols(data = data_full,
  fml = total_spending ~ Income  + Income_quadartic )

```

::: {.content-visible when-format="html"}
```{r}
modelsummary(list(feols_noquadratic,feols_quadratic), 
             stars = T)
```
:::

::: {.content-visible when-format="beamer"}
## Quadratic Terms in Linear Regression

```{r}
modelsummary(list(feols_noquadratic,feols_quadratic), 
             stars = T)
```
:::

## Quadratic Terms: Compute the Vertex

-   We can compute the vertex point where total spending is maximized by income

```{r}
# extract the coeffcient vector
feols_coefficient <- feols_quadratic$coefficients
feols_coefficient

# Use b / (-2a) to get the vertex
- feols_coefficient[2]/ 
  (2 * feols_coefficient[3])

```

# Linear Probability Model

## Linear Probability Model

-   In Predictive Analytics, we learned how to use decision tree and random forest to make predictions. In fact, linear regression can also be used as another supervised learning model.

-   On the one hand, regression predicts the expectation of response $Y$ conditional on $X$; that is
$$
    E[Y|X]= X\beta
$$

-   On the other hand, for a binary outcome variable, if the probability of outcome occurring is $p$, then we can write the expectation of $Y$ is
$$
E[Y|X] = 1 * p + 0 * (1 - p) = p
$$

-   As a result, we have the following equation
$$
   \operatorname{Probability}[Y=1|X] = E[Y|X] = X \beta
$$

-   Interpretation of LPM: Everything else equal, a unit change in $x$ will change the probability of the outcome occurring by $\beta$ units.

## Pros and Cons of LPM

- The procedures of training LPM is similar to training a decision tree `rpart()`/random forest `ranger()`: we use linear regression function `feols()` to train the LPM on the **training data** and make predictions on the **test data**.

- Advantages
    - Easy and fast to run 
    - High interpretability: coefficients have clear economic meanings
    
- Disadvantages
    - Predicted probabilities of occurring may fall out of the [0,1] range
    - Accuracy tends to be low




