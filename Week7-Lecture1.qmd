---
author: Dr Wei Miao
date: "`r (lubridate::ymd('20231004')+lubridate::dweeks(6))`"
date-format: long
institute: UCL School of Management
title: "Class 13 OLS Regression Advanced"
df-print: kable
colorlinks: true
code-line-numbers: true
format:
  html: 
    toc: true
    embed-resources: true
    number-sections: true
    page-layout: full
    toc-depth: 2
    code-line-numbers: true
    code-copy: hover
  beamer: 
    toc: false
    toc-title: ""
    slide-level: 2
    section-titles: true
    theme: Frankfurt
    colortheme: beaver
    fonttheme: structurebold
    navigation: horizontal
    tbl-colwidths: auto
    fontsize: 9pt
    suppress-bibliography: true
knitr:
  opts_chunk:
    echo: true
    warning: true
    message: true
    error: false
execute: 
  freeze: auto
editor_options: 
  chunk_output_type: inline
bibliography: references.bib
---

# Categorical Variables

## Categorical variables

::: {.content-visible when-format="beamer"}
```{r}
#| echo: false
pacman::p_load(dplyr,ggplot2,ggthemes)
# Load both datasets
data_full <- read.csv(file = "https://www.dropbox.com/scl/fi/hhweiqsuwgcwgd1jiuyte/data_full.csv?rlkey=jwyd9z409b5wpwz41ow8d1otj&dl=1", 
                      header = T)
```
:::

::: {.content-visible when-format="html"}
```{r}
#| echo: true
pacman::p_load(dplyr,ggplot2,ggthemes)
# Load both datasets
data_full <- read.csv(file = "https://www.dropbox.com/scl/fi/hhweiqsuwgcwgd1jiuyte/data_full.csv?rlkey=jwyd9z409b5wpwz41ow8d1otj&dl=1", 
                      header = T)
```
:::

-   So far, the independent variables we have used are `Income` and `Kidhome`, which are **continuous variables**.

-   Some variables are intrinsically not countable; we need to treat them as **categorical variables**

    -   e.g., gender, education group, city.

## Handling Categorical Variables in R using `factor()`

-   In R, we need to use a function `factor()` to explicitly inform R that this variable is a categorical variable, such that statistical models will treat them differently from continuous variables.
    -   e.g., we can use `factor(Education)` to indicate that, `Education` is a categorical variable.

```{r}
data_full <- data_full %>%
  mutate(Education_factor = factor(Education))
```

-   We can use `levels()` to check how many categories there are in the factor variable.
    -   e.g., `Education` has 5 different levels.

```{r}
# check levels of a factor
levels(data_full$Education_factor)
```

::: {.content-visible when-format="beamer"}
## Handling Categorical Variables using `factor()`

-   `factor()` will check all levels of the categorical variables, and then choose the default level based on alphabetic order.
-   If needed, we can revise the baseline group to another group using `relevel()` function.

```{r}
# Create a new factor variable, with Basic as the baseline.
data_full <- data_full %>%
  mutate(Education_factor_2 = relevel(Education_factor, 
                                      ref = "Basic") )

levels(data_full$Education_factor_2)
```
:::

::: {.content-visible when-format="html"}
```{r}
# Create a new factor variable, with Basic as the baseline.
data_full <- data_full %>%
  mutate(Education_factor_2 = relevel(Education_factor, ref = "Basic") )

levels(data_full$Education_factor_2)
```
:::

## Running Regression with Factor Variables

\tiny

```{r}
pacman::p_load(fixest,modelsummary)
feols_categorical <- feols(data = data_full,
  fml = total_spending ~ Income + Kidhome + Education_factor_2)
modelsummary(feols_categorical,
             stars = T,
             gof_map = c('nobs','r.squared'))
```

## One-Hot Encoding of `factor()`

-   In the raw data, Education is label-encoded with 5 levels.

![](images/onehot_encoding1.png){fig-align="center" width="76"}

-   After factorizing education with "*Basic"* as the baseline group, internally, we have 4 binary indicators as follows. Because we have the intercept,"*Basic*" is omitted as the baseline group. Other groups represent the comparison relative to the baseline group.

![](images/onehot_encoding2.png){fig-align="center" width="201"}

## Interpretation of Coefficients for Categorical Variables

-   In general, R uses **one-hot encoding** to encode factor variables with **K** levels into **K-1** binary variables.
    -   As we have the intercept term, we can only have **K-1** binary variables.
-   The interpretation of coefficients for factor variables: Ceteris paribus, compared with the ***\[baseline group\]***, the ***\[outcome variable\]*** of ***\[group X\]*** is higher/lower by ***\[coefficient\]***, and the coefficient is statistically ***\[significant/insignificant\]***.
    -   Ceteris paribus, compared with the basic education group, the total spending of PhD group is lower by 153.190 dollars. The coefficient is statistically significant at the 1% level.
-   Now please rerun the regression using `Education_factor` and interpret the coefficients. What's your finding?
    -   Conclusion: factor variables can only measure the relative difference in outcome variable across different groups rather than the absolute levels.

## Application of Categorical Variables in Marketing

-   Analyze the treatment effects in A/B/N testing, where $Treatment_i$ is a categorical variable that specifies the treatment group customer $i$ is in:

$$
Outcome_i = \beta_0 + \delta Treatment_i + \epsilon
$$

-   Analyze the brand premiums or country-of-origin effects:

$$
Sales_i = \beta_0 + \beta_1 Brand_i + \beta_2 Country_i + X\beta +\epsilon
$$

# Non-linear Effects

## Quadratic Terms

-   If we believe the relationship between the outcome variable and explanatory variable is a quadratic function, we can include **an additional quadratic term** in the regression to model such non-linear relationship.

$$
totalspending = \beta_0 + \beta_1Income + \beta_2Income^2  + \epsilon
$$

::: {.content-visible when-format="beamer"}
```{r}
#| echo: false
#| out-width: 50%
#| fig-align: 'center'

ggplot(data = data_full,
       aes(x = Income, y = total_spending)) + 
  geom_point()+theme_stata()
```
:::

::: {.content-visible when-format="html"}
```{r}
#| echo: true

ggplot(data = data_full,
       aes(x = Income, y = total_spending)) + 
  geom_point()+theme_stata()
```
:::

## Quadratic Terms

-   If the coefficient for $Income^2$ is negative, then we have an downward open parabola. That is, as income increases, total spending first increases and then decreases, i.e., a non-linear, non-monotonic effect.
    -   As income first increases, customers increase their spending with Tesco due to the **income effect**; however, as customers get even richer, they may switch to more premium brands such as Waitrose, so their spending may decrease due to the **substitution effect**.

![](images/parabola.jpeg){fig-align="center" width="300"}

## Quadratic Terms in Linear Regression

-   Let's run two regressions in the Quarto document, with and without the quadratic term.

```{r}

# model 1: without quadratic term
feols_noquadratic <- feols(data = data_full,
  fml = total_spending ~ Income )

# model 2: with quadratic term
feols_quadratic <- feols(data = data_full%>%
                           mutate(Income_squared = Income^2 ),
  fml = total_spending ~ Income  + Income_squared )

```

::: {.content-visible when-format="html"}
```{r}
modelsummary(list(feols_noquadratic,
       feols_quadratic),
       stars = T,
       fmt = fmt_sprintf("%.2e"),
       gof_map = c('nobs','r.squared'))
```
:::

::: {.content-visible when-format="beamer"}
## Quadratic Terms in Linear Regression

```{r}
modelsummary(list(feols_noquadratic,
       feols_quadratic),
       stars = T,
       fmt = fmt_sprintf("%.2e"),
       gof_map = c('nobs','r.squared'))
```
:::

## Quadratic Terms: Compute the Vertex

-   We can compute the vertex point where total spending is maximized by income

```{r}
# extract the coeffcient vector using $ sign
feols_coefficient <- feols_quadratic$coefficients
feols_coefficient

# Use b / (-2a) to get the vertex
- feols_coefficient[2]/ 
  (2 * feols_coefficient[3])

```

# Linear Probability Model

## Linear Probability Model

-   In Predictive Analytics, we learned how to use decision tree and random forest to make predictions for binary outcome variables.

-   In fact, linear regression can also be used as another supervised learning model to predict binary outcomes. When the outcome variable is a binary variable, the linear regression model is also called linear probability model.

    -   On the one hand, regression predicts the expectation of response $Y$ conditional on $X$; that is $$
        E[Y]= E[X\beta+\epsilon]=X\beta
        $$

    -   On the other hand, for a binary outcome variable, if the probability of outcome occurring is $p$, then we can write the expectation of $Y$ is $$
        E[Y] = 1 * p + 0 * (1 - p) = p
        $$

    -   As a result, we have the following equation $$
           p = X \beta
        $$

-   Interpretation of LPM coefficients: Everything else equal, a unit change in $x$ will change the **probability of the outcome occurring** by $\beta$.

## Pros and Cons of LPM

-   We use linear regression function `feols()` to train the LPM on the **training data** and make predictions using `predict(LPM, data_test)` to make predictions on the **test data**.

-   Advantages

    -   Fast to run, even with a large number of fixed effects and features
    -   High interpretability: coefficients have clear economic meanings

-   Disadvantages

    -   Predicted probabilities of occurring may fall out of the \[0,1\] range
    -   Accuracy tends to be low
